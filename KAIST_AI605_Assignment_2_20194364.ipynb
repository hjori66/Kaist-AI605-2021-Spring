{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KAIST AI605 Assignment 2_20194364.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ed599609dce646eeab6d7786eaf8f1ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d7da4b84388041fa827f2cea2a107f3b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f1a259616a894fef8343cea8cd87b335",
              "IPY_MODEL_94617f73c0aa483a892bde1920ff92b5"
            ]
          }
        },
        "d7da4b84388041fa827f2cea2a107f3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1a259616a894fef8343cea8cd87b335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8cc04860b3e6451db9c788de29d9379b",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1877,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1877,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c7be632c2bea41119ea8e99ebd9bd807"
          }
        },
        "94617f73c0aa483a892bde1920ff92b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_010409b89be64017a6eed78ea3fc6cf1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5.03k/? [00:02&lt;00:00, 2.13kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_00502a4ce70649ba97e91dbf602befb3"
          }
        },
        "8cc04860b3e6451db9c788de29d9379b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c7be632c2bea41119ea8e99ebd9bd807": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "010409b89be64017a6eed78ea3fc6cf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "00502a4ce70649ba97e91dbf602befb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd6abd63c4604f71939fc6fa7c0fa51d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c726f319ff6c40518f500105b089885d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4f5c9db535eb469a9c62017a830e5f0e",
              "IPY_MODEL_1c55e3467ac64a48963ab3830dbe4146"
            ]
          }
        },
        "c726f319ff6c40518f500105b089885d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4f5c9db535eb469a9c62017a830e5f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4a83705ce7634b309b5d60680fc54507",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 955,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 955,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c141b5c1e0bc404284872dfdacd69e87"
          }
        },
        "1c55e3467ac64a48963ab3830dbe4146": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_018be2efabc2460bb6b2827725dbca5c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.19k/? [00:00&lt;00:00, 31.8kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e07669929bb84117b62baea8621a1f36"
          }
        },
        "4a83705ce7634b309b5d60680fc54507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c141b5c1e0bc404284872dfdacd69e87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "018be2efabc2460bb6b2827725dbca5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e07669929bb84117b62baea8621a1f36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6a260e01320497cbda223ac0fab49a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7a8013b37fdb419fa28233081b7684c0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_316af366649441a887f9cc5d7013f4c0",
              "IPY_MODEL_a9d3d1b5c83d4cffb549b7ee1e63e925"
            ]
          }
        },
        "7a8013b37fdb419fa28233081b7684c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "316af366649441a887f9cc5d7013f4c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_14485e6ed1be4da18f88182aad1dc212",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 8116577,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 8116577,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_23da3f03d06b4367b7999cb5d6a776c2"
          }
        },
        "a9d3d1b5c83d4cffb549b7ee1e63e925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8365df8e631f47e9a686723c48e6e504",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 30.3M/? [00:00&lt;00:00, 30.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7f43b4970ad244139d9033161a05d5b3"
          }
        },
        "14485e6ed1be4da18f88182aad1dc212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "23da3f03d06b4367b7999cb5d6a776c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8365df8e631f47e9a686723c48e6e504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7f43b4970ad244139d9033161a05d5b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "81e5c183c1f3491180603685e74a547b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_de66fdc5a5b24c4c805d486fed4e92c0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3d9c3938fc644278ac376f2a9bdaa272",
              "IPY_MODEL_440b70f8371e4840b775f04a0439eb38"
            ]
          }
        },
        "de66fdc5a5b24c4c805d486fed4e92c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3d9c3938fc644278ac376f2a9bdaa272": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7272bcfe6dd24f6ba2ddf8f23bd7ea67",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1054280,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1054280,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0c52797916a743b88a545e25687843d4"
          }
        },
        "440b70f8371e4840b775f04a0439eb38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d35e242cdb4942529cf1f859018c2482",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4.85M/? [00:00&lt;00:00, 16.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f146d6b82e049b0a5fc7cbbf5428b49"
          }
        },
        "7272bcfe6dd24f6ba2ddf8f23bd7ea67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0c52797916a743b88a545e25687843d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d35e242cdb4942529cf1f859018c2482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f146d6b82e049b0a5fc7cbbf5428b49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f4e2cb1eeb5a4205bf91b772d4cc5bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_581b5be899244074abc90c1dd795947a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_699e5192d1a24bc9b50ce4e1fb6faab5",
              "IPY_MODEL_84a50413511d455f9a3c3afe064a2a42"
            ]
          }
        },
        "581b5be899244074abc90c1dd795947a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "699e5192d1a24bc9b50ce4e1fb6faab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_deac1b10c8b544fa9b45275d72dce116",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22bb5af80e374e9b9cbf017408ca432a"
          }
        },
        "84a50413511d455f9a3c3afe064a2a42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ae9917c2decf4f3db9cf0e397b36820c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 87599/0 [00:06&lt;00:00, 20578.69 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_08c2c496f27845cda4d198bee590206b"
          }
        },
        "deac1b10c8b544fa9b45275d72dce116": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22bb5af80e374e9b9cbf017408ca432a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ae9917c2decf4f3db9cf0e397b36820c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "08c2c496f27845cda4d198bee590206b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f3e91985c1a4ae793888cf3d810adbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_682f00b78b7b414fa5a42a9395f6a7a7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9bf772d2ce07413e9c7513f27a86cd25",
              "IPY_MODEL_1b29487acf064b48af31fbd90764b9fc"
            ]
          }
        },
        "682f00b78b7b414fa5a42a9395f6a7a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9bf772d2ce07413e9c7513f27a86cd25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f34e28c67a4b4abf920606981746636b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1d461e8fc3404098bb06fc2fa0d7d477"
          }
        },
        "1b29487acf064b48af31fbd90764b9fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d83ac70f127d453d82c1a134e0278a61",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10570/0 [00:00&lt;00:00, 84.26 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b7ff39f63d4645e1ba01d46be33d3b2b"
          }
        },
        "f34e28c67a4b4abf920606981746636b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1d461e8fc3404098bb06fc2fa0d7d477": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d83ac70f127d453d82c1a134e0278a61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b7ff39f63d4645e1ba01d46be33d3b2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "738c408171c14571b0b0a61fb02904b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_00ae909e7c38429bb414672d341535e2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_288ed80702a4499da42544444bd84cac",
              "IPY_MODEL_f857138d0c414b0eaf9e0ee23cf1572c"
            ]
          }
        },
        "00ae909e7c38429bb414672d341535e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "288ed80702a4499da42544444bd84cac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4642ba4f8522440e8c4ed70de8bbbd28",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 30,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aaa5adc4b96546a39c3499d0b4d64749"
          }
        },
        "f857138d0c414b0eaf9e0ee23cf1572c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7c1f578e61d24a29885a78dbc9ff2e2b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/30 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e4d050990f5947429bf0aa153594c9e8"
          }
        },
        "4642ba4f8522440e8c4ed70de8bbbd28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aaa5adc4b96546a39c3499d0b4d64749": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7c1f578e61d24a29885a78dbc9ff2e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e4d050990f5947429bf0aa153594c9e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hjori66/Kaist-AI605-2021-Spring/blob/main/KAIST_AI605_Assignment_2_20194364.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReonT_YasRSx"
      },
      "source": [
        "# KAIST AI605 Assignment 2: Token Classification with RNNs and Attention\n",
        "Author: Minjoon Seo (minjoon@kaist.ac.kr)\n",
        "\n",
        "TA in charge: Taehyung Kwon (taehyung.kwon@kaist.ac.kr)\n",
        "\n",
        "**Due date**:  April 19 (Mon) 11:00pm, 2021  \n",
        "\n",
        "\n",
        "Your name: Taehwan Kim\n",
        "\n",
        "Your student ID: 20194364\n",
        "\n",
        "Your collaborators: -\n",
        "\n",
        "## Assignment Objectives\n",
        "- Verify theoretically and empirically how Transformer's attention mechanism works for sequence modeling task.\n",
        "- Implement Transformer's encoder attention layer from scratch using PyTorch.\n",
        "- Design an Attention-based token classification model using PyTorch.\n",
        "- Apply the token classification model to a popular machine reading comprehension task, Stanford Question Answering Dataset (SQuAD).\n",
        "- (Bonus) Analyze pros and cons between using RNN + attention versus purely attention.\n",
        "\n",
        "## Your Submission\n",
        "Your submission will be a link to a Colab notebook that has all written answers and is fully executable. You will submit your assignment via KLMS. Use in-line LaTeX (see below) for mathematical expressions. Collaboration among students is allowed but it is not a group assignment so make sure your answer and code are your own. Also make sure to mention your collaborators in your assignment with their names and their student ids.\n",
        "\n",
        "## Grading\n",
        "The entire assignment is out of 100 points. There are two bonus questions with 30 points altogether. Your final score can be higher than 100 points.\n",
        "\n",
        "\n",
        "## Environment\n",
        "You will only use Python 3.7 and PyTorch 1.8, which is already available on Colab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qwta269rqLQ",
        "outputId": "cd4c6efe-cf9a-4c49-912c-75be8c7fa981"
      },
      "source": [
        "from platform import python_version\n",
        "import torch\n",
        "\n",
        "print(\"python\", python_version())\n",
        "print(\"torch\", torch.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python 3.7.10\n",
            "torch 1.8.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTm5eq4NwQZs"
      },
      "source": [
        "## 1. Transformer's Attention Layer\n",
        "\n",
        "We will first start with going over a few concepts that you learned in your high school statistics class. The variance of a random variable $X$, $\\text{Var}(X)$ is defined as $\\text{E}[(X-\\mu)^2]$ where $\\mu$ is the mean of $X$. Furthermore, given two independent random variables $X$ and $Y$ and a constant $a$,\n",
        "$$ \\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y), \\quad \\ldots \\; \\text{(1)}$$ \n",
        "$$ \\text{Var}(aX) = a^2\\text{Var}(X), \\quad \\ldots \\; \\text{(2)}$$\n",
        "$$ \\text{Var}(XY) = \\text{E}(X^2)\\text{E}(Y^2) - [\\text{E}(X)]^2[\\text{E}(Y)]^2. \\quad \\ldots \\; \\text{(3)}$$\n",
        "\n",
        "**Problem 1.1** *(10 points)* Suppose we are given two sets of $n$ random variables, $X_1 \\dots X_n$ and $Y_1 \\dots Y_n$, where all of these $2n$ variables are mutually independent and have a mean of $0$ and a variance of $1$. Prove that\n",
        "$$\\text{Var}\\left(\\sum_i^n X_i Y_i\\right) = n.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDO1QDJOXza8"
      },
      "source": [
        "There is a typo in the formula $\\text{(2)}$. I changed it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3ULLzK0NOPA"
      },
      "source": [
        "**Answer 1.1** \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "  \\text{Var}\\left(\\sum_i^n X_i Y_i\\right)\n",
        "  &= \\sum_i^n \\text{Var} \\left(X_i Y_i\\right) \\quad \\because \\text{(1), independence} \\\\\n",
        "  &= \\sum_i^n \\left[\\text{E}(X_i^2)\\text{E}(Y_i^2) - [\\text{E}(X_i)]^2[\\text{E}(Y_i)]^2 \\right] \\quad \\because \\text{(3)} \\\\\n",
        "  &= \\sum_i^n \\left[\\text{E}((X_i-0)^2)\\text{E}((Y_i-0)^2)\\right] \\\\\n",
        "  &= \\sum_i^n \\left[\\text{E}((X_i-[\\text{E}(X_i)])^2)\\text{E}((Y_i-[\\text{E}(Y_i)])^2)\\right] \\\\\n",
        "  &= \\sum_i^n \\left[\\text{Var}(X_i) \\text{Var}(Y_i)\\right] \\\\\n",
        "  &= \\sum_i^n \\left[1\\right] = n \\\\\n",
        "\\end{align}\n",
        "\\\\\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KIelEM56jn_"
      },
      "source": [
        "In Lecture 08 and 09, we discussed how the attention is computed in Transformer via the following equation,\n",
        "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V.$$\n",
        "**Problem 1.2** *(10 points)*  Suppose $Q$ and $K$ are matrices of independent variables each of which has a mean of $0$ and a variance of $1$. Using what you learned from Problem 1.1., show that\n",
        "$$\\text{Var}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) = 1.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wNoM7FtW0gi"
      },
      "source": [
        "**Answer 1.2** \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "  \\text{Var}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)\n",
        "  &= \\left(\\frac{1}{\\sqrt{d_k}}\\right)^2 \\text{Var}\\left(QK^\\top\\right)  \\quad \\because \\text{(2)} \\\\\n",
        "  &= \\frac{1}{d_k} \\text{Var}\\left(QK^\\top\\right) \\\\\n",
        "\\end{align}\n",
        "\\\\\n",
        "\\\\\n",
        "$$\n",
        "\n",
        "Then, we can focus on the one element of $QK$, $\\left(QK\\right)_{ij}$ without loss of generality.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "  \\frac{1}{d_k} \\text{Var}\\left(\\left(QK\\right)_{ij}^\\top\\right)\n",
        "  &= \\frac{1}{d_k} \\text{Var}\\left(\\sum_{t}^{d_k} \\left(Q_{it} K_{tj}\\right) \\right) \\\\\n",
        "  &= \\frac{1}{d_k} \\left(\\sum_{t}^{d_k} \\text{Var}\\left(Q_{it} K_{tj}\\right) \\right) \\quad \\because \\text{(1), } Q_{it} \\text{ and } Y_{tj} \\text{ are mutually independent} \\\\\n",
        "  &= \\frac{1}{d_k} \\left(d_k\\right) = 1 \\quad \\because \\text{Problem 1.1} \\\\\n",
        "\\end{align}\n",
        "\\\\\n",
        "$$\n",
        "\n",
        "Therefore, \n",
        "\n",
        "$$\n",
        "\\text{Var}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) = 1.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU3a3FEu6loq"
      },
      "source": [
        "\n",
        "**Problem 1.3** *(10 points)* What would happen if the assumption that the variance of $Q$ and $K$ is $1$ does not hold? Consider each case of it being higher and lower than $1$ and conjecture what it implies, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS4xjbWnmZL7"
      },
      "source": [
        "**Answer 1.3** \\\n",
        "\n",
        "If the variance of $Q_{ij}$ and $K_{ij}$ is higher than 1 for all i and j, then\n",
        "\n",
        "$$\n",
        "\\text{Var}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) > 1.\n",
        "$$\n",
        "\n",
        "Then, the variance of the output of the decoder becomes larger.\n",
        "If we use the softmax function on this output, then the final result might be \"too\" sharp. (Actually, this is not true, because of the residual connection) \\\n",
        "So, I guess that the model overfits faster than original model.\n",
        "\n",
        "\\\n",
        "\n",
        "Otherwse, if the variance of $Q_{ij}$ and $K_{ij}$ is lower than 1 for all i and j, then\n",
        "\n",
        "$$\n",
        "\\text{Var}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) < 1.\n",
        "$$\n",
        "\n",
        "Then, the variance of the output of the decoder becomes smaller.\n",
        "If we use the softmax function on this output, then the final result might be \"too\" smooth. \n",
        "\\\n",
        "So, I guess that the early training would be more unstable than normal although we use the bigger learning rate. The training time would be longer than the original version. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtGo8MAt7By2"
      },
      "source": [
        "## 2. Preprocessing SQuAD\n",
        "\n",
        "We will use `datasets` package offered by Hugging Face, which allows us to easily download various language datasets, including Stanford Question Answering Dataset (SQuAD).\n",
        "\n",
        "First, install the package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEGhK5tO8DcT",
        "outputId": "016f0fbf-bc70-44bc-db6c-7524238e056c"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/d6/a3d2c55b940a7c556e88f5598b401990805fc0f0a28b2fc9870cf0b8c761/datasets-1.6.0-py3-none-any.whl (202kB)\n",
            "\r\u001b[K     |█▋                              | 10kB 19.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 20kB 14.5MB/s eta 0:00:01\r\u001b[K     |████▉                           | 30kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 40kB 11.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 51kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 61kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 71kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 81kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 92kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 102kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 112kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 122kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 133kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 143kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 153kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 163kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 174kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 184kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 194kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 14.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 11.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: fsspec, huggingface-hub, xxhash, datasets\n",
            "Successfully installed datasets-1.6.0 fsspec-2021.4.0 huggingface-hub-0.0.8 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lrJLeSG8Xkl"
      },
      "source": [
        "Then, download SQuAD and print the first example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288,
          "referenced_widgets": [
            "ed599609dce646eeab6d7786eaf8f1ec",
            "d7da4b84388041fa827f2cea2a107f3b",
            "f1a259616a894fef8343cea8cd87b335",
            "94617f73c0aa483a892bde1920ff92b5",
            "8cc04860b3e6451db9c788de29d9379b",
            "c7be632c2bea41119ea8e99ebd9bd807",
            "010409b89be64017a6eed78ea3fc6cf1",
            "00502a4ce70649ba97e91dbf602befb3",
            "fd6abd63c4604f71939fc6fa7c0fa51d",
            "c726f319ff6c40518f500105b089885d",
            "4f5c9db535eb469a9c62017a830e5f0e",
            "1c55e3467ac64a48963ab3830dbe4146",
            "4a83705ce7634b309b5d60680fc54507",
            "c141b5c1e0bc404284872dfdacd69e87",
            "018be2efabc2460bb6b2827725dbca5c",
            "e07669929bb84117b62baea8621a1f36",
            "d6a260e01320497cbda223ac0fab49a7",
            "7a8013b37fdb419fa28233081b7684c0",
            "316af366649441a887f9cc5d7013f4c0",
            "a9d3d1b5c83d4cffb549b7ee1e63e925",
            "14485e6ed1be4da18f88182aad1dc212",
            "23da3f03d06b4367b7999cb5d6a776c2",
            "8365df8e631f47e9a686723c48e6e504",
            "7f43b4970ad244139d9033161a05d5b3",
            "81e5c183c1f3491180603685e74a547b",
            "de66fdc5a5b24c4c805d486fed4e92c0",
            "3d9c3938fc644278ac376f2a9bdaa272",
            "440b70f8371e4840b775f04a0439eb38",
            "7272bcfe6dd24f6ba2ddf8f23bd7ea67",
            "0c52797916a743b88a545e25687843d4",
            "d35e242cdb4942529cf1f859018c2482",
            "8f146d6b82e049b0a5fc7cbbf5428b49",
            "f4e2cb1eeb5a4205bf91b772d4cc5bd6",
            "581b5be899244074abc90c1dd795947a",
            "699e5192d1a24bc9b50ce4e1fb6faab5",
            "84a50413511d455f9a3c3afe064a2a42",
            "deac1b10c8b544fa9b45275d72dce116",
            "22bb5af80e374e9b9cbf017408ca432a",
            "ae9917c2decf4f3db9cf0e397b36820c",
            "08c2c496f27845cda4d198bee590206b",
            "2f3e91985c1a4ae793888cf3d810adbf",
            "682f00b78b7b414fa5a42a9395f6a7a7",
            "9bf772d2ce07413e9c7513f27a86cd25",
            "1b29487acf064b48af31fbd90764b9fc",
            "f34e28c67a4b4abf920606981746636b",
            "1d461e8fc3404098bb06fc2fa0d7d477",
            "d83ac70f127d453d82c1a134e0278a61",
            "b7ff39f63d4645e1ba01d46be33d3b2b"
          ]
        },
        "id": "ePc6IF9I8Jg1",
        "outputId": "33a32b3d-239d-4707-ce2b-5b954514b5ce"
      },
      "source": [
        "from datasets import load_dataset\n",
        "squad_dataset = load_dataset('squad')\n",
        "print(squad_dataset['train'][0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed599609dce646eeab6d7786eaf8f1ec",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1877.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd6abd63c4604f71939fc6fa7c0fa51d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=955.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.75 MiB, post-processed: Unknown size, total: 119.27 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6a260e01320497cbda223ac0fab49a7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=8116577.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81e5c183c1f3491180603685e74a547b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1054280.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4e2cb1eeb5a4205bf91b772d4cc5bd6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f3e91985c1a4ae793888cf3d810adbf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rDataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a. Subsequent calls will reuse this data.\n",
            "{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']}, 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'id': '5733be284776f41900661182', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'title': 'University_of_Notre_Dame'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIzhbD6a81Q6"
      },
      "source": [
        "Here, `answer_start` corresponds to the character-level start position of the answer, and `text` is the answer text itself. You will note that `answer_start` and `text` fields are given as lists but they only contain one item each. In fact, you can safely assume that this is the case for the training data. During evaluation, however, you will utilize several possible answers so that your evaluation can be compared against all of them. So your code need to handle multiple-answers case as well.\n",
        "\n",
        "As we discussed in Lecture 05, we want to formulate this task as a token classification problem. That is, we want to find which token of the context corresponds to the start position of the answer, and which corresponds to the end.\n",
        "\n",
        "**Problem 2.1** *(10 points)* Write `preprocess()` function that takes a SQuAD example as the input and outputs space-tokenized context and question, as well as the start and end token position of the answer if it has the answer field. That is, a pseudo code would look like:\n",
        "```python\n",
        "def preprocess(example):\n",
        "  out = {'context': ['each', 'token'], \n",
        "         'question': ['each', 'token']}\n",
        "  if 'answers' not in example:\n",
        "    return out\n",
        "  out['answers'] = [{'start': 3, 'end': 5}]\n",
        "  return out\n",
        "```\n",
        "Verify that this code works by comparing between the original answer text and the concatenation of the answer tokens from start to end in training data. Report the percentage of the questions that have exact match."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiBkG_u_1D2l"
      },
      "source": [
        "**Answer 2.1**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtoL89-n1H1a",
        "outputId": "626dc69f-1b35-4e4e-ba3e-cd2cad258ddd"
      },
      "source": [
        "def preprocess(example):\n",
        "    def tokenizer(sentence):\n",
        "        if sentence is None:\n",
        "            return list()\n",
        "        return sentence.split()\n",
        "\n",
        "    out = dict()\n",
        "    context = example['context']\n",
        "    question = example['question']\n",
        "\n",
        "    out['context'] = tokenizer(context)\n",
        "    out['question'] = tokenizer(question)\n",
        "\n",
        "    answer_list = list()\n",
        "    for answer_index in range(len(example['answers']['text'])):\n",
        "        answer = example['answers']['text'][answer_index]\n",
        "        answer_start = example['answers']['answer_start'][answer_index]\n",
        "        answer_end = answer_start + len(answer)\n",
        "\n",
        "        if (answer_start == 0 or context[answer_start-1] == ' ') \\\n",
        "            and (answer_end == len(context) or context[answer_end] == ' '):\n",
        "            n_tokens_before_answer = len(tokenizer(context[:answer_start]))\n",
        "            n_tokens_answer = len(tokenizer(answer))\n",
        "            answer_list.append({'start': n_tokens_before_answer, 'end': n_tokens_before_answer+n_tokens_answer-1})\n",
        "    \n",
        "    if not answer_list:\n",
        "        out['answers'] = answer_list\n",
        "\n",
        "    return out\n",
        "\n",
        "n_exact_match = 0.0\n",
        "for i, example in enumerate(squad_dataset['train']):\n",
        "    out = preprocess(example)\n",
        "    if 'answers' in out.keys():\n",
        "        n_exact_match += 1\n",
        "\n",
        "print(\"number of answers that have exact match : \", n_exact_match)\n",
        "print(\"number of training data : \", len(squad_dataset['train']))\n",
        "print(\"the percentage of the exact match (training) : \", n_exact_match / len(squad_dataset['train']))\n",
        "\n",
        "# number of answers that have exact match :  41616.0\n",
        "# number of training data :  87599\n",
        "# the percentage of the exact match :  0.47507391636890833 -> pretty low\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of answers that have exact match :  41616.0\n",
            "number of training data :  87599\n",
            "the percentage of the exact match (training) :  0.47507391636890833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wikI9OxVCL0E"
      },
      "source": [
        "We want to maximize the percentage of the exact match. You might see a low percentage however, due to bad tokenization. For instance, such space-based tokenization will fail to separate between \"world\" and \"!\" in \"hello world!\". \n",
        "\n",
        "**Problem 2.2** *(10 points)* Write an advanced tokenization model that always separates non-alphabet characters as independent tokens. For instance, \"hello1 world!!\" will be tokenized into \"hello\", \"1\", \"world\", \"!\", and \"!\". Using this new tokenizer, re-run the `preprocess` function and report the exact match percentage. How does the ratio change?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0SqpETMGUkW"
      },
      "source": [
        "**Answer 2.2**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UHzcv-7GWla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58191de8-5727-43c7-daf3-43859342ce05"
      },
      "source": [
        "def find_nonalpha_list(dataset):\n",
        "    nonalpha_list = list()\n",
        "    for example in dataset:\n",
        "        for c in example['context']:\n",
        "            if not c.isalpha() and c not in nonalpha_list:\n",
        "                nonalpha_list.append(c)\n",
        "        for c in example['question']:\n",
        "            if not c.isalpha() and c not in nonalpha_list:\n",
        "                nonalpha_list.append(c)\n",
        "    return nonalpha_list\n",
        "\n",
        "\n",
        "def preprocess(example, nonalpha_list):\n",
        "    def tokenizer(sentence, nonalpha_list):\n",
        "        if sentence is None:\n",
        "            return list()\n",
        "\n",
        "        for nonalpha_token in nonalpha_list:\n",
        "            sentence = sentence.replace(nonalpha_token, ' ' + nonalpha_token + ' ')\n",
        "\n",
        "        sentence = ' '.join(sentence.split())\n",
        "        return sentence.split()\n",
        "        \n",
        "    out = dict()\n",
        "    context = example['context']\n",
        "    question = example['question']\n",
        "    id = example['id']\n",
        "\n",
        "    out['context'] = tokenizer(context, nonalpha_list)\n",
        "    out['question'] = tokenizer(question, nonalpha_list)\n",
        "    out['id'] = id\n",
        "\n",
        "    answer_list = list()\n",
        "    for answer_index in range(len(example['answers']['text'])):\n",
        "        answer = example['answers']['text'][answer_index]\n",
        "        answer_start = example['answers']['answer_start'][answer_index]\n",
        "        answer_end = answer_start + len(answer)\n",
        "\n",
        "        if (answer_start == 0 or context[answer_start-1] in nonalpha_list) \\\n",
        "            and (answer_end == len(context) or context[answer_end] in nonalpha_list):\n",
        "            n_tokens_before_answer = len(tokenizer(context[:answer_start], nonalpha_list))\n",
        "            n_tokens_answer = len(tokenizer(answer, nonalpha_list))\n",
        "            answer_list.append({'start': n_tokens_before_answer, 'end': n_tokens_before_answer+n_tokens_answer-1})\n",
        "\n",
        "    if answer_list:\n",
        "        out['answers'] = answer_list\n",
        "\n",
        "    return out\n",
        "\n",
        "dataset = squad_dataset['train']\n",
        "# dataset = squad_dataset['validation'] # Do this if you want to check the valid dataset\n",
        "\n",
        "nonalpha_list = find_nonalpha_list(squad_dataset['train'])\n",
        "print(\"nonalpha_list : \", nonalpha_list)\n",
        "\n",
        "n_exact_match = 0.0\n",
        "for i, example in enumerate(dataset):\n",
        "  out = preprocess(example, nonalpha_list)\n",
        "  if 'answers' in out.keys():\n",
        "    n_exact_match += 1\n",
        "\n",
        "print(\"number of answers that have exact match : \", n_exact_match)\n",
        "print(\"number of training data : \", len(dataset))\n",
        "print(\"the percentage of the exact match (training) : \", n_exact_match / len(dataset))\n",
        "\n",
        "\n",
        "# nonalpha_list :  [',', ' ', '.', \"'\", '\"', '1', '8', '5', '(', '3', ')', '?', '-', '7', '6', '9', '2', '0', ';', '–', '&', '4', '%', '$', '[', ']', '/', ':', '#', '—', '!', '“', '’', '”', '<', '\\u200b', '̃', '£', '½', '+', '¢', '−', '°', '>', '€', '《', '》', '±', '~', '¥', '²', '❤', '=', '\\u200e', '͡', '́', '`', '्', 'ु', 'ः', 'ॊ', 'ि', 'ा', '\\u200d', '\\u200c', '*', '‘', '\\u3000', '•', '§', '⁄', '\\n', '̯', '̩', '…', '·', 'ָ', 'ִ', 'ׁ', 'ַ', 'ּ', 'ְ', 'ّ', '⟨', '◌', '⟩', '˭', '̤', '♠', '∅', '̞', '×', '̥', '′', '″', '\\ufeff', '_', 'ֿ', '´', '^', '̧', '̄', '→', '‑', '，', '₹', '\\u202f', '♯', '₂', '₥', '⁊', '\\u2009', '{', '}', '|', '@', '̪', '‚', '›', 'ׂ', 'ֵ', 'ِ', 'ْ', 'َ', '̍', '˥', '˨', '˩', '¡', '√', '¿', 'ာ', 'း', 'ُ', '≥', '˚', '≈', '⋅', 'ี', '︘', '�', '～', '〜', '̀', 'ོ', '་', '˧', 'ಾ', 'ು', '್', 'া', '্', 'ಿ', '∗', '∈', '≡', '∖', '№', '÷', 'ٔ', '¶', 'ิ', '₤', '♆', '⅓', '∝', '¼', 'ٍ', 'ֹ', '̌', '。', '̠', '₯']\n",
        "# number of answers that have exact match :  87108.0\n",
        "# number of training data :  87599\n",
        "# the percentage of the exact match (training) :  0.9943949131839405 -> now, it is OK\n",
        "\n",
        "# number of answers that have exact match :  10566.0\n",
        "# number of validation data :  10570\n",
        "# the percentage of the exact match (validation) :  0.9996215704824977 -> Also, it is OK\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nonalpha_list :  [',', ' ', '.', \"'\", '\"', '1', '8', '5', '(', '3', ')', '?', '-', '7', '6', '9', '2', '0', ';', '–', '&', '4', '%', '$', '[', ']', '/', ':', '#', '—', '!', '“', '’', '”', '<', '\\u200b', '̃', '£', '½', '+', '¢', '−', '°', '>', '€', '《', '》', '±', '~', '¥', '²', '❤', '=', '\\u200e', '͡', '́', '`', '्', 'ु', 'ः', 'ॊ', 'ि', 'ा', '\\u200d', '\\u200c', '*', '‘', '\\u3000', '•', '§', '⁄', '\\n', '̯', '̩', '…', '·', 'ָ', 'ִ', 'ׁ', 'ַ', 'ּ', 'ְ', 'ّ', '⟨', '◌', '⟩', '˭', '̤', '♠', '∅', '̞', '×', '̥', '′', '″', '\\ufeff', '_', 'ֿ', '´', '^', '̧', '̄', '→', '‑', '，', '₹', '\\u202f', '♯', '₂', '₥', '⁊', '\\u2009', '{', '}', '|', '@', '̪', '‚', '›', 'ׂ', 'ֵ', 'ِ', 'ْ', 'َ', '̍', '˥', '˨', '˩', '¡', '√', '¿', 'ာ', 'း', 'ُ', '≥', '˚', '≈', '⋅', 'ี', '︘', '�', '～', '〜', '̀', 'ོ', '་', '˧', 'ಾ', 'ು', '್', 'া', '্', 'ಿ', '∗', '∈', '≡', '∖', '№', '÷', 'ٔ', '¶', 'ิ', '₤', '♆', '⅓', '∝', '¼', 'ٍ', 'ֹ', '̌', '。', '̠', '₯']\n",
            "number of answers that have exact match :  87108.0\n",
            "number of training data :  87599\n",
            "the percentage of the exact match (training) :  0.9943949131839405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJtJXDSvDpE8"
      },
      "source": [
        "## 3. LSTM Baseline for SQuAD\n",
        "\n",
        "We will bring and reuse our model from Assignment 1. There are two key differences, however. First, we need to classify each token instead of the entire sentence. Second, we have two inputs (context and question) instead of just one.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX3hFmUAVD9K",
        "outputId": "c74643c4-3282-42fd-dc36-bb171a30587c"
      },
      "source": [
        "# My model from Assignment 1 is too slow to use it.\n",
        "# I will use torchtext and torch.nn.lstm in the assignment 2.\n",
        "\n",
        "import torchtext\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "from torchtext.legacy.data import BucketIterator\n",
        "\n",
        "\n",
        "class SQuAD1Dataset(data.Dataset):\n",
        "  \"\"\"\n",
        "  Defines a dataset for squad1.0.\n",
        "  \"\"\"\n",
        "  \n",
        "  @staticmethod\n",
        "  def sort_key(ex):\n",
        "    return data.interleave_keys(len(ex.context), len(ex.question))\n",
        "\n",
        "  def __init__(self, data_list, fields, use_bos=True, max_length=None, **kwargs):\n",
        "    if not isinstance(fields[0], (tuple, list)):\n",
        "      fields = [('context', fields[0]), \n",
        "                ('question', fields[1]), \n",
        "                # ('context_question', fields[2]), # For Problem 3.2+, put the question after the context\n",
        "                ('answer_start', fields[2]), \n",
        "                ('answer_end', fields[3]), \n",
        "                ('id_index', fields[4])\n",
        "                ]\n",
        "\n",
        "    examples = []\n",
        "    nonalpha_list = find_nonalpha_list(data_list)\n",
        "\n",
        "    self.id_list = list()\n",
        "    self.reference = list()\n",
        "\n",
        "    for _, example in enumerate(data_list):\n",
        "        out = preprocess(example, nonalpha_list)\n",
        "        # use data if the answer exists\n",
        "        if max_length and max_length < max(len(out['context']), len(out['question'])):\n",
        "            continue\n",
        "        if 'answers' in out.keys():\n",
        "            answer_start = out['answers'][0]['start'] # Use index 0 for instant valid accuracy\n",
        "            answer_end = out['answers'][0]['end'] # Use index 0 for instant valid accuracy\n",
        "            if use_bos:\n",
        "                answer_start += 1 # for <BOS> token\n",
        "                answer_end += 1 # for <BOS> token\n",
        "            examples.append(data.Example.fromlist([out['context'], \n",
        "                                                   out['question'], \n",
        "                                                #    out['context'] + [\"<CLS>\"] + out['question'], # Use <CLS> token\n",
        "                                                   answer_start,\n",
        "                                                   answer_end,\n",
        "                                                   len(self.id_list)], \n",
        "                                                  fields))\n",
        "            self.id_list.append(out['id'])\n",
        "            self.reference.append({'id':example['id'], 'answers':example['answers']})\n",
        "\n",
        "    super(SQuAD1Dataset, self).__init__(examples, fields, **kwargs)\n",
        "\n",
        "\n",
        "class SQuAD1Dataloader():\n",
        "  \"\"\"\n",
        "  Make the dataloader for SQuAD 1.0\n",
        "  \"\"\"\n",
        "  def __init__(self, train_data=None, valid_data=None, batch_size=64, device='cpu', \n",
        "                max_length=255, min_freq=2, fix_length=None,\n",
        "                use_bos=True, use_eos=True, shuffle=True\n",
        "              ):\n",
        "\n",
        "    super(SQuAD1Dataloader, self).__init__()\n",
        "\n",
        "    self.text = data.Field(sequential=True, use_vocab=True, batch_first=True, \n",
        "                           include_lengths=True, fix_length=fix_length, \n",
        "                           init_token='<BOS>' if use_bos else None, \n",
        "                           eos_token='<EOS>' if use_eos else None\n",
        "                          )\n",
        "    self.answer_start = data.Field(sequential = False, use_vocab = False)\n",
        "    self.answer_end = data.Field(sequential = False, use_vocab = False)\n",
        "    self.id_index = data.Field(sequential = False, use_vocab = False)\n",
        "    \n",
        "    train = SQuAD1Dataset(data_list=train_data, \n",
        "                          fields = [('context', self.text),\n",
        "                                    ('question', self.text),\n",
        "                                    # ('context_question', self.text),\n",
        "                                    ('answer_start', self.answer_start),\n",
        "                                    ('answer_end', self.answer_end),\n",
        "                                    ('id_index', self.id_index)\n",
        "                                    ], \n",
        "                          max_length = max_length\n",
        "                          )\n",
        "    valid = SQuAD1Dataset(data_list=valid_data, \n",
        "                          fields = [('context', self.text),\n",
        "                                    ('question', self.text),\n",
        "                                    # ('context_question', self.text),\n",
        "                                    ('answer_start', self.answer_start),\n",
        "                                    ('answer_end', self.answer_end),\n",
        "                                    ('id_index', self.id_index)\n",
        "                                    ], \n",
        "                          max_length = max_length\n",
        "                          )\n",
        "    self.train_id_list = train.id_list\n",
        "    self.valid_id_list = valid.id_list\n",
        "\n",
        "    self.train_reference = train.reference\n",
        "    self.valid_reference = valid.reference\n",
        "    \n",
        "    self.train_iter = data.BucketIterator(train, batch_size=batch_size,\n",
        "                                          device=device,\n",
        "                                          shuffle=shuffle,\n",
        "                                          sort_key=lambda x: len(x.question) + (max_length * len(x.context)), \n",
        "                                          sort_within_batch = True\n",
        "                                          )\n",
        "    self.valid_iter = data.BucketIterator(valid, batch_size=batch_size,\n",
        "                                          device=device,\n",
        "                                          shuffle=shuffle,\n",
        "                                          sort_key=lambda x: len(x.question) + (max_length * len(x.context)), \n",
        "                                          sort_within_batch = True\n",
        "                                          )\n",
        "    \n",
        "    self.text.build_vocab(train)\n",
        "\n",
        "\n",
        "train_dataset = squad_dataset['train']\n",
        "valid_dataset = squad_dataset['validation']\n",
        "\n",
        "print('# of train data : {}'.format(len(train_dataset)))\n",
        "print('# of vaild data : {}'.format(len(valid_dataset)))\n",
        "\n",
        "batch_size = 128\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "max_length = 255\n",
        "min_freq = 2\n",
        "use_bos = True\n",
        "use_eos = True\n",
        "print(\"device : \", device)\n",
        "\n",
        "loader = SQuAD1Dataloader(train_dataset, valid_dataset, batch_size=batch_size, \n",
        "                          device=device, max_length=max_length, min_freq=min_freq,\n",
        "                          use_bos=use_bos, use_eos=use_eos)\n",
        "print('\\nFinish making the dataloader')\n",
        "print(\"batch_size : \", batch_size)\n",
        "print(\"max_length : \", max_length)\n",
        "print('number of used train data ~ {}'.format((len(loader.train_iter)) * batch_size))\n",
        "print('number of used vaild data ~ {}'.format((len(loader.valid_iter)) * batch_size))\n",
        "\n",
        "vocab = loader.text.vocab\n",
        "vocab_list = list(vocab.stoi.keys())\n",
        "print('number of vocab : {}'.format(len(vocab_list)))\n",
        "\n",
        "# number of train data : 87599\n",
        "# number of vaild data : 10570\n",
        "# device :  cuda\n",
        "\n",
        "# Finish making the dataloader\n",
        "# batch_size :  128\n",
        "# max_length :  255\n",
        "# number of used train data ~ 81536\n",
        "# number of used vaild data ~ 9856\n",
        "# number of vocab : 86580"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of train data : 87599\n",
            "# of vaild data : 10570\n",
            "device :  cuda\n",
            "\n",
            "Finish making the dataloader\n",
            "batch_size :  128\n",
            "max_length :  255\n",
            "number of used train data ~ 81536\n",
            "number of used vaild data ~ 9856\n",
            "number of vocab : 86580\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhrgT3Xc11yw"
      },
      "source": [
        "\n",
        "Before resolving these differences, you will need to define your evaluation function to correctly evaluate how well your model is doing. Note that the evaluation was very straightforward in Assignment 1's sentiment classification (it is either positive or negative) while it is a bit complicated in SQuAD. We will use the evaluation function provided by `datasets`. You can access to it via the following code.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLMA7wXjHkxP"
      },
      "source": [
        "from datasets import load_metric\n",
        "squad_metric = load_metric('squad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7smOVxqXJxyB"
      },
      "source": [
        "You can also easily learn about how to use the function by simply typing the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yr7rl9ihHsdx",
        "outputId": "9c209127-09c8-461d-9a61-ba4ccdc9225d"
      },
      "source": [
        "squad_metric"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Metric(name: \"squad\", features: {'predictions': {'id': Value(dtype='string', id=None), 'prediction_text': Value(dtype='string', id=None)}, 'references': {'id': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}}, usage: \"\"\"\n",
              "Computes SQuAD scores (F1 and EM).\n",
              "Args:\n",
              "    predictions: List of question-answers dictionaries with the following key-values:\n",
              "        - 'id': id of the question-answer pair as given in the references (see below)\n",
              "        - 'prediction_text': the text of the answer\n",
              "    references: List of question-answers dictionaries with the following key-values:\n",
              "        - 'id': id of the question-answer pair (see above),\n",
              "        - 'answers': a Dict in the SQuAD dataset format\n",
              "            {\n",
              "                'text': list of possible texts for the answer, as a list of strings\n",
              "                'answer_start': list of start positions for the answer, as a list of ints\n",
              "            }\n",
              "            Note that answer_start values are not taken into account to compute the metric.\n",
              "Returns:\n",
              "    'exact_match': Exact match (the normalized answer exactly match the gold answer)\n",
              "    'f1': The F-score of predicted tokens versus the gold answer\n",
              "Examples:\n",
              "\n",
              "    >>> predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]\n",
              "    >>> references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]\n",
              "    >>> squad_metric = datasets.load_metric(\"squad\")\n",
              "    >>> results = squad_metric.compute(predictions=predictions, references=references)\n",
              "    >>> print(results)\n",
              "    {'exact_match': 100.0, 'f1': 100.0}\n",
              "\"\"\", stored examples: 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh72l3MtJW6D"
      },
      "source": [
        "**Problem 3.1** *(10 points)* Let's resolve the first issue here. Hence, for now, assume that your only input is context and you want to obtain the answer without seeing the question. While this may seem to be a non-sense, actually it can be considered as modeling the prior $\\text{Prob}(a|c)$ before observing $q$ (we ultimately want $\\text{Prob}(a|q,c)$). Transform your model into a token classification model by imposing $\\text{softmax}$ over the tokens instead of predefined classes. You will need to do this twice for each of start and end. Report the accuracy (using the metric above) on `squad_dataset['validation']`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PShlCxLcGai_"
      },
      "source": [
        "**Answer 3.1**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "738c408171c14571b0b0a61fb02904b9",
            "00ae909e7c38429bb414672d341535e2",
            "288ed80702a4499da42544444bd84cac",
            "f857138d0c414b0eaf9e0ee23cf1572c",
            "4642ba4f8522440e8c4ed70de8bbbd28",
            "aaa5adc4b96546a39c3499d0b4d64749",
            "7c1f578e61d24a29885a78dbc9ff2e2b",
            "e4d050990f5947429bf0aa153594c9e8"
          ]
        },
        "id": "Gtkm4TsgGcem",
        "outputId": "0775b044-1fba-4b47-a2a1-987b6f7f0067"
      },
      "source": [
        "import torch.nn as nn\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class ClassificationLSTMModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, n_layers, n_label, emb_dropout, rnn_dropout, bidirectional, enable_layer_norm, device):\n",
        "        super(ClassificationLSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
        "                            hidden_size=hidden_dim, \n",
        "                            num_layers=n_layers, \n",
        "                            dropout=rnn_dropout, \n",
        "                            bidirectional=bidirectional)\n",
        "      \n",
        "        n_direction = 2 if bidirectional else 1\n",
        "        self.fc_start = nn.Linear(hidden_dim*n_direction, n_label, bias=True)\n",
        "        self.fc_end = nn.Linear(hidden_dim*n_direction, n_label, bias=True)\n",
        "\n",
        "        # Layer_normalization\n",
        "        if enable_layer_norm:\n",
        "            self.enable_layer_norm = enable_layer_norm\n",
        "            self.emb_layer_norm = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
        "        self.fc_dropout = nn.Dropout(rnn_dropout)\n",
        "        self.bidirectional = bidirectional\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, input_tensor, src_seq_lens):\n",
        "        emb = self.embedding(input_tensor) # emb.shape = batch * len * hidden\n",
        "\n",
        "        # Layer_normalization\n",
        "        if self.enable_layer_norm:\n",
        "            emb = self.emb_layer_norm(emb)\n",
        "\n",
        "        emb = self.emb_dropout(emb)\n",
        "        emb = emb.transpose(0, 1) # emb.shape = len * batch * hidden\n",
        "\n",
        "        # n_direction = 2 if bidirectional else 1\n",
        "        # hidden = torch.zeros(n_layers*n_direction, context.shape[0], hidden_dim, requires_grad=True).to(self.device)\n",
        "        # cell = torch.zeros(n_layers*n_direction, context.shape[0], hidden_dim, requires_grad=True).to(self.device)\n",
        "\n",
        "        # nn.LSTM\n",
        "        packed = pack_padded_sequence(emb, src_seq_lens.tolist(), batch_first=False)\n",
        "        outs, (hidden, cell) = self.lstm(packed)\n",
        "        outs, out_lens = pad_packed_sequence(outs, batch_first=False)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            hidden = torch.stack([hidden[-2], hidden[-1]], dim=0)\n",
        "        else:\n",
        "            hidden = hidden[-1].unsqueeze(dim=0)\n",
        "        hidden = hidden.transpose(0, 1)\n",
        "        hidden = hidden.contiguous().view(hidden.shape[0], -1)\n",
        "\n",
        "        hidden = self.fc_dropout(hidden)\n",
        "        logits_start = self.fc_start(hidden)\n",
        "        logits_end = self.fc_end(hidden)\n",
        "        return (logits_start, logits_end)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device : \", device)\n",
        "\n",
        "# Vocabulary : Use vocab_list\n",
        "\n",
        "# Construct the LSTM Model\n",
        "embedding_dim = 128 # usually bigger, e.g. 128\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "n_label = max_length\n",
        "emb_dropout = 0.5\n",
        "rnn_dropout = 0.5\n",
        "bidirectional = True\n",
        "enable_layer_norm = True\n",
        "rnnmodel = ClassificationLSTMModel(embedding_dim, hidden_dim, n_layers, n_label, emb_dropout, rnn_dropout, bidirectional, enable_layer_norm, device).to(device)\n",
        "\n",
        "print(\"batch_size : \", batch_size)\n",
        "print(\"max_length : \", max_length)\n",
        "print(\"embedding_dim : \", embedding_dim)\n",
        "print(\"hidden_dim : \", hidden_dim)\n",
        "print(\"n_layers : \", n_layers)\n",
        "print(\"emb_dropout : \", emb_dropout)\n",
        "print(\"rnn_dropout_and_fc_dropout : \", rnn_dropout)\n",
        "if bidirectional:\n",
        "    print(\"bidirectional : True\")\n",
        "else:\n",
        "    print(\"bidirectional : False\")\n",
        "if enable_layer_norm:\n",
        "    print(\"enable_layer_norm : True\")\n",
        "else:\n",
        "    print(\"enable_layer_norm : False\")\n",
        "\n",
        "# Construct the data loader\n",
        "train_iter = loader.train_iter\n",
        "valid_iter = loader.valid_iter\n",
        "\n",
        "train_id_list = loader.train_id_list\n",
        "valid_id_list = loader.valid_id_list\n",
        "train_reference = loader.train_reference\n",
        "valid_reference = loader.valid_reference\n",
        "\n",
        "print(len(train_id_list), len(valid_id_list), len(train_reference), len(valid_reference))\n",
        "\n",
        "# Training\n",
        "learning_rate = 1e-3\n",
        "print(\"learning_rate : \", learning_rate)\n",
        "\n",
        "PAD_IDX = vocab.stoi['<pad>']\n",
        "cel = nn.CrossEntropyLoss(ignore_index=PAD_IDX) # Ignore Padding\n",
        "# optimizer = torch.optim.SGD(rnnmodel.parameters(), lr=1e-1)\n",
        "optimizer = torch.optim.Adam(rnnmodel.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 30\n",
        "max_norm = 5\n",
        "\n",
        "# Evaluate\n",
        "squad_metric = load_metric('squad')\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    train_loss = 0\n",
        "    train_accuracy = 0.0\n",
        "    train_data_num = 0\n",
        "    train_prediction = list()\n",
        "    for train_i, train_batch in enumerate(train_iter):\n",
        "        context, context_length = train_batch.context\n",
        "        question, question_length = train_batch.question # Unused\n",
        "        answer_start = train_batch.answer_start\n",
        "        answer_end = train_batch.answer_end\n",
        "        train_id_index = train_batch.id_index\n",
        "\n",
        "        logits_start, logits_end = rnnmodel(context, context_length)\n",
        "\n",
        "        optimizer.zero_grad() # reset process\n",
        "        loss = cel(logits_start, answer_start) + cel(logits_end, answer_end) # Loss, a.k.a L\n",
        "        loss.backward() # compute gradients\n",
        "        # print(torch.norm(rnnmodel.lstm.weight_hh_l0.grad), loss.item())\n",
        "        torch.nn.utils.clip_grad_norm_(rnnmodel.parameters(), max_norm) # gradent clipping\n",
        "        optimizer.step() # update parameters\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        _, train_start_preds = torch.max(logits_start, 1)\n",
        "        _, train_end_preds = torch.max(logits_end, 1)\n",
        "        train_accuracy += ((train_start_preds == answer_start) * (train_end_preds == answer_end)).sum().float()\n",
        "\n",
        "        train_data_num += context.shape[0]\n",
        "\n",
        "        # for train_j in range(context.shape[0]):\n",
        "        #     pred_text = \"\"\n",
        "        #     start = train_start_preds[train_j]\n",
        "        #     end = train_end_preds[train_j]\n",
        "        #     if start < end:\n",
        "        #         pred_text = [vocab_list[text_id] for text_id in context[train_j][start:end+1]]\n",
        "        #         pred_text = \" \".join(pred_text)\n",
        "        #     train_prediction.append({'id':train_id_list[train_id_index[train_j]], 'prediction_text':pred_text})\n",
        "\n",
        "    # train_result = squad_metric.compute(predictions=train_prediction, references=train_reference)\n",
        "    print('train:: Epoch:', '%04d' % (epoch + 1), \n",
        "          'cost =', '{:.6f},'.format(train_loss / train_data_num), \n",
        "          'argmax acc =', '{:.6f}'.format(train_accuracy / train_data_num),\n",
        "        #   'other_squad_metric : ', train_result\n",
        "          )\n",
        "        \n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        with torch.no_grad():\n",
        "            valid_loss = 0\n",
        "            valid_accuracy = 0.0\n",
        "            valid_data_num = 0\n",
        "            valid_prediction = list()\n",
        "            for valid_i, valid_batch in enumerate(valid_iter):\n",
        "                context, context_length = valid_batch.context\n",
        "                question, question_length = valid_batch.question # Unused\n",
        "                answer_start = valid_batch.answer_start\n",
        "                answer_end = valid_batch.answer_end\n",
        "                valid_id_index = valid_batch.id_index\n",
        "\n",
        "                logits_start, logits_end = rnnmodel(context, context_length)\n",
        "\n",
        "                loss = cel(logits_start, answer_start) + cel(logits_end, answer_end) # Loss, a.k.a L\n",
        "                valid_loss += loss.item()\n",
        "\n",
        "                _, valid_start_preds = torch.max(logits_start, 1)\n",
        "                _, valid_end_preds = torch.max(logits_end, 1)\n",
        "                valid_accuracy += ((valid_start_preds == answer_start) * (valid_end_preds == answer_end)).sum().float()\n",
        "\n",
        "                valid_data_num += context.shape[0]\n",
        "\n",
        "                for valid_j in range(context.shape[0]):\n",
        "                    pred_text = \"\"\n",
        "                    start = valid_start_preds[valid_j]\n",
        "                    end = valid_end_preds[valid_j]\n",
        "                    if start < end:\n",
        "                        pred_text = [vocab_list[text_id] for text_id in context[valid_j][start:end+1]]\n",
        "                        pred_text = \" \".join(pred_text)\n",
        "                    valid_prediction.append({'id':valid_id_list[valid_id_index[valid_j]], 'prediction_text':pred_text})\n",
        "                \n",
        "            valid_result = squad_metric.compute(predictions=valid_prediction, references=valid_reference)\n",
        "            print('valid:: Epoch:', '%04d' % (epoch + 1), \n",
        "                  'cost =', '{:.6f},'.format(valid_loss / valid_data_num), \n",
        "                  'argmax acc =', '{:.6f},'.format(valid_accuracy / valid_data_num),\n",
        "                  'other_squad_metric : ', valid_result)\n",
        "            \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device :  cuda\n",
            "batch_size :  128\n",
            "max_length :  255\n",
            "embedding_dim :  128\n",
            "hidden_dim :  256\n",
            "n_layers :  2\n",
            "emb_dropout :  0.5\n",
            "rnn_dropout_and_fc_dropout :  0.5\n",
            "bidirectional : True\n",
            "enable_layer_norm : True\n",
            "81507 9853 81507 9853\n",
            "learning_rate :  0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "738c408171c14571b0b0a61fb02904b9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZFVcZ0oTy8h"
      },
      "source": [
        "# Should match both the start index and the end index\n",
        "# Random :: 1/256 * 1/256 = 0.0000152587890625 Accuracy\n",
        "\n",
        "# device :  cuda\n",
        "# # of vocab : 86580\n",
        "# batch_size :  128\n",
        "# max_length :  255\n",
        "# embedding_dim :  128\n",
        "# hidden_dim :  256\n",
        "# n_layers :  2\n",
        "# bidirectional : True\n",
        "# learning_rate :  0.001\n",
        "\n",
        "# train:: Epoch: 0001 cost = 0.079758, acc = 0.004233\n",
        "# valid:: Epoch: 0001 cost = 0.079524, acc = 0.007123\n",
        "# train:: Epoch: 0002 cost = 0.079234, acc = 0.010294\n",
        "# valid:: Epoch: 0002 cost = 0.079435, acc = 0.010786\n",
        "# train:: Epoch: 0003 cost = 0.079000, acc = 0.014048\n",
        "# valid:: Epoch: 0003 cost = 0.079326, acc = 0.014652\n",
        "# train:: Epoch: 0004 cost = 0.078740, acc = 0.016686\n",
        "# valid:: Epoch: 0004 cost = 0.079321, acc = 0.015263\n",
        "# train:: Epoch: 0005 cost = 0.078444, acc = 0.018661\n",
        "# valid:: Epoch: 0005 cost = 0.079526, acc = 0.015975\n",
        "# train:: Epoch: 0006 cost = 0.078154, acc = 0.020550\n",
        "# valid:: Epoch: 0006 cost = 0.079506, acc = 0.016992\n",
        "# train:: Epoch: 0007 cost = 0.077840, acc = 0.021728\n",
        "# valid:: Epoch: 0007 cost = 0.079699, acc = 0.018519\n",
        "# train:: Epoch: 0008 cost = 0.077550, acc = 0.023434\n",
        "# valid:: Epoch: 0008 cost = 0.079824, acc = 0.019129\n",
        "# train:: Epoch: 0009 cost = 0.077251, acc = 0.024611\n",
        "# valid:: Epoch: 0009 cost = 0.079951, acc = 0.018315\n",
        "# train:: Epoch: 0010 cost = 0.076993, acc = 0.024722\n",
        "# valid:: Epoch: 0010 cost = 0.080069, acc = 0.019333 -> max valid acc ~1/50\n",
        "# train:: Epoch: 0011 cost = 0.076738, acc = 0.026550\n",
        "# valid:: Epoch: 0011 cost = 0.080116, acc = 0.018620"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr15BA5QKqTd"
      },
      "source": [
        "**Problem 3.2** *(10 points)*  Now let's resolve the second issue, by simply concatenating the two inputs into one sequence. The simplest way would be to append the the question at the start *OR* the end of the context. If you put it at the start, you will need to shift the start and the end positions of the answer accordingly. If you put it at the end, it will be necesary to use bidirectional LSTM for the context to be aware of what is ahead (though it is recommended to use bidirectional LSTM even if the question is appended at the start). Whichever you choose, carry it out and report the accuracy. How does it differ from 3.1?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_nAqQFEZ55R"
      },
      "source": [
        "# Put the question sentence before the context sentence (for Prob 3.2+)\n",
        "# Use <CLS> token to separate two sentences\n",
        "\n",
        "import torchtext\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "from torchtext.legacy.data import BucketIterator\n",
        "\n",
        "\n",
        "class SQuAD2Dataset(data.Dataset):\n",
        "  \"\"\"\n",
        "  Defines a dataset for squad1.0.\n",
        "  \"\"\"\n",
        "  \n",
        "  @staticmethod\n",
        "  def sort_key(ex):\n",
        "    return data.interleave_keys(len(ex.context_question))\n",
        "\n",
        "  def __init__(self, data_list, fields, use_bos=True, max_length=None, **kwargs):\n",
        "    if not isinstance(fields[0], (tuple, list)):\n",
        "      fields = [\n",
        "                # ('context', fields[0]), \n",
        "                # ('question', fields[1]), \n",
        "                ('context_question', fields[0]), # For Problem 3.2+, put the question after the context\n",
        "                ('answer_start', fields[1]), \n",
        "                ('answer_end', fields[2]), \n",
        "                ('id_index', fields[3])\n",
        "                ]\n",
        "\n",
        "    examples = []\n",
        "    nonalpha_list = find_nonalpha_list(data_list)\n",
        "\n",
        "    self.id_list = list()\n",
        "    self.reference = list()\n",
        "\n",
        "    for _, example in enumerate(data_list):\n",
        "        out = preprocess(example, nonalpha_list)\n",
        "        # use data if the answer exists\n",
        "        if max_length and max_length < max(len(out['context']), len(out['question'])):\n",
        "            continue\n",
        "        if 'answers' in out.keys():\n",
        "            answer_start = out['answers'][0]['start'] # Use index 0 for instant valid accuracy\n",
        "            answer_end = out['answers'][0]['end'] # Use index 0 for instant valid accuracy\n",
        "            if use_bos:\n",
        "                answer_start += 1 # for <BOS> token\n",
        "                answer_end += 1 # for <BOS> token\n",
        "            examples.append(data.Example.fromlist([\n",
        "                                                #    out['context'], \n",
        "                                                #    out['question'], \n",
        "                                                   out['context'] + [\"<CLS>\"] + out['question'], # Use <CLS> token\n",
        "                                                   answer_start,\n",
        "                                                   answer_end,\n",
        "                                                   len(self.id_list)], \n",
        "                                                  fields))\n",
        "            self.id_list.append(out['id'])\n",
        "            self.reference.append({'id':example['id'], 'answers':example['answers']})\n",
        "\n",
        "    super(SQuAD2Dataset, self).__init__(examples, fields, **kwargs)\n",
        "\n",
        "\n",
        "class SQuAD2Dataloader():\n",
        "  \"\"\"\n",
        "  Make the dataloader for SQuAD 1.0\n",
        "  \"\"\"\n",
        "  def __init__(self, train_data=None, valid_data=None, batch_size=64, device='cpu', \n",
        "                max_length=255, min_freq=2, fix_length=None,\n",
        "                use_bos=True, use_eos=True, shuffle=True\n",
        "              ):\n",
        "\n",
        "    super(SQuAD2Dataloader, self).__init__()\n",
        "\n",
        "    self.text = data.Field(sequential=True, use_vocab=True, batch_first=True, \n",
        "                           include_lengths=True, fix_length=fix_length, \n",
        "                           init_token='<BOS>' if use_bos else None, \n",
        "                           eos_token='<EOS>' if use_eos else None\n",
        "                          )\n",
        "    self.answer_start = data.Field(sequential = False, use_vocab = False)\n",
        "    self.answer_end = data.Field(sequential = False, use_vocab = False)\n",
        "    self.id_index = data.Field(sequential = False, use_vocab = False)\n",
        "    \n",
        "    train = SQuAD2Dataset(data_list=train_data, \n",
        "                          fields = [\n",
        "                                    # ('context', self.text),\n",
        "                                    # ('question', self.text),\n",
        "                                    ('context_question', self.text),\n",
        "                                    ('answer_start', self.answer_start),\n",
        "                                    ('answer_end', self.answer_end),\n",
        "                                    ('id_index', self.id_index)\n",
        "                                    ], \n",
        "                          max_length = max_length\n",
        "                          )\n",
        "    valid = SQuAD2Dataset(data_list=valid_data, \n",
        "                          fields = [\n",
        "                                    # ('context', self.text),\n",
        "                                    # ('question', self.text),\n",
        "                                    ('context_question', self.text),\n",
        "                                    ('answer_start', self.answer_start),\n",
        "                                    ('answer_end', self.answer_end),\n",
        "                                    ('id_index', self.id_index)\n",
        "                                    ], \n",
        "                          max_length = max_length\n",
        "                          )\n",
        "    self.train_id_list = train.id_list\n",
        "    self.valid_id_list = valid.id_list\n",
        "\n",
        "    self.train_reference = train.reference\n",
        "    self.valid_reference = valid.reference\n",
        "    \n",
        "    self.train_iter = data.BucketIterator(train, batch_size=batch_size,\n",
        "                                          device=device,\n",
        "                                          shuffle=shuffle,\n",
        "                                          sort_key=lambda x: len(x.context_question), \n",
        "                                          sort_within_batch = True\n",
        "                                          )\n",
        "    self.valid_iter = data.BucketIterator(valid, batch_size=batch_size,\n",
        "                                          device=device,\n",
        "                                          shuffle=shuffle,\n",
        "                                          sort_key=lambda x: len(x.context_question),\n",
        "                                          sort_within_batch = True\n",
        "                                          )\n",
        "    \n",
        "    self.text.build_vocab(train)\n",
        "\n",
        "\n",
        "train_dataset = squad_dataset['train']\n",
        "valid_dataset = squad_dataset['validation']\n",
        "\n",
        "print('# of train data : {}'.format(len(train_dataset)))\n",
        "print('# of vaild data : {}'.format(len(valid_dataset)))\n",
        "\n",
        "batch_size = 128\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "max_length = 255\n",
        "min_freq = 2\n",
        "use_bos = True\n",
        "use_eos = True\n",
        "print(\"device : \", device)\n",
        "\n",
        "loader = SQuAD2Dataloader(train_dataset, valid_dataset, batch_size=batch_size, \n",
        "                          device=device, max_length=max_length, min_freq=min_freq,\n",
        "                          use_bos=use_bos, use_eos=use_eos)\n",
        "print('\\nFinish making the dataloader')\n",
        "print(\"batch_size : \", batch_size)\n",
        "print(\"max_length : \", max_length)\n",
        "print('# of used train data ~ {}'.format((len(loader.train_iter)) * batch_size))\n",
        "print('# of used vaild data ~ {}'.format((len(loader.valid_iter)) * batch_size))\n",
        "\n",
        "vocab = loader.text.vocab\n",
        "vocab_list = list(vocab.stoi.keys())\n",
        "print('# of vocab : {}'.format(len(vocab_list)))\n",
        "\n",
        "# # of train data : 87599\n",
        "# # of vaild data : 10570\n",
        "# device :  cuda\n",
        "\n",
        "# Finish making the dataloader\n",
        "# batch_size :  128\n",
        "# max_length :  255\n",
        "# # of used train data ~ 81536\n",
        "# # of used vaild data ~ 9856\n",
        "# # of vocab : 86580"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXpHPdDLGdd0"
      },
      "source": [
        "**Answer 3.2**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGstR1-CGfr6"
      },
      "source": [
        "import torch.nn as nn\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device : \", device)\n",
        "\n",
        "# Vocabulary : Use vocab_list\n",
        "\n",
        "# Construct the LSTM Model\n",
        "embedding_dim = 128 # usually bigger, e.g. 128\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "n_label = max_length\n",
        "emb_dropout = 0.5\n",
        "rnn_dropout = 0.5\n",
        "bidirectional = True\n",
        "enable_layer_norm = True\n",
        "rnnmodel = ClassificationLSTMModel(embedding_dim, hidden_dim, n_layers, n_label, emb_dropout, rnn_dropout, bidirectional, enable_layer_norm, device).to(device)\n",
        "\n",
        "print(\"batch_size : \", batch_size)\n",
        "print(\"max_length : \", max_length)\n",
        "print(\"embedding_dim : \", embedding_dim)\n",
        "print(\"hidden_dim : \", hidden_dim)\n",
        "print(\"n_layers : \", n_layers)\n",
        "print(\"emb_dropout : \", emb_dropout)\n",
        "print(\"rnn_dropout_and_fc_dropout : \", rnn_dropout)\n",
        "if bidirectional:\n",
        "    print(\"bidirectional : True\")\n",
        "else:\n",
        "    print(\"bidirectional : False\")\n",
        "if enable_layer_norm:\n",
        "    print(\"enable_layer_norm : True\")\n",
        "else:\n",
        "    print(\"enable_layer_norm : False\")\n",
        "\n",
        "# Construct the data loader\n",
        "train_iter = loader.train_iter\n",
        "valid_iter = loader.valid_iter\n",
        "\n",
        "train_id_list = loader.train_id_list\n",
        "valid_id_list = loader.valid_id_list\n",
        "train_reference = loader.train_reference\n",
        "valid_reference = loader.valid_reference\n",
        "\n",
        "print(len(train_id_list), len(valid_id_list), len(train_reference), len(valid_reference))\n",
        "\n",
        "# Training\n",
        "learning_rate = 1e-3\n",
        "print(\"learning_rate : \", learning_rate)\n",
        "\n",
        "PAD_IDX = vocab.stoi['<pad>']\n",
        "cel = nn.CrossEntropyLoss(ignore_index=PAD_IDX) # Ignore Padding\n",
        "# optimizer = torch.optim.SGD(rnnmodel.parameters(), lr=1e-1)\n",
        "optimizer = torch.optim.Adam(rnnmodel.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 30\n",
        "max_norm = 5\n",
        "\n",
        "# Evaluate\n",
        "squad_metric = load_metric('squad')\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    train_loss = 0\n",
        "    train_accuracy = 0.0\n",
        "    train_data_num = 0\n",
        "    train_prediction = list()\n",
        "    for train_i, train_batch in enumerate(train_iter):\n",
        "        context, context_length = train_batch.context_question\n",
        "        answer_start = train_batch.answer_start\n",
        "        answer_end = train_batch.answer_end\n",
        "        train_id_index = train_batch.id_index\n",
        "\n",
        "        logits_start, logits_end = rnnmodel(context, context_length)\n",
        "\n",
        "        optimizer.zero_grad() # reset process\n",
        "        loss = cel(logits_start, answer_start) + cel(logits_end, answer_end) # Loss, a.k.a L\n",
        "        loss.backward() # compute gradients\n",
        "        # print(torch.norm(rnnmodel.lstm.weight_hh_l0.grad), loss.item())\n",
        "        torch.nn.utils.clip_grad_norm_(rnnmodel.parameters(), max_norm) # gradent clipping\n",
        "        optimizer.step() # update parameters\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        _, train_start_preds = torch.max(logits_start, 1)\n",
        "        _, train_end_preds = torch.max(logits_end, 1)\n",
        "        train_accuracy += ((train_start_preds == answer_start) * (train_end_preds == answer_end)).sum().float()\n",
        "\n",
        "        train_data_num += context.shape[0]\n",
        "\n",
        "        for train_j in range(context.shape[0]):\n",
        "            pred_text = \"\"\n",
        "            start = train_start_preds[train_j]\n",
        "            end = train_end_preds[train_j]\n",
        "            if start < end:\n",
        "                pred_text = [vocab_list[text_id] for text_id in context[train_j][start:end+1]]\n",
        "                pred_text = \" \".join(pred_text)\n",
        "            train_prediction.append({'id':train_id_list[train_id_index[train_j]], 'prediction_text':pred_text})\n",
        "\n",
        "    train_result = squad_metric.compute(predictions=train_prediction, references=train_reference)\n",
        "    print('train:: Epoch:', '%04d' % (epoch + 1), \n",
        "          'cost =', '{:.6f},'.format(train_loss / train_data_num), \n",
        "        #   'argmax acc =', '{:.6f}'.format(train_accuracy / train_data_num),\n",
        "          'other_squad_metric : ', train_result)\n",
        "        \n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        with torch.no_grad():\n",
        "            valid_loss = 0\n",
        "            valid_accuracy = 0.0\n",
        "            valid_data_num = 0\n",
        "            valid_prediction = list()\n",
        "            for valid_i, valid_batch in enumerate(valid_iter):\n",
        "                context, context_length = valid_batch.context_question\n",
        "                question, question_length = valid_batch.question # Unused\n",
        "                answer_start = valid_batch.answer_start\n",
        "                answer_end = valid_batch.answer_end\n",
        "                valid_id_index = valid_batch.id_index\n",
        "\n",
        "                logits_start, logits_end = rnnmodel(context, context_length)\n",
        "\n",
        "                loss = cel(logits_start, answer_start) + cel(logits_end, answer_end) # Loss, a.k.a L\n",
        "                valid_loss += loss.item()\n",
        "\n",
        "                _, valid_start_preds = torch.max(logits_start, 1)\n",
        "                _, valid_end_preds = torch.max(logits_end, 1)\n",
        "                valid_accuracy += ((valid_start_preds == answer_start) * (valid_end_preds == answer_end)).sum().float()\n",
        "\n",
        "                valid_data_num += context.shape[0]\n",
        "\n",
        "                for valid_j in range(context.shape[0]):\n",
        "                    pred_text = \"\"\n",
        "                    start = valid_start_preds[valid_j]\n",
        "                    end = valid_end_preds[valid_j]\n",
        "                    if start < end:\n",
        "                        pred_text = [vocab_list[text_id] for text_id in context[valid_j][start:end+1]]\n",
        "                        pred_text = \" \".join(pred_text)\n",
        "                    valid_prediction.append({'id':valid_id_list[valid_id_index[valid_j]], 'prediction_text':pred_text})\n",
        "                \n",
        "            valid_result = squad_metric.compute(predictions=valid_prediction, references=valid_reference)\n",
        "            print('valid:: Epoch:', '%04d' % (epoch + 1), \n",
        "                  'cost =', '{:.6f},'.format(valid_loss / valid_data_num), \n",
        "                #   'argmax acc =', '{:.6f},'.format(valid_accuracy / valid_data_num),\n",
        "                  'other_squad_metric : ', valid_result)\n",
        "            \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEjTZSqGuAdy"
      },
      "source": [
        "device :  cuda\n",
        "# of vocab : 86580\n",
        "batch_size :  128\n",
        "max_length :  255\n",
        "embedding_dim :  256\n",
        "hidden_dim :  512\n",
        "n_layers :  2\n",
        "bidirectional : True\n",
        "learning_rate :  0.001\n",
        "13%\n",
        "13/100 [33:49<3:46:29, 156.21s/it]\n",
        "train:: Epoch: 0001 cost = 0.079619, acc = 0.006944\n",
        "valid:: Epoch: 0001 cost = 0.079696, acc = 0.012922\n",
        "train:: Epoch: 0002 cost = 0.079165, acc = 0.013582\n",
        "valid:: Epoch: 0002 cost = 0.079346, acc = 0.015466\n",
        "train:: Epoch: 0003 cost = 0.078792, acc = 0.017164\n",
        "valid:: Epoch: 0003 cost = 0.079322, acc = 0.016280\n",
        "train:: Epoch: 0004 cost = 0.078353, acc = 0.020182\n",
        "valid:: Epoch: 0004 cost = 0.079481, acc = 0.017908\n",
        "train:: Epoch: 0005 cost = 0.077817, acc = 0.022685\n",
        "valid:: Epoch: 0005 cost = 0.079363, acc = 0.019638\n",
        "train:: Epoch: 0006 cost = 0.077266, acc = 0.025286\n",
        "valid:: Epoch: 0006 cost = 0.079417, acc = 0.019027\n",
        "train:: Epoch: 0007 cost = 0.076714, acc = 0.026452\n",
        "valid:: Epoch: 0007 cost = 0.079670, acc = 0.018722\n",
        "train:: Epoch: 0008 cost = 0.076170, acc = 0.026881\n",
        "valid:: Epoch: 0008 cost = 0.080020, acc = 0.018112\n",
        "train:: Epoch: 0009 cost = 0.075587, acc = 0.029507\n",
        "valid:: Epoch: 0009 cost = 0.080270, acc = 0.020350\n",
        "train:: Epoch: 0010 cost = 0.075055, acc = 0.029605\n",
        "valid:: Epoch: 0010 cost = 0.080645, acc = 0.020045\n",
        "train:: Epoch: 0011 cost = 0.074553, acc = 0.031519\n",
        "valid:: Epoch: 0011 cost = 0.080962, acc = 0.017806\n",
        "train:: Epoch: 0012 cost = 0.074054, acc = 0.032329\n",
        "valid:: Epoch: 0012 cost = 0.081124, acc = 0.019129\n",
        "train:: Epoch: 0013 cost = 0.073636, acc = 0.032574\n",
        "valid:: Epoch: 0013 cost = 0.081502, acc = 0.018315"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKk9QKEz4qnN"
      },
      "source": [
        "device :  cuda\n",
        "# of vocab : 86580\n",
        "batch_size :  128\n",
        "max_length :  255\n",
        "embedding_dim :  128\n",
        "hidden_dim :  256\n",
        "n_layers :  2\n",
        "bidirectional : True\n",
        "learning_rate :  0.001\n",
        "23%\n",
        "23/100 [20:52<1:09:54, 54.47s/it]\n",
        "train:: Epoch: 0001 cost = 0.079600, acc = 0.007619\n",
        "valid:: Epoch: 0001 cost = 0.079375, acc = 0.015059\n",
        "train:: Epoch: 0002 cost = 0.078746, acc = 0.017029\n",
        "valid:: Epoch: 0002 cost = 0.079089, acc = 0.017094\n",
        "train:: Epoch: 0003 cost = 0.077435, acc = 0.024035\n",
        "valid:: Epoch: 0003 cost = 0.079096, acc = 0.020452\n",
        "train:: Epoch: 0004 cost = 0.074976, acc = 0.032267\n",
        "valid:: Epoch: 0004 cost = 0.080308, acc = 0.020045\n",
        "train:: Epoch: 0005 cost = 0.070933, acc = 0.041861\n",
        "valid:: Epoch: 0005 cost = 0.082924, acc = 0.015263\n",
        "train:: Epoch: 0006 cost = 0.064731, acc = 0.058449\n",
        "valid:: Epoch: 0006 cost = 0.087871, acc = 0.012210\n",
        "train:: Epoch: 0007 cost = 0.056475, acc = 0.090802\n",
        "valid:: Epoch: 0007 cost = 0.094999, acc = 0.008140\n",
        "train:: Epoch: 0008 cost = 0.047037, acc = 0.147779\n",
        "valid:: Epoch: 0008 cost = 0.104818, acc = 0.007326\n",
        "train:: Epoch: 0009 cost = 0.037530, acc = 0.239709\n",
        "valid:: Epoch: 0009 cost = 0.117320, acc = 0.005698\n",
        "train:: Epoch: 0010 cost = 0.028805, acc = 0.358301\n",
        "valid:: Epoch: 0010 cost = 0.130119, acc = 0.005393\n",
        "train:: Epoch: 0011 cost = 0.021250, acc = 0.492866\n",
        "valid:: Epoch: 0011 cost = 0.140989, acc = 0.004375\n",
        "train:: Epoch: 0012 cost = 0.015138, acc = 0.627100\n",
        "valid:: Epoch: 0012 cost = 0.153281, acc = 0.005698\n",
        "train:: Epoch: 0013 cost = 0.010398, acc = 0.749727\n",
        "valid:: Epoch: 0013 cost = 0.163834, acc = 0.003867\n",
        "train:: Epoch: 0014 cost = 0.007055, acc = 0.842112\n",
        "valid:: Epoch: 0014 cost = 0.173189, acc = 0.004375\n",
        "train:: Epoch: 0015 cost = 0.004955, acc = 0.898868\n",
        "valid:: Epoch: 0015 cost = 0.181411, acc = 0.003663\n",
        "train:: Epoch: 0016 cost = 0.003635, acc = 0.932202\n",
        "valid:: Epoch: 0016 cost = 0.189040, acc = 0.003968\n",
        "train:: Epoch: 0017 cost = 0.003136, acc = 0.939085\n",
        "valid:: Epoch: 0017 cost = 0.194765, acc = 0.004172\n",
        "train:: Epoch: 0018 cost = 0.003493, acc = 0.920534\n",
        "valid:: Epoch: 0018 cost = 0.198848, acc = 0.003765\n",
        "train:: Epoch: 0019 cost = 0.003538, acc = 0.912314\n",
        "valid:: Epoch: 0019 cost = 0.204216, acc = 0.003663\n",
        "train:: Epoch: 0020 cost = 0.002579, acc = 0.941907\n",
        "valid:: Epoch: 0020 cost = 0.209405, acc = 0.003460\n",
        "train:: Epoch: 0021 cost = 0.001781, acc = 0.964163\n",
        "valid:: Epoch: 0021 cost = 0.214543, acc = 0.004274\n",
        "train:: Epoch: 0022 cost = 0.001338, acc = 0.976125\n",
        "valid:: Epoch: 0022 cost = 0.218680, acc = 0.004172\n",
        "train:: Epoch: 0023 cost = 0.001490, acc = 0.968776\n",
        "valid:: Epoch: 0023 cost = 0.221427, acc = 0.004375"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LggHWu2hjPrc"
      },
      "source": [
        "device :  cuda\n",
        "# of vocab : 86580\n",
        "batch_size :  128\n",
        "max_length :  255\n",
        "embedding_dim :  128\n",
        "hidden_dim :  256\n",
        "n_layers :  2\n",
        "bidirectional : True\n",
        "learning_rate :  0.001\n",
        "11%\n",
        "11/100 [13:27<1:48:54, 73.42s/it]\n",
        "train:: Epoch: 0001 cost = 0.079707, acc = 0.005570\n",
        "valid:: Epoch: 0001 cost = 0.079532, acc = 0.007835\n",
        "train:: Epoch: 0002 cost = 0.079195, acc = 0.010968\n",
        "valid:: Epoch: 0002 cost = 0.079389, acc = 0.014042\n",
        "train:: Epoch: 0003 cost = 0.078919, acc = 0.014490\n",
        "valid:: Epoch: 0003 cost = 0.079269, acc = 0.014449\n",
        "train:: Epoch: 0004 cost = 0.078588, acc = 0.016992\n",
        "valid:: Epoch: 0004 cost = 0.079284, acc = 0.017705\n",
        "train:: Epoch: 0005 cost = 0.078201, acc = 0.019238\n",
        "valid:: Epoch: 0005 cost = 0.079297, acc = 0.016585\n",
        "train:: Epoch: 0006 cost = 0.077760, acc = 0.020992\n",
        "valid:: Epoch: 0006 cost = 0.079323, acc = 0.017806\n",
        "train:: Epoch: 0007 cost = 0.077337, acc = 0.022182\n",
        "valid:: Epoch: 0007 cost = 0.079581, acc = 0.016077\n",
        "train:: Epoch: 0008 cost = 0.076933, acc = 0.023213\n",
        "valid:: Epoch: 0008 cost = 0.079590, acc = 0.019333\n",
        "train:: Epoch: 0009 cost = 0.076559, acc = 0.024857\n",
        "valid:: Epoch: 0009 cost = 0.079766, acc = 0.018824\n",
        "train:: Epoch: 0010 cost = 0.076169, acc = 0.025556\n",
        "valid:: Epoch: 0010 cost = 0.079945, acc = 0.018315\n",
        "train:: Epoch: 0011 cost = 0.075802, acc = 0.026734\n",
        "valid:: Epoch: 0011 cost = 0.080164, acc = 0.017806"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76NH5MK7L1OG"
      },
      "source": [
        "## 4. LSTM + Attention for SQuAD\n",
        "\n",
        "**Problem 4.1** *(20 points)* Here, we will be appending an attention layer on top of LSTM outputs. We will use a single-head attention sublayer from Transformer. That is, you will implement \n",
        "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d}}\\right)V,$$\n",
        "where $Q, K, V$ is obtained by the linear transformation of the hidden states of the LSTM outputs $H$, i.e. $Q = HW^Q, K=HW^K, V=HW^V$ ($W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d}$ are trainable weights). Note that the output of $\\text{Attention}$ layer has the same dimension as $H$, so you can directly append your token classification layer on top of it. Report the accuracy and compare it with 3.2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoL0X9C2Gi0v"
      },
      "source": [
        "**Answer 4.1**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAvP_sFiGkYj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT_N1lJqQFY-"
      },
      "source": [
        "**Problem 4.2** *(10 points)* On top of the attention layer, let's add another layer of (bi-directional) LSTM. So this will look like a *sandwich* where the LSTM is bread and the attention is ham. How does it affect the accuracy? Explain why do you think this happens. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yi557rwGlSG"
      },
      "source": [
        "**Answer 4.2**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJv7a2fiGmTe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN6XQLYJRrR6"
      },
      "source": [
        "## 5. Attention is All You Need\n",
        "\n",
        "**Problem 5.1 (bonus)** *(20 points)*  Implement full Transformer encoder to entirely replace LSTMs. You are allowed to copy and paste code from [*Annotated Transformer*](https://nlp.seas.harvard.edu/2018/04/03/attention.html) (but nowhere else). Report the accuracy and explain what seems to happening with attetion-only model compared to LSTM+Attention model(s). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPmcwOtUJg6f"
      },
      "source": [
        "**Answer 5.1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGhCnK0WJjst"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jeE1MaYJej5"
      },
      "source": [
        "\n",
        "**Problem 5.2 (bonus)** *(10 points)* Replace Transformer's sinusoidal position encoding with a fixed-length (of 256) position embedding. That is, you will create a 256-by-$d$ trainable parameter matrix for the position encoding that replaces the variable-length sinusoidal encoding. What is the clear disdvantage of this approach? Report the accuracy and compare it with 5.1. Note that this also has a clear advantage, as we will see in our future lecture on Pretrained Language Model, and more specifically, BERT (Devlin et al., 2018)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zesTXw0GpK2"
      },
      "source": [
        "**Answer 5.2**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_-YXpobIfUP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}