{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "KAIST AI605 Assignment 2_20194364.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f8ef456b234d4b8391a2e58acdc2db07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1b30d21737f4462f81460aacc4e28beb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f1772891790e46fa8c43e2d0dafa3258",
              "IPY_MODEL_c96bfea6e1a44dcb9f8cfa4acada2126"
            ]
          }
        },
        "1b30d21737f4462f81460aacc4e28beb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1772891790e46fa8c43e2d0dafa3258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_152c00e3d03244c7a6e59f72bb51be4e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 30,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 30,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c4e1165c7cee431b9551e8c7297173c5"
          }
        },
        "c96bfea6e1a44dcb9f8cfa4acada2126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5038c98bdfc3449eb09a6855217e0c47",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 30/30 [53:13&lt;00:00, 106.46s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6de5b3254e464ed29d3dd6e5b02810c5"
          }
        },
        "152c00e3d03244c7a6e59f72bb51be4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c4e1165c7cee431b9551e8c7297173c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5038c98bdfc3449eb09a6855217e0c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6de5b3254e464ed29d3dd6e5b02810c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72be28b00b274d759acda1728732bb0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e9fd50cd2f23464bbd8a4c150265933d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_173283dcd6834f3b9dd67ba595b37443",
              "IPY_MODEL_554874a3c03a4ba5b42d824ba7738935"
            ]
          }
        },
        "e9fd50cd2f23464bbd8a4c150265933d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "173283dcd6834f3b9dd67ba595b37443": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6789272aca9049bb89a9227da3d34995",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 50,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 50,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f3968173b2f942b69a4f14318d376284"
          }
        },
        "554874a3c03a4ba5b42d824ba7738935": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e1cba5c2060b49d1be6b1c06bf38a0fc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 50/50 [1:29:20&lt;00:00, 107.20s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2ab9da4c64d54829a9405c416ad79b6c"
          }
        },
        "6789272aca9049bb89a9227da3d34995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f3968173b2f942b69a4f14318d376284": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e1cba5c2060b49d1be6b1c06bf38a0fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2ab9da4c64d54829a9405c416ad79b6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hjori66/Kaist-AI605-2021-Spring/blob/main/KAIST_AI605_Assignment_2_20194364.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReonT_YasRSx"
      },
      "source": [
        "# KAIST AI605 Assignment 2: Token Classification with RNNs and Attention\n",
        "Author: Minjoon Seo (minjoon@kaist.ac.kr)\n",
        "\n",
        "TA in charge: Taehyung Kwon (taehyung.kwon@kaist.ac.kr)\n",
        "\n",
        "**Due date**:  April 19 (Mon) 11:00pm, 2021  \n",
        "\n",
        "\n",
        "Your name: Taehwan Kim\n",
        "\n",
        "Your student ID: 20194364\n",
        "\n",
        "Your collaborators: -\n",
        "\n",
        "## Assignment Objectives\n",
        "- Verify theoretically and empirically how Transformer's attention mechanism works for sequence modeling task.\n",
        "- Implement Transformer's encoder attention layer from scratch using PyTorch.\n",
        "- Design an Attention-based token classification model using PyTorch.\n",
        "- Apply the token classification model to a popular machine reading comprehension task, Stanford Question Answering Dataset (SQuAD).\n",
        "- (Bonus) Analyze pros and cons between using RNN + attention versus purely attention.\n",
        "\n",
        "## Your Submission\n",
        "Your submission will be a link to a Colab notebook that has all written answers and is fully executable. You will submit your assignment via KLMS. Use in-line LaTeX (see below) for mathematical expressions. Collaboration among students is allowed but it is not a group assignment so make sure your answer and code are your own. Also make sure to mention your collaborators in your assignment with their names and their student ids.\n",
        "\n",
        "## Grading\n",
        "The entire assignment is out of 100 points. There are two bonus questions with 30 points altogether. Your final score can be higher than 100 points.\n",
        "\n",
        "\n",
        "## Environment\n",
        "You will only use Python 3.7 and PyTorch 1.8, which is already available on Colab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qwta269rqLQ",
        "outputId": "da995d39-5def-4298-9a85-6696a6741e1f"
      },
      "source": [
        "from platform import python_version\n",
        "import torch\n",
        "\n",
        "print(\"python\", python_version())\n",
        "print(\"torch\", torch.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python 3.7.10\n",
            "torch 1.8.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTm5eq4NwQZs"
      },
      "source": [
        "## 1. Transformer's Attention Layer\n",
        "\n",
        "We will first start with going over a few concepts that you learned in your high school statistics class. The variance of a random variable $X$, $\\text{Var}(X)$ is defined as $\\text{E}[(X-\\mu)^2]$ where $\\mu$ is the mean of $X$. Furthermore, given two independent random variables $X$ and $Y$ and a constant $a$,\n",
        "$$ \\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y), \\quad \\ldots \\; \\text{(1)}$$ \n",
        "$$ \\text{Var}(aX) = a^2\\text{Var}(X), \\quad \\ldots \\; \\text{(2)}$$\n",
        "$$ \\text{Var}(XY) = \\text{E}(X^2)\\text{E}(Y^2) - [\\text{E}(X)]^2[\\text{E}(Y)]^2. \\quad \\ldots \\; \\text{(3)}$$\n",
        "\n",
        "**Problem 1.1** *(10 points)* Suppose we are given two sets of $n$ random variables, $X_1 \\dots X_n$ and $Y_1 \\dots Y_n$, where all of these $2n$ variables are mutually independent and have a mean of $0$ and a variance of $1$. Prove that\n",
        "$$\\text{Var}\\left(\\sum_i^n X_i Y_i\\right) = n.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDO1QDJOXza8"
      },
      "source": [
        "There is a typo in the formula $\\text{(2)}$. I changed it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3ULLzK0NOPA"
      },
      "source": [
        "**Answer 1.1** \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "  \\text{Var}\\left(\\sum_i^n X_i Y_i\\right)\n",
        "  &= \\sum_i^n \\text{Var} \\left(X_i Y_i\\right) \\quad \\because \\text{(1), independence} \\\\\n",
        "  &= \\sum_i^n \\left[\\text{E}(X_i^2)\\text{E}(Y_i^2) - [\\text{E}(X_i)]^2[\\text{E}(Y_i)]^2 \\right] \\quad \\because \\text{(3)} \\\\\n",
        "  &= \\sum_i^n \\left[\\text{E}((X_i-0)^2)\\text{E}((Y_i-0)^2)\\right] \\\\\n",
        "  &= \\sum_i^n \\left[\\text{E}((X_i-[\\text{E}(X_i)])^2)\\text{E}((Y_i-[\\text{E}(Y_i)])^2)\\right] \\\\\n",
        "  &= \\sum_i^n \\left[\\text{Var}(X_i) \\text{Var}(Y_i)\\right] \\\\\n",
        "  &= \\sum_i^n \\left[1\\right] = n \\\\\n",
        "\\end{align}\n",
        "\\\\\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KIelEM56jn_"
      },
      "source": [
        "In Lecture 08 and 09, we discussed how the attention is computed in Transformer via the following equation,\n",
        "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V.$$\n",
        "**Problem 1.2** *(10 points)*  Suppose $Q$ and $K$ are matrices of independent variables each of which has a mean of $0$ and a variance of $1$. Using what you learned from Problem 1.1., show that\n",
        "$$\\text{Var}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) = 1.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wNoM7FtW0gi"
      },
      "source": [
        "**Answer 1.2** \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "  \\text{Var}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)\n",
        "  &= \\left(\\frac{1}{\\sqrt{d_k}}\\right)^2 \\text{Var}\\left(QK^\\top\\right)  \\quad \\because \\text{(2)} \\\\\n",
        "  &= \\frac{1}{d_k} \\text{Var}\\left(QK^\\top\\right) \\\\\n",
        "\\end{align}\n",
        "\\\\\n",
        "\\\\\n",
        "$$\n",
        "\n",
        "Then, we can focus on the one element of $QK$, $\\left(QK\\right)_{ij}$ without loss of generality.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "  \\frac{1}{d_k} \\text{Var}\\left(\\left(QK\\right)_{ij}^\\top\\right)\n",
        "  &= \\frac{1}{d_k} \\text{Var}\\left(\\sum_{t}^{d_k} \\left(Q_{it} K_{tj}\\right) \\right) \\\\\n",
        "  &= \\frac{1}{d_k} \\left(\\sum_{t}^{d_k} \\text{Var}\\left(Q_{it} K_{tj}\\right) \\right) \\quad \\because \\text{(1), } Q_{it} \\text{ and } Y_{tj} \\text{ are mutually independent} \\\\\n",
        "  &= \\frac{1}{d_k} \\left(d_k\\right) = 1 \\quad \\because \\text{Problem 1.1} \\\\\n",
        "\\end{align}\n",
        "\\\\\n",
        "$$\n",
        "\n",
        "Therefore, \n",
        "\n",
        "$$\n",
        "\\text{Var}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) = 1.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU3a3FEu6loq"
      },
      "source": [
        "\n",
        "**Problem 1.3** *(10 points)* What would happen if the assumption that the variance of $Q$ and $K$ is $1$ does not hold? Consider each case of it being higher and lower than $1$ and conjecture what it implies, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS4xjbWnmZL7"
      },
      "source": [
        "**Answer 1.3** \\\n",
        "\n",
        "If the variance of $Q_{ij}$ and $K_{ij}$ is higher than 1 for all i and j, then\n",
        "\n",
        "$$\n",
        "\\text{Var}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) > 1.\n",
        "$$\n",
        "\n",
        "Then, the variance of the output of the decoder becomes larger.\n",
        "If we use the softmax function on this output, then the final result might be \"too\" sharp. (Actually, this is not true, because of the residual connection) \\\n",
        "So, I guess that the model overfits faster than original model.\n",
        "\n",
        "\\\n",
        "\n",
        "Otherwse, if the variance of $Q_{ij}$ and $K_{ij}$ is lower than 1 for all i and j, then\n",
        "\n",
        "$$\n",
        "\\text{Var}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) < 1.\n",
        "$$\n",
        "\n",
        "Then, the variance of the output of the decoder becomes smaller.\n",
        "If we use the softmax function on this output, then the final result might be \"too\" smooth. \n",
        "\\\n",
        "So, I guess that the early training would be more unstable than normal although we use the bigger learning rate. The training time would be longer than the original version. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtGo8MAt7By2"
      },
      "source": [
        "## 2. Preprocessing SQuAD\n",
        "\n",
        "We will use `datasets` package offered by Hugging Face, which allows us to easily download various language datasets, including Stanford Question Answering Dataset (SQuAD).\n",
        "\n",
        "First, install the package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEGhK5tO8DcT",
        "outputId": "31ac33cc-de98-466b-fda0-16e4e10118f0"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.6.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pyarrow>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lrJLeSG8Xkl"
      },
      "source": [
        "Then, download SQuAD and print the first example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePc6IF9I8Jg1",
        "outputId": "fa0a2d37-470a-4370-9cb7-72820bb75dd1"
      },
      "source": [
        "from datasets import load_dataset\n",
        "squad_dataset = load_dataset('squad')\n",
        "print(squad_dataset['train'][0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']}, 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'id': '5733be284776f41900661182', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'title': 'University_of_Notre_Dame'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIzhbD6a81Q6"
      },
      "source": [
        "Here, `answer_start` corresponds to the character-level start position of the answer, and `text` is the answer text itself. You will note that `answer_start` and `text` fields are given as lists but they only contain one item each. In fact, you can safely assume that this is the case for the training data. During evaluation, however, you will utilize several possible answers so that your evaluation can be compared against all of them. So your code need to handle multiple-answers case as well.\n",
        "\n",
        "As we discussed in Lecture 05, we want to formulate this task as a token classification problem. That is, we want to find which token of the context corresponds to the start position of the answer, and which corresponds to the end.\n",
        "\n",
        "**Problem 2.1** *(10 points)* Write `preprocess()` function that takes a SQuAD example as the input and outputs space-tokenized context and question, as well as the start and end token position of the answer if it has the answer field. That is, a pseudo code would look like:\n",
        "```python\n",
        "def preprocess(example):\n",
        "  out = {'context': ['each', 'token'], \n",
        "         'question': ['each', 'token']}\n",
        "  if 'answers' not in example:\n",
        "    return out\n",
        "  out['answers'] = [{'start': 3, 'end': 5}]\n",
        "  return out\n",
        "```\n",
        "Verify that this code works by comparing between the original answer text and the concatenation of the answer tokens from start to end in training data. Report the percentage of the questions that have exact match."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiBkG_u_1D2l"
      },
      "source": [
        "**Answer 2.1**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtoL89-n1H1a"
      },
      "source": [
        "# def preprocess(example):\n",
        "#     def tokenizer(sentence):\n",
        "#         if sentence is None:\n",
        "#             return list()\n",
        "#         return sentence.split()\n",
        "\n",
        "#     out = dict()\n",
        "#     context = example['context']\n",
        "#     question = example['question']\n",
        "\n",
        "#     out['context'] = tokenizer(context)\n",
        "#     out['question'] = tokenizer(question)\n",
        "\n",
        "#     answer_list = list()\n",
        "#     for answer_index in range(len(example['answers']['text'])):\n",
        "#         answer = example['answers']['text'][answer_index]\n",
        "#         answer_start = example['answers']['answer_start'][answer_index]\n",
        "#         answer_end = answer_start + len(answer)\n",
        "\n",
        "#         if (answer_start == 0 or context[answer_start-1] == ' ') \\\n",
        "#             and (answer_end == len(context) or context[answer_end] == ' '):\n",
        "#             n_tokens_before_answer = len(tokenizer(context[:answer_start]))\n",
        "#             n_tokens_answer = len(tokenizer(answer))\n",
        "#             answer_list.append({'start': n_tokens_before_answer, 'end': n_tokens_before_answer+n_tokens_answer-1})\n",
        "    \n",
        "#     if not answer_list:\n",
        "#         out['answers'] = answer_list\n",
        "\n",
        "#     return out\n",
        "\n",
        "# n_exact_match = 0.0\n",
        "# for i, example in enumerate(squad_dataset['train']):\n",
        "#     out = preprocess(example)\n",
        "#     if 'answers' in out.keys():\n",
        "#         n_exact_match += 1\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60pb_dqgRlYl"
      },
      "source": [
        "** Result 2.1 **\n",
        "\n",
        "\n",
        "```\n",
        "print(\"number of answers that have exact match : \", n_exact_match)\n",
        "print(\"number of training data : \", len(squad_dataset['train']))\n",
        "print(\"the percentage of the exact match (training) : \", n_exact_match / len(squad_dataset['train']))\n",
        "\n",
        "# number of answers that have exact match :  41616.0\n",
        "# number of training data :  87599\n",
        "# the percentage of the exact match :  0.47507391636890833 -> pretty low\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wikI9OxVCL0E"
      },
      "source": [
        "We want to maximize the percentage of the exact match. You might see a low percentage however, due to bad tokenization. For instance, such space-based tokenization will fail to separate between \"world\" and \"!\" in \"hello world!\". \n",
        "\n",
        "**Problem 2.2** *(10 points)* Write an advanced tokenization model that always separates non-alphabet characters as independent tokens. For instance, \"hello1 world!!\" will be tokenized into \"hello\", \"1\", \"world\", \"!\", and \"!\". Using this new tokenizer, re-run the `preprocess` function and report the exact match percentage. How does the ratio change?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0SqpETMGUkW"
      },
      "source": [
        "**Answer 2.2**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UHzcv-7GWla"
      },
      "source": [
        "def find_nonalpha_list(dataset):\n",
        "    nonalpha_list = list()\n",
        "    for example in dataset:\n",
        "        for c in example['context']:\n",
        "            if not c.isalpha() and c not in nonalpha_list:\n",
        "                nonalpha_list.append(c)\n",
        "        for c in example['question']:\n",
        "            if not c.isalpha() and c not in nonalpha_list:\n",
        "                nonalpha_list.append(c)\n",
        "    return nonalpha_list\n",
        "\n",
        "\n",
        "def preprocess(example, nonalpha_list):\n",
        "    def tokenizer(sentence, nonalpha_list):\n",
        "        if sentence is None:\n",
        "            return list()\n",
        "\n",
        "        for nonalpha_token in nonalpha_list:\n",
        "            sentence = sentence.replace(nonalpha_token, ' ' + nonalpha_token + ' ')\n",
        "\n",
        "        sentence = ' '.join(sentence.split())\n",
        "        return sentence.split()\n",
        "        \n",
        "    out = dict()\n",
        "    context = example['context']\n",
        "    question = example['question']\n",
        "    id = example['id']\n",
        "\n",
        "    out['context'] = tokenizer(context, nonalpha_list)\n",
        "    out['question'] = tokenizer(question, nonalpha_list)\n",
        "    out['id'] = id\n",
        "\n",
        "    answer_list = list()\n",
        "    for answer_index in range(len(example['answers']['text'])):\n",
        "        answer = example['answers']['text'][answer_index]\n",
        "        answer_start = example['answers']['answer_start'][answer_index]\n",
        "        answer_end = answer_start + len(answer)\n",
        "\n",
        "        if (answer_start == 0 or context[answer_start-1] in nonalpha_list) \\\n",
        "            and (answer_end == len(context) or context[answer_end] in nonalpha_list):\n",
        "            n_tokens_before_answer = len(tokenizer(context[:answer_start], nonalpha_list))\n",
        "            n_tokens_answer = len(tokenizer(answer, nonalpha_list))\n",
        "            answer_list.append({'start': n_tokens_before_answer, 'end': n_tokens_before_answer+n_tokens_answer-1})\n",
        "\n",
        "    if answer_list:\n",
        "        out['answers'] = answer_list\n",
        "\n",
        "    return out\n",
        "\n",
        "# dataset = squad_dataset['train']\n",
        "# # dataset = squad_dataset['validation'] # Do this if you want to check the valid dataset\n",
        "\n",
        "# nonalpha_list = find_nonalpha_list(squad_dataset['train'])\n",
        "# print(\"nonalpha_list : \", nonalpha_list)\n",
        "\n",
        "# n_exact_match = 0.0\n",
        "# for i, example in enumerate(dataset):\n",
        "#   out = preprocess(example, nonalpha_list)\n",
        "#   if 'answers' in out.keys():\n",
        "#     n_exact_match += 1\n",
        "\n",
        "# print(\"number of answers that have exact match : \", n_exact_match)\n",
        "# print(\"number of training data : \", len(dataset))\n",
        "# print(\"the percentage of the exact match (training) : \", n_exact_match / len(dataset))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXwQjv8LRYOd"
      },
      "source": [
        "** Result 2.2 **\n",
        "\n",
        "\n",
        "```\n",
        "nonalpha_list :  [',', ' ', '.', \"'\", '\"', '1', '8', '5', '(', '3', ')', '?', '-', '7', '6', '9', '2', '0', ';', '–', '&', '4', '%', '$', '[', ']', '/', ':', '#', '—', '!', '“', '’', '”', '<', '\\u200b', '̃', '£', '½', '+', '¢', '−', '°', '>', '€', '《', '》', '±', '~', '¥', '²', '❤', '=', '\\u200e', '͡', '́', '`', '्', 'ु', 'ः', 'ॊ', 'ि', 'ा', '\\u200d', '\\u200c', '*', '‘', '\\u3000', '•', '§', '⁄', '\\n', '̯', '̩', '…', '·', 'ָ', 'ִ', 'ׁ', 'ַ', 'ּ', 'ְ', 'ّ', '⟨', '◌', '⟩', '˭', '̤', '♠', '∅', '̞', '×', '̥', '′', '″', '\\ufeff', '_', 'ֿ', '´', '^', '̧', '̄', '→', '‑', '，', '₹', '\\u202f', '♯', '₂', '₥', '⁊', '\\u2009', '{', '}', '|', '@', '̪', '‚', '›', 'ׂ', 'ֵ', 'ِ', 'ْ', 'َ', '̍', '˥', '˨', '˩', '¡', '√', '¿', 'ာ', 'း', 'ُ', '≥', '˚', '≈', '⋅', 'ี', '︘', '�', '～', '〜', '̀', 'ོ', '་', '˧', 'ಾ', 'ು', '್', 'া', '্', 'ಿ', '∗', '∈', '≡', '∖', '№', '÷', 'ٔ', '¶', 'ิ', '₤', '♆', '⅓', '∝', '¼', 'ٍ', 'ֹ', '̌', '。', '̠', '₯']\n",
        "number of answers that have exact match :  87108.0\n",
        "number of training data :  87599\n",
        "the percentage of the exact match (training) :  0.9943949131839405 -> now, it is OK\n",
        "\n",
        "number of answers that have exact match :  10566.0\n",
        "number of validation data :  10570\n",
        "the percentage of the exact match (validation) :  0.9996215704824977 -> Also, it is OK\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJtJXDSvDpE8"
      },
      "source": [
        "## 3. LSTM Baseline for SQuAD\n",
        "\n",
        "We will bring and reuse our model from Assignment 1. There are two key differences, however. First, we need to classify each token instead of the entire sentence. Second, we have two inputs (context and question) instead of just one.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX3hFmUAVD9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81940b6f-9299-478f-c946-1745c1060e35"
      },
      "source": [
        "# My model from Assignment 1 is too slow to use it.\n",
        "# I will use torchtext and torch.nn.lstm in the assignment 2.\n",
        "\n",
        "import torchtext\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "from torchtext.legacy.data import BucketIterator\n",
        "\n",
        "\n",
        "class SQuAD1Dataset(data.Dataset):\n",
        "  \"\"\"\n",
        "  Defines a dataset for squad1.0.\n",
        "  \"\"\"\n",
        "  \n",
        "  @staticmethod\n",
        "  def sort_key(ex):\n",
        "    return data.interleave_keys(len(ex.context), len(ex.question))\n",
        "\n",
        "  def __init__(self, data_list, fields, use_bos=True, max_length=None, **kwargs):\n",
        "    if not isinstance(fields[0], (tuple, list)):\n",
        "      fields = [('context', fields[0]), \n",
        "                ('question', fields[1]), \n",
        "                # ('context_question', fields[2]), # For Problem 3.2+, put the question after the context\n",
        "                ('answer_start', fields[2]), \n",
        "                ('answer_end', fields[3]), \n",
        "                ('id_index', fields[4])\n",
        "                ]\n",
        "\n",
        "    examples = []\n",
        "    nonalpha_list = find_nonalpha_list(data_list)\n",
        "\n",
        "    self.id_list = list()\n",
        "    self.reference = list()\n",
        "\n",
        "    for _, example in enumerate(data_list):\n",
        "        out = preprocess(example, nonalpha_list)\n",
        "        # use data if the answer exists\n",
        "        if max_length and max_length < max(len(out['context']), len(out['question'])):\n",
        "            continue\n",
        "        if 'answers' in out.keys():\n",
        "            answer_start = out['answers'][0]['start'] # Use index 0 for instant valid accuracy\n",
        "            answer_end = out['answers'][0]['end'] # Use index 0 for instant valid accuracy\n",
        "            if use_bos:\n",
        "                answer_start += 1 # for <BOS> token\n",
        "                answer_end += 1 # for <BOS> token\n",
        "            examples.append(data.Example.fromlist([out['context'], \n",
        "                                                   out['question'], \n",
        "                                                #    out['context'] + [\"<CLS>\"] + out['question'], # Use <CLS> token\n",
        "                                                   answer_start,\n",
        "                                                   answer_end,\n",
        "                                                   len(self.id_list)], \n",
        "                                                  fields))\n",
        "            self.id_list.append(out['id'])\n",
        "            self.reference.append({'id':example['id'], 'answers':example['answers']})\n",
        "\n",
        "    super(SQuAD1Dataset, self).__init__(examples, fields, **kwargs)\n",
        "\n",
        "\n",
        "class SQuAD1Dataloader():\n",
        "  \"\"\"\n",
        "  Make the dataloader for SQuAD 1.0\n",
        "  \"\"\"\n",
        "  def __init__(self, train_data=None, valid_data=None, batch_size=64, device='cpu', \n",
        "                max_length=255, min_freq=2, fix_length=None,\n",
        "                use_bos=True, use_eos=True, shuffle=True\n",
        "              ):\n",
        "\n",
        "    super(SQuAD1Dataloader, self).__init__()\n",
        "\n",
        "    self.text = data.Field(sequential=True, use_vocab=True, batch_first=True, \n",
        "                           include_lengths=True, fix_length=fix_length, \n",
        "                           init_token='<BOS>' if use_bos else None, \n",
        "                           eos_token='<EOS>' if use_eos else None\n",
        "                          )\n",
        "    self.answer_start = data.Field(sequential = False, use_vocab = False)\n",
        "    self.answer_end = data.Field(sequential = False, use_vocab = False)\n",
        "    self.id_index = data.Field(sequential = False, use_vocab = False)\n",
        "    \n",
        "    train = SQuAD1Dataset(data_list=train_data, \n",
        "                          fields = [('context', self.text),\n",
        "                                    ('question', self.text),\n",
        "                                    # ('context_question', self.text),\n",
        "                                    ('answer_start', self.answer_start),\n",
        "                                    ('answer_end', self.answer_end),\n",
        "                                    ('id_index', self.id_index)\n",
        "                                    ], \n",
        "                          use_bos = use_bos,\n",
        "                          max_length = max_length\n",
        "                          )\n",
        "    valid = SQuAD1Dataset(data_list=valid_data, \n",
        "                          fields = [('context', self.text),\n",
        "                                    ('question', self.text),\n",
        "                                    # ('context_question', self.text),\n",
        "                                    ('answer_start', self.answer_start),\n",
        "                                    ('answer_end', self.answer_end),\n",
        "                                    ('id_index', self.id_index)\n",
        "                                    ], \n",
        "                          use_bos = use_bos,\n",
        "                          max_length = max_length\n",
        "                          )\n",
        "    self.train_id_list = train.id_list\n",
        "    self.valid_id_list = valid.id_list\n",
        "\n",
        "    self.train_reference = train.reference\n",
        "    self.valid_reference = valid.reference\n",
        "    \n",
        "    self.train_iter = data.BucketIterator(train, batch_size=batch_size,\n",
        "                                          device=device,\n",
        "                                          shuffle=shuffle,\n",
        "                                          sort_key=lambda x: len(x.question) + (max_length * len(x.context)), \n",
        "                                          sort_within_batch = True\n",
        "                                          )\n",
        "    self.valid_iter = data.BucketIterator(valid, batch_size=batch_size,\n",
        "                                          device=device,\n",
        "                                          shuffle=shuffle,\n",
        "                                          sort_key=lambda x: len(x.question) + (max_length * len(x.context)), \n",
        "                                          sort_within_batch = True\n",
        "                                          )\n",
        "    \n",
        "    self.text.build_vocab(train)\n",
        "\n",
        "\n",
        "train_dataset = squad_dataset['train']\n",
        "valid_dataset = squad_dataset['validation']\n",
        "\n",
        "print('# of train data : {}'.format(len(train_dataset)))\n",
        "print('# of vaild data : {}'.format(len(valid_dataset)))\n",
        "\n",
        "batch_size = 128\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "max_length = 253 # 255 - 2 for <BOS> and <EOS>\n",
        "min_freq = 2\n",
        "use_bos = False\n",
        "use_eos = False\n",
        "print(\"device : \", device)\n",
        "\n",
        "loader = SQuAD1Dataloader(train_dataset, valid_dataset, batch_size=batch_size, \n",
        "                          device=device, max_length=max_length, min_freq=min_freq,\n",
        "                          use_bos=use_bos, use_eos=use_eos)\n",
        "print('\\nFinish making the dataloader')\n",
        "print(\"batch_size : \", batch_size)\n",
        "print(\"max_length : \", max_length)\n",
        "print('number of used train data ~ {}'.format((len(loader.train_iter)) * batch_size))\n",
        "print('number of used vaild data ~ {}'.format((len(loader.valid_iter)) * batch_size))\n",
        "\n",
        "vocab = loader.text.vocab\n",
        "vocab_list = list(vocab.stoi.keys())\n",
        "print('number of vocab : {}'.format(len(vocab)))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of train data : 87599\n",
            "# of vaild data : 10570\n",
            "device :  cuda\n",
            "\n",
            "Finish making the dataloader\n",
            "batch_size :  128\n",
            "max_length :  253\n",
            "number of used train data ~ 81408\n",
            "number of used vaild data ~ 9856\n",
            "number of vocab : 86389\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsr-bjuxRHQ2"
      },
      "source": [
        "** Result 3.1.1 (Preprocessing) **\n",
        "\n",
        "\n",
        "```\n",
        "# of train data : 87599\n",
        "# of vaild data : 10570\n",
        "device :  cuda\n",
        "\n",
        "Finish making the dataloader\n",
        "batch_size :  128\n",
        "max_length :  253\n",
        "number of used train data ~ 81408\n",
        "number of used vaild data ~ 9856\n",
        "number of vocab : 86389\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhrgT3Xc11yw"
      },
      "source": [
        "\n",
        "Before resolving these differences, you will need to define your evaluation function to correctly evaluate how well your model is doing. Note that the evaluation was very straightforward in Assignment 1's sentiment classification (it is either positive or negative) while it is a bit complicated in SQuAD. We will use the evaluation function provided by `datasets`. You can access to it via the following code.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLMA7wXjHkxP"
      },
      "source": [
        "from datasets import load_metric\n",
        "squad_metric = load_metric('squad')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7smOVxqXJxyB"
      },
      "source": [
        "You can also easily learn about how to use the function by simply typing the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr7rl9ihHsdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fc0c0ad-7a4c-4542-8d35-d4e50a675168"
      },
      "source": [
        "squad_metric"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Metric(name: \"squad\", features: {'predictions': {'id': Value(dtype='string', id=None), 'prediction_text': Value(dtype='string', id=None)}, 'references': {'id': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}}, usage: \"\"\"\n",
              "Computes SQuAD scores (F1 and EM).\n",
              "Args:\n",
              "    predictions: List of question-answers dictionaries with the following key-values:\n",
              "        - 'id': id of the question-answer pair as given in the references (see below)\n",
              "        - 'prediction_text': the text of the answer\n",
              "    references: List of question-answers dictionaries with the following key-values:\n",
              "        - 'id': id of the question-answer pair (see above),\n",
              "        - 'answers': a Dict in the SQuAD dataset format\n",
              "            {\n",
              "                'text': list of possible texts for the answer, as a list of strings\n",
              "                'answer_start': list of start positions for the answer, as a list of ints\n",
              "            }\n",
              "            Note that answer_start values are not taken into account to compute the metric.\n",
              "Returns:\n",
              "    'exact_match': Exact match (the normalized answer exactly match the gold answer)\n",
              "    'f1': The F-score of predicted tokens versus the gold answer\n",
              "Examples:\n",
              "\n",
              "    >>> predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]\n",
              "    >>> references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]\n",
              "    >>> squad_metric = datasets.load_metric(\"squad\")\n",
              "    >>> results = squad_metric.compute(predictions=predictions, references=references)\n",
              "    >>> print(results)\n",
              "    {'exact_match': 100.0, 'f1': 100.0}\n",
              "\"\"\", stored examples: 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh72l3MtJW6D"
      },
      "source": [
        "**Problem 3.1** *(10 points)* Let's resolve the first issue here. Hence, for now, assume that your only input is context and you want to obtain the answer without seeing the question. While this may seem to be a non-sense, actually it can be considered as modeling the prior $\\text{Prob}(a|c)$ before observing $q$ (we ultimately want $\\text{Prob}(a|q,c)$). Transform your model into a token classification model by imposing $\\text{softmax}$ over the tokens instead of predefined classes. You will need to do this twice for each of start and end. Report the accuracy (using the metric above) on `squad_dataset['validation']`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PShlCxLcGai_"
      },
      "source": [
        "**Answer 3.1**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtkm4TsgGcem",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f8ef456b234d4b8391a2e58acdc2db07",
            "1b30d21737f4462f81460aacc4e28beb",
            "f1772891790e46fa8c43e2d0dafa3258",
            "c96bfea6e1a44dcb9f8cfa4acada2126",
            "152c00e3d03244c7a6e59f72bb51be4e",
            "c4e1165c7cee431b9551e8c7297173c5",
            "5038c98bdfc3449eb09a6855217e0c47",
            "6de5b3254e464ed29d3dd6e5b02810c5"
          ]
        },
        "outputId": "f397264a-d869-447e-d15f-ecabe0f8f60a"
      },
      "source": [
        "import torch.nn as nn\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class ClassificationLSTMModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, n_layers, n_label, emb_dropout, rnn_dropout, bidirectional, enable_layer_norm, device):\n",
        "        super(ClassificationLSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
        "                            hidden_size=hidden_dim, \n",
        "                            num_layers=n_layers, \n",
        "                            dropout=rnn_dropout, \n",
        "                            bidirectional=bidirectional)\n",
        "      \n",
        "        n_direction = 2 if bidirectional else 1\n",
        "        self.fc_start = nn.Linear(hidden_dim*n_direction, n_label, bias=True)\n",
        "        self.fc_end = nn.Linear(hidden_dim*n_direction, n_label, bias=True)\n",
        "\n",
        "        # Layer_normalization\n",
        "        self.enable_layer_norm = enable_layer_norm\n",
        "        if enable_layer_norm:\n",
        "            self.emb_layer_norm = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
        "        self.fc_dropout = nn.Dropout(rnn_dropout)\n",
        "        self.bidirectional = bidirectional\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, input_tensor, src_seq_lens):\n",
        "        emb = self.embedding(input_tensor) # emb.shape = batch * len * hidden\n",
        "\n",
        "        # Layer_normalization\n",
        "        if self.enable_layer_norm:\n",
        "            emb = self.emb_layer_norm(emb)\n",
        "\n",
        "        emb = self.emb_dropout(emb)\n",
        "        emb = emb.transpose(0, 1) # emb.shape = len * batch * hidden\n",
        "\n",
        "        # n_direction = 2 if bidirectional else 1\n",
        "        # hidden = torch.zeros(n_layers*n_direction, context.shape[0], hidden_dim, requires_grad=True).to(self.device)\n",
        "        # cell = torch.zeros(n_layers*n_direction, context.shape[0], hidden_dim, requires_grad=True).to(self.device)\n",
        "\n",
        "        # nn.LSTM\n",
        "        packed = pack_padded_sequence(emb, src_seq_lens.tolist(), batch_first=False)\n",
        "        outs, (hidden, cell) = self.lstm(packed)\n",
        "        outs, out_lens = pad_packed_sequence(outs, batch_first=False)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            hidden = torch.stack([hidden[-2], hidden[-1]], dim=0)\n",
        "        else:\n",
        "            hidden = hidden[-1].unsqueeze(dim=0)\n",
        "        hidden = hidden.transpose(0, 1)\n",
        "        hidden = hidden.contiguous().view(hidden.shape[0], -1)\n",
        "\n",
        "        hidden = self.fc_dropout(hidden)\n",
        "        logits_start = self.fc_start(hidden)\n",
        "        logits_end = self.fc_end(hidden)\n",
        "        return (logits_start, logits_end)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device : \", device)\n",
        "\n",
        "# Vocabulary : Use vocab_list\n",
        "\n",
        "# Construct the LSTM Model\n",
        "embedding_dim = 128 # usually bigger, e.g. 128\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "n_label = max_length+1 if use_bos else max_length\n",
        "emb_dropout = 0.1\n",
        "rnn_dropout = 0.5\n",
        "bidirectional = True\n",
        "enable_layer_norm = True\n",
        "rnnmodel = ClassificationLSTMModel(embedding_dim, hidden_dim, n_layers, n_label, emb_dropout, rnn_dropout, bidirectional, enable_layer_norm, device).to(device)\n",
        "\n",
        "print(\"batch_size : \", batch_size)\n",
        "print(\"max_length : \", max_length)\n",
        "print(\"embedding_dim : \", embedding_dim)\n",
        "print(\"hidden_dim : \", hidden_dim)\n",
        "print(\"n_layers : \", n_layers)\n",
        "print(\"emb_dropout : \", emb_dropout)\n",
        "print(\"rnn_dropout_and_fc_dropout : \", rnn_dropout)\n",
        "if bidirectional:\n",
        "    print(\"bidirectional : True\")\n",
        "else:\n",
        "    print(\"bidirectional : False\")\n",
        "if enable_layer_norm:\n",
        "    print(\"enable_layer_norm : True\")\n",
        "else:\n",
        "    print(\"enable_layer_norm : False\")\n",
        "\n",
        "# Construct the data loader\n",
        "train_iter = loader.train_iter\n",
        "valid_iter = loader.valid_iter\n",
        "\n",
        "train_id_list = loader.train_id_list\n",
        "valid_id_list = loader.valid_id_list\n",
        "train_reference = loader.train_reference\n",
        "valid_reference = loader.valid_reference\n",
        "\n",
        "# Training\n",
        "learning_rate = 1e-3\n",
        "print(\"learning_rate : \", learning_rate)\n",
        "\n",
        "PAD_IDX = vocab.stoi['<pad>']\n",
        "cel = nn.CrossEntropyLoss(ignore_index=PAD_IDX) # Ignore Padding\n",
        "# optimizer = torch.optim.SGD(rnnmodel.parameters(), lr=1e-1)\n",
        "optimizer = torch.optim.Adam(rnnmodel.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 30\n",
        "max_norm = 5\n",
        "\n",
        "# Evaluate\n",
        "squad_metric = load_metric('squad') # get_tokens() in squad_metric is not exactly same as my preprocess()..\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    train_loss = 0\n",
        "    train_accuracy = 0.0\n",
        "    train_data_num = 0\n",
        "    train_prediction = list()\n",
        "    for train_i, train_batch in enumerate(train_iter):\n",
        "        context, context_length = train_batch.context\n",
        "        question, question_length = train_batch.question # Unused\n",
        "        answer_start = train_batch.answer_start\n",
        "        answer_end = train_batch.answer_end\n",
        "        train_id_index = train_batch.id_index\n",
        "\n",
        "        logits_start, logits_end = rnnmodel(context, context_length)\n",
        "\n",
        "        optimizer.zero_grad() # reset process\n",
        "        loss = cel(logits_start, answer_start) + cel(logits_end, answer_end) # Loss, a.k.a L\n",
        "\n",
        "        loss.backward() # compute gradients\n",
        "        # print(torch.norm(rnnmodel.lstm.weight_hh_l0.grad), loss.item())\n",
        "        # torch.nn.utils.clip_grad_norm_(rnnmodel.parameters(), max_norm) # gradent clipping\n",
        "        optimizer.step() # update parameters\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        _, train_start_preds = torch.max(logits_start, 1)\n",
        "        _, train_end_preds = torch.max(logits_end, 1)\n",
        "        # train_accuracy += ((train_start_preds == answer_start) * (train_end_preds == answer_end)).sum().float()\n",
        "\n",
        "        train_data_num += context.shape[0]\n",
        "\n",
        "        for train_j in range(context.shape[0]):\n",
        "            pred_text = \"\"\n",
        "            start = train_start_preds[train_j]\n",
        "            end = train_end_preds[train_j]\n",
        "            if start < end:\n",
        "                pred_text = [vocab_list[text_id] for text_id in context[train_j][start:end+1]]\n",
        "                pred_text = \" \".join(pred_text)\n",
        "            \n",
        "            # start = answer_start[train_j]\n",
        "            # end = answer_end[train_j]\n",
        "            # answer_text = [vocab_list[text_id] for text_id in context[train_j][start:end+1]]\n",
        "            # answer_text = \" \".join(answer_text)\n",
        "            # print(pred_text, answer_text, train_reference[train_id_index[train_j]], \"\\n\")\n",
        "\n",
        "            train_prediction.append({'id':train_id_list[train_id_index[train_j]], 'prediction_text':pred_text})\n",
        "\n",
        "    train_result = squad_metric.compute(predictions=train_prediction, references=train_reference)\n",
        "    print('train:: Epoch:', '%04d' % (epoch + 1), \n",
        "          'cost =', '{:.6f},'.format(train_loss / train_data_num), \n",
        "        #   'argmax acc =', '{:.6f}'.format(train_accuracy / train_data_num),\n",
        "          'squad_metric : ', train_result\n",
        "          )\n",
        "        \n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        with torch.no_grad():\n",
        "            valid_loss = 0\n",
        "            valid_accuracy = 0.0\n",
        "            valid_data_num = 0\n",
        "            valid_prediction = list()\n",
        "            for valid_i, valid_batch in enumerate(valid_iter):\n",
        "                context, context_length = valid_batch.context\n",
        "                question, question_length = valid_batch.question # Unused\n",
        "                answer_start = valid_batch.answer_start\n",
        "                answer_end = valid_batch.answer_end\n",
        "                valid_id_index = valid_batch.id_index\n",
        "\n",
        "                logits_start, logits_end = rnnmodel(context, context_length)\n",
        "\n",
        "                loss = cel(logits_start, answer_start) + cel(logits_end, answer_end) # Loss, a.k.a L\n",
        "                valid_loss += loss.item()\n",
        "\n",
        "                _, valid_start_preds = torch.max(logits_start, 1)\n",
        "                _, valid_end_preds = torch.max(logits_end, 1)\n",
        "                # valid_accuracy += ((valid_start_preds == answer_start) * (valid_end_preds == answer_end)).sum().float()\n",
        "\n",
        "                valid_data_num += context.shape[0]\n",
        "\n",
        "                for valid_j in range(context.shape[0]):\n",
        "                    pred_text = \"\"\n",
        "                    start = valid_start_preds[valid_j]\n",
        "                    end = valid_end_preds[valid_j]\n",
        "                    if start < end:\n",
        "                        pred_text = [vocab_list[text_id] for text_id in context[valid_j][start:end+1]]\n",
        "                        pred_text = \" \".join(pred_text)\n",
        "                    valid_prediction.append({'id':valid_id_list[valid_id_index[valid_j]], 'prediction_text':pred_text})\n",
        "                \n",
        "            valid_result = squad_metric.compute(predictions=valid_prediction, references=valid_reference)\n",
        "            print('valid:: Epoch:', '%04d' % (epoch + 1), \n",
        "                  'cost =', '{:.6f},'.format(valid_loss / valid_data_num), \n",
        "                #   'argmax acc =', '{:.6f},'.format(valid_accuracy / valid_data_num),\n",
        "                  'squad_metric : ', valid_result\n",
        "                 )\n",
        "            "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device :  cuda\n",
            "batch_size :  128\n",
            "max_length :  253\n",
            "embedding_dim :  128\n",
            "hidden_dim :  256\n",
            "n_layers :  2\n",
            "emb_dropout :  0.1\n",
            "rnn_dropout_and_fc_dropout :  0.5\n",
            "bidirectional : True\n",
            "enable_layer_norm : True\n",
            "learning_rate :  0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8ef456b234d4b8391a2e58acdc2db07",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "train:: Epoch: 0001 cost = 0.079760, squad_metric :  {'exact_match': 0.05904059040590406, 'f1': 5.52339938822115}\n",
            "valid:: Epoch: 0001 cost = 0.079491, squad_metric :  {'exact_match': 0.040679345062544496, 'f1': 4.141770941182735}\n",
            "train:: Epoch: 0002 cost = 0.079107, squad_metric :  {'exact_match': 0.27921279212792127, 'f1': 4.516358336672799}\n",
            "valid:: Epoch: 0002 cost = 0.079342, squad_metric :  {'exact_match': 0.3356045967659921, 'f1': 4.610341876288495}\n",
            "train:: Epoch: 0003 cost = 0.078534, squad_metric :  {'exact_match': 0.5301353013530136, 'f1': 4.568132562807337}\n",
            "valid:: Epoch: 0003 cost = 0.079561, squad_metric :  {'exact_match': 0.4881521407505339, 'f1': 4.664714353463255}\n",
            "train:: Epoch: 0004 cost = 0.077731, squad_metric :  {'exact_match': 0.7380073800738007, 'f1': 4.899126557698399}\n",
            "valid:: Epoch: 0004 cost = 0.079900, squad_metric :  {'exact_match': 0.4779823044848978, 'f1': 4.848227029812154}\n",
            "train:: Epoch: 0005 cost = 0.076743, squad_metric :  {'exact_match': 0.915129151291513, 'f1': 5.173145030424936}\n",
            "valid:: Epoch: 0005 cost = 0.080609, squad_metric :  {'exact_match': 0.3966236143598088, 'f1': 4.724701760655256}\n",
            "train:: Epoch: 0006 cost = 0.075741, squad_metric :  {'exact_match': 0.998769987699877, 'f1': 5.401154805239039}\n",
            "valid:: Epoch: 0006 cost = 0.081186, squad_metric :  {'exact_match': 0.3966236143598088, 'f1': 4.607046859821393}\n",
            "train:: Epoch: 0007 cost = 0.074733, squad_metric :  {'exact_match': 1.137761377613776, 'f1': 5.502323035873868}\n",
            "valid:: Epoch: 0007 cost = 0.081922, squad_metric :  {'exact_match': 0.4271331231567172, 'f1': 4.551865069269158}\n",
            "train:: Epoch: 0008 cost = 0.073769, squad_metric :  {'exact_match': 1.204182041820418, 'f1': 5.661646000270595}\n",
            "valid:: Epoch: 0008 cost = 0.082516, squad_metric :  {'exact_match': 0.32543476050035597, 'f1': 4.222652645407895}\n",
            "train:: Epoch: 0009 cost = 0.072869, squad_metric :  {'exact_match': 1.3751537515375154, 'f1': 5.777759974047754}\n",
            "valid:: Epoch: 0009 cost = 0.083157, squad_metric :  {'exact_match': 0.3356045967659921, 'f1': 4.191452313944129}\n",
            "train:: Epoch: 0010 cost = 0.072054, squad_metric :  {'exact_match': 1.3714637146371464, 'f1': 5.790611846923638}\n",
            "valid:: Epoch: 0010 cost = 0.083795, squad_metric :  {'exact_match': 0.3152649242347198, 'f1': 4.178425942910128}\n",
            "train:: Epoch: 0011 cost = 0.071280, squad_metric :  {'exact_match': 1.3936039360393604, 'f1': 5.905970686867983}\n",
            "valid:: Epoch: 0011 cost = 0.084291, squad_metric :  {'exact_match': 0.3864537780941727, 'f1': 4.214191459500743}\n",
            "train:: Epoch: 0012 cost = 0.070560, squad_metric :  {'exact_match': 1.4772447724477245, 'f1': 5.87064883243569}\n",
            "valid:: Epoch: 0012 cost = 0.085198, squad_metric :  {'exact_match': 0.28475541543781147, 'f1': 4.112992304254845}\n",
            "train:: Epoch: 0013 cost = 0.069845, squad_metric :  {'exact_match': 1.5461254612546125, 'f1': 5.881162910122298}\n",
            "valid:: Epoch: 0013 cost = 0.085816, squad_metric :  {'exact_match': 0.3661141055629004, 'f1': 3.868770208199774}\n",
            "train:: Epoch: 0014 cost = 0.069205, squad_metric :  {'exact_match': 1.5362853628536286, 'f1': 6.011535672205428}\n",
            "valid:: Epoch: 0014 cost = 0.086096, squad_metric :  {'exact_match': 0.2644157429065392, 'f1': 3.9362743996948257}\n",
            "train:: Epoch: 0015 cost = 0.068615, squad_metric :  {'exact_match': 1.5817958179581795, 'f1': 5.894389559932659}\n",
            "valid:: Epoch: 0015 cost = 0.086543, squad_metric :  {'exact_match': 0.19322688904708635, 'f1': 3.936425858306148}\n",
            "train:: Epoch: 0016 cost = 0.068076, squad_metric :  {'exact_match': 1.6248462484624846, 'f1': 5.984265813361757}\n",
            "valid:: Epoch: 0016 cost = 0.087069, squad_metric :  {'exact_match': 0.3050950879690837, 'f1': 3.9069096913585173}\n",
            "train:: Epoch: 0017 cost = 0.067535, squad_metric :  {'exact_match': 1.5621156211562115, 'f1': 5.895822398815512}\n",
            "valid:: Epoch: 0017 cost = 0.087296, squad_metric :  {'exact_match': 0.20339672531272246, 'f1': 4.0214630329923375}\n",
            "train:: Epoch: 0018 cost = 0.066968, squad_metric :  {'exact_match': 1.6728167281672817, 'f1': 6.115089538610587}\n",
            "valid:: Epoch: 0018 cost = 0.088093, squad_metric :  {'exact_match': 0.28475541543781147, 'f1': 3.9959138206550118}\n",
            "train:: Epoch: 0019 cost = 0.066491, squad_metric :  {'exact_match': 1.7404674046740467, 'f1': 6.059234042789079}\n",
            "valid:: Epoch: 0019 cost = 0.087950, squad_metric :  {'exact_match': 0.2135665615783586, 'f1': 3.844752147723429}\n",
            "train:: Epoch: 0020 cost = 0.066065, squad_metric :  {'exact_match': 1.7699876998769988, 'f1': 6.149869687787748}\n",
            "valid:: Epoch: 0020 cost = 0.088572, squad_metric :  {'exact_match': 0.20339672531272246, 'f1': 3.84057640971435}\n",
            "train:: Epoch: 0021 cost = 0.065629, squad_metric :  {'exact_match': 1.7269372693726937, 'f1': 6.077576685990024}\n",
            "valid:: Epoch: 0021 cost = 0.089076, squad_metric :  {'exact_match': 0.2135665615783586, 'f1': 3.668484128969692}\n",
            "train:: Epoch: 0022 cost = 0.065204, squad_metric :  {'exact_match': 1.7798277982779829, 'f1': 6.191290550684122}\n",
            "valid:: Epoch: 0022 cost = 0.089141, squad_metric :  {'exact_match': 0.20339672531272246, 'f1': 3.762071948778777}\n",
            "train:: Epoch: 0023 cost = 0.064740, squad_metric :  {'exact_match': 1.8757687576875768, 'f1': 6.236611392119068}\n",
            "valid:: Epoch: 0023 cost = 0.089259, squad_metric :  {'exact_match': 0.2542459066409031, 'f1': 3.7709149997595928}\n",
            "train:: Epoch: 0024 cost = 0.064380, squad_metric :  {'exact_match': 1.8204182041820418, 'f1': 6.217691244710026}\n",
            "valid:: Epoch: 0024 cost = 0.089861, squad_metric :  {'exact_match': 0.11186819892199736, 'f1': 3.6802549974475056}\n",
            "train:: Epoch: 0025 cost = 0.063955, squad_metric :  {'exact_match': 1.91389913899139, 'f1': 6.317859799188245}\n",
            "valid:: Epoch: 0025 cost = 0.090195, squad_metric :  {'exact_match': 0.14237770771890573, 'f1': 3.795095819731973}\n",
            "train:: Epoch: 0026 cost = 0.063569, squad_metric :  {'exact_match': 1.895448954489545, 'f1': 6.309099116230587}\n",
            "valid:: Epoch: 0026 cost = 0.090376, squad_metric :  {'exact_match': 0.12203803518763348, 'f1': 3.627915130292577}\n",
            "train:: Epoch: 0027 cost = 0.063159, squad_metric :  {'exact_match': 1.954489544895449, 'f1': 6.303810374911365}\n",
            "valid:: Epoch: 0027 cost = 0.090593, squad_metric :  {'exact_match': 0.1322078714532696, 'f1': 3.7959551917323835}\n",
            "train:: Epoch: 0028 cost = 0.062920, squad_metric :  {'exact_match': 1.980319803198032, 'f1': 6.390372746952164}\n",
            "valid:: Epoch: 0028 cost = 0.090556, squad_metric :  {'exact_match': 0.15254754398454184, 'f1': 3.754198485161899}\n",
            "train:: Epoch: 0029 cost = 0.062495, squad_metric :  {'exact_match': 2.097170971709717, 'f1': 6.464851302425954}\n",
            "valid:: Epoch: 0029 cost = 0.091080, squad_metric :  {'exact_match': 0.12203803518763348, 'f1': 3.628710220283829}\n",
            "train:: Epoch: 0030 cost = 0.062238, squad_metric :  {'exact_match': 1.96309963099631, 'f1': 6.353958784307238}\n",
            "valid:: Epoch: 0030 cost = 0.090873, squad_metric :  {'exact_match': 0.14237770771890573, 'f1': 3.8562488945032305}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhuJnQPAQweM"
      },
      "source": [
        "** Comment 3.1 **\n",
        "\n",
        "\"squad_metric\" has two metric, 'exact_match' and 'f1'.\n",
        "squad_metric get the sentence with string type, not list of tokens.\n",
        "Therefore, I joint the word? tokens with one space before put it in the given metric.\n",
        "After that, they use simple space splitter with split(), punctuation remover and Lowercase English for numericalizer.\n",
        "\\\n",
        "However, it is different from my numericalizer in Prob 2.2, the function 'process()'. I believe that this issue brings the harsh performance (especially, 'exact_match').\n",
        "\\\n",
        "For example, I double checked the 'exact_match' accuracy using only the start and end position of FIRST answer. Then, accuracy value is much higher than 'exact_match' value. (argmax acc on Prob 3.2)\n",
        "\n",
        "** Result 3.1.2 (Training & Validation) **\n",
        "\n",
        "\n",
        "```\n",
        "# Result ::\n",
        "\n",
        "device :  cuda\n",
        "batch_size :  128\n",
        "max_length :  253\n",
        "embedding_dim :  128\n",
        "hidden_dim :  256\n",
        "n_layers :  2\n",
        "emb_dropout :  0.1\n",
        "rnn_dropout_and_fc_dropout :  0.5\n",
        "bidirectional : True\n",
        "enable_layer_norm : True\n",
        "learning_rate :  0.001\n",
        "100%\n",
        "30/30 [52:55<00:00, 105.84s/it]\n",
        "train:: Epoch: 0001 cost = 0.079760, squad_metric :  {'exact_match': 0.05904059040590406, 'f1': 5.52339938822115}\n",
        "valid:: Epoch: 0001 cost = 0.079491, squad_metric :  {'exact_match': 0.040679345062544496, 'f1': 4.141770941182735}\n",
        "train:: Epoch: 0002 cost = 0.079107, squad_metric :  {'exact_match': 0.27921279212792127, 'f1': 4.516358336672799}\n",
        "valid:: Epoch: 0002 cost = 0.079342, squad_metric :  {'exact_match': 0.3356045967659921, 'f1': 4.610341876288495}\n",
        "train:: Epoch: 0003 cost = 0.078534, squad_metric :  {'exact_match': 0.5301353013530136, 'f1': 4.568132562807337}\n",
        "valid:: Epoch: 0003 cost = 0.079561, squad_metric :  {'exact_match': 0.4881521407505339, 'f1': 4.664714353463255}\n",
        "train:: Epoch: 0004 cost = 0.077731, squad_metric :  {'exact_match': 0.7380073800738007, 'f1': 4.899126557698399}\n",
        "valid:: Epoch: 0004 cost = 0.079900, squad_metric :  {'exact_match': 0.4779823044848978, 'f1': 4.848227029812154}\n",
        "train:: Epoch: 0005 cost = 0.076743, squad_metric :  {'exact_match': 0.915129151291513, 'f1': 5.173145030424936}\n",
        "valid:: Epoch: 0005 cost = 0.080609, squad_metric :  {'exact_match': 0.3966236143598088, 'f1': 4.724701760655256}\n",
        "train:: Epoch: 0006 cost = 0.075741, squad_metric :  {'exact_match': 0.998769987699877, 'f1': 5.401154805239039}\n",
        "valid:: Epoch: 0006 cost = 0.081186, squad_metric :  {'exact_match': 0.3966236143598088, 'f1': 4.607046859821393}\n",
        "train:: Epoch: 0007 cost = 0.074733, squad_metric :  {'exact_match': 1.137761377613776, 'f1': 5.502323035873868}\n",
        "valid:: Epoch: 0007 cost = 0.081922, squad_metric :  {'exact_match': 0.4271331231567172, 'f1': 4.551865069269158}\n",
        "train:: Epoch: 0008 cost = 0.073769, squad_metric :  {'exact_match': 1.204182041820418, 'f1': 5.661646000270595}\n",
        "valid:: Epoch: 0008 cost = 0.082516, squad_metric :  {'exact_match': 0.32543476050035597, 'f1': 4.222652645407895}\n",
        "train:: Epoch: 0009 cost = 0.072869, squad_metric :  {'exact_match': 1.3751537515375154, 'f1': 5.777759974047754}\n",
        "valid:: Epoch: 0009 cost = 0.083157, squad_metric :  {'exact_match': 0.3356045967659921, 'f1': 4.191452313944129}\n",
        "train:: Epoch: 0010 cost = 0.072054, squad_metric :  {'exact_match': 1.3714637146371464, 'f1': 5.790611846923638}\n",
        "valid:: Epoch: 0010 cost = 0.083795, squad_metric :  {'exact_match': 0.3152649242347198, 'f1': 4.178425942910128}\n",
        "train:: Epoch: 0011 cost = 0.071280, squad_metric :  {'exact_match': 1.3936039360393604, 'f1': 5.905970686867983}\n",
        "valid:: Epoch: 0011 cost = 0.084291, squad_metric :  {'exact_match': 0.3864537780941727, 'f1': 4.214191459500743}\n",
        "train:: Epoch: 0012 cost = 0.070560, squad_metric :  {'exact_match': 1.4772447724477245, 'f1': 5.87064883243569}\n",
        "valid:: Epoch: 0012 cost = 0.085198, squad_metric :  {'exact_match': 0.28475541543781147, 'f1': 4.112992304254845}\n",
        "train:: Epoch: 0013 cost = 0.069845, squad_metric :  {'exact_match': 1.5461254612546125, 'f1': 5.881162910122298}\n",
        "valid:: Epoch: 0013 cost = 0.085816, squad_metric :  {'exact_match': 0.3661141055629004, 'f1': 3.868770208199774}\n",
        "train:: Epoch: 0014 cost = 0.069205, squad_metric :  {'exact_match': 1.5362853628536286, 'f1': 6.011535672205428}\n",
        "valid:: Epoch: 0014 cost = 0.086096, squad_metric :  {'exact_match': 0.2644157429065392, 'f1': 3.9362743996948257}\n",
        "train:: Epoch: 0015 cost = 0.068615, squad_metric :  {'exact_match': 1.5817958179581795, 'f1': 5.894389559932659}\n",
        "valid:: Epoch: 0015 cost = 0.086543, squad_metric :  {'exact_match': 0.19322688904708635, 'f1': 3.936425858306148}\n",
        "train:: Epoch: 0016 cost = 0.068076, squad_metric :  {'exact_match': 1.6248462484624846, 'f1': 5.984265813361757}\n",
        "valid:: Epoch: 0016 cost = 0.087069, squad_metric :  {'exact_match': 0.3050950879690837, 'f1': 3.9069096913585173}\n",
        "train:: Epoch: 0017 cost = 0.067535, squad_metric :  {'exact_match': 1.5621156211562115, 'f1': 5.895822398815512}\n",
        "valid:: Epoch: 0017 cost = 0.087296, squad_metric :  {'exact_match': 0.20339672531272246, 'f1': 4.0214630329923375}\n",
        "train:: Epoch: 0018 cost = 0.066968, squad_metric :  {'exact_match': 1.6728167281672817, 'f1': 6.115089538610587}\n",
        "valid:: Epoch: 0018 cost = 0.088093, squad_metric :  {'exact_match': 0.28475541543781147, 'f1': 3.9959138206550118}\n",
        "train:: Epoch: 0019 cost = 0.066491, squad_metric :  {'exact_match': 1.7404674046740467, 'f1': 6.059234042789079}\n",
        "valid:: Epoch: 0019 cost = 0.087950, squad_metric :  {'exact_match': 0.2135665615783586, 'f1': 3.844752147723429}\n",
        "train:: Epoch: 0020 cost = 0.066065, squad_metric :  {'exact_match': 1.7699876998769988, 'f1': 6.149869687787748}\n",
        "valid:: Epoch: 0020 cost = 0.088572, squad_metric :  {'exact_match': 0.20339672531272246, 'f1': 3.84057640971435}\n",
        "train:: Epoch: 0021 cost = 0.065629, squad_metric :  {'exact_match': 1.7269372693726937, 'f1': 6.077576685990024}\n",
        "valid:: Epoch: 0021 cost = 0.089076, squad_metric :  {'exact_match': 0.2135665615783586, 'f1': 3.668484128969692}\n",
        "train:: Epoch: 0022 cost = 0.065204, squad_metric :  {'exact_match': 1.7798277982779829, 'f1': 6.191290550684122}\n",
        "valid:: Epoch: 0022 cost = 0.089141, squad_metric :  {'exact_match': 0.20339672531272246, 'f1': 3.762071948778777}\n",
        "train:: Epoch: 0023 cost = 0.064740, squad_metric :  {'exact_match': 1.8757687576875768, 'f1': 6.236611392119068}\n",
        "valid:: Epoch: 0023 cost = 0.089259, squad_metric :  {'exact_match': 0.2542459066409031, 'f1': 3.7709149997595928}\n",
        "train:: Epoch: 0024 cost = 0.064380, squad_metric :  {'exact_match': 1.8204182041820418, 'f1': 6.217691244710026}\n",
        "valid:: Epoch: 0024 cost = 0.089861, squad_metric :  {'exact_match': 0.11186819892199736, 'f1': 3.6802549974475056}\n",
        "train:: Epoch: 0025 cost = 0.063955, squad_metric :  {'exact_match': 1.91389913899139, 'f1': 6.317859799188245}\n",
        "valid:: Epoch: 0025 cost = 0.090195, squad_metric :  {'exact_match': 0.14237770771890573, 'f1': 3.795095819731973}\n",
        "train:: Epoch: 0026 cost = 0.063569, squad_metric :  {'exact_match': 1.895448954489545, 'f1': 6.309099116230587}\n",
        "valid:: Epoch: 0026 cost = 0.090376, squad_metric :  {'exact_match': 0.12203803518763348, 'f1': 3.627915130292577}\n",
        "train:: Epoch: 0027 cost = 0.063159, squad_metric :  {'exact_match': 1.954489544895449, 'f1': 6.303810374911365}\n",
        "valid:: Epoch: 0027 cost = 0.090593, squad_metric :  {'exact_match': 0.1322078714532696, 'f1': 3.7959551917323835}\n",
        "train:: Epoch: 0028 cost = 0.062920, squad_metric :  {'exact_match': 1.980319803198032, 'f1': 6.390372746952164}\n",
        "valid:: Epoch: 0028 cost = 0.090556, squad_metric :  {'exact_match': 0.15254754398454184, 'f1': 3.754198485161899}\n",
        "train:: Epoch: 0029 cost = 0.062495, squad_metric :  {'exact_match': 2.097170971709717, 'f1': 6.464851302425954}\n",
        "valid:: Epoch: 0029 cost = 0.091080, squad_metric :  {'exact_match': 0.12203803518763348, 'f1': 3.628710220283829}\n",
        "train:: Epoch: 0030 cost = 0.062238, squad_metric :  {'exact_match': 1.96309963099631, 'f1': 6.353958784307238}\n",
        "valid:: Epoch: 0030 cost = 0.090873, squad_metric :  {'exact_match': 0.14237770771890573, 'f1': 3.8562488945032305}\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr15BA5QKqTd"
      },
      "source": [
        "**Problem 3.2** *(10 points)*  Now let's resolve the second issue, by simply concatenating the two inputs into one sequence. The simplest way would be to append the the question at the start *OR* the end of the context. If you put it at the start, you will need to shift the start and the end positions of the answer accordingly. If you put it at the end, it will be necesary to use bidirectional LSTM for the context to be aware of what is ahead (though it is recommended to use bidirectional LSTM even if the question is appended at the start). Whichever you choose, carry it out and report the accuracy. How does it differ from 3.1?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_nAqQFEZ55R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fe2df4e-08a5-4697-f797-66ba1c905f6f"
      },
      "source": [
        "# Put the question sentence before the context sentence (for Prob 3.2+)\n",
        "# Use <CLS> token to separate two sentences\n",
        "\n",
        "import torchtext\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "from torchtext.legacy.data import BucketIterator\n",
        "\n",
        "\n",
        "class SQuAD2Dataset(data.Dataset):\n",
        "  \"\"\"\n",
        "  Defines a dataset for squad1.0.\n",
        "  \"\"\"\n",
        "  \n",
        "  @staticmethod\n",
        "  def sort_key(ex):\n",
        "    return data.interleave_keys(len(ex.context_question))\n",
        "\n",
        "  def __init__(self, data_list, fields, use_bos=True, max_length=None, **kwargs):\n",
        "    if not isinstance(fields[0], (tuple, list)):\n",
        "      fields = [\n",
        "                # ('context', fields[0]), \n",
        "                # ('question', fields[1]), \n",
        "                ('context_question', fields[0]), # For Problem 3.2+, put the question after the context\n",
        "                ('answer_start', fields[1]), \n",
        "                ('answer_end', fields[2]), \n",
        "                ('id_index', fields[3])\n",
        "                ]\n",
        "\n",
        "    examples = []\n",
        "    nonalpha_list = find_nonalpha_list(data_list)\n",
        "\n",
        "    self.id_list = list()\n",
        "    self.reference = list()\n",
        "\n",
        "    for _, example in enumerate(data_list):\n",
        "        out = preprocess(example, nonalpha_list)\n",
        "        # use data if the answer exists\n",
        "        if max_length and max_length < max(len(out['context']), len(out['question'])):\n",
        "            continue\n",
        "        if 'answers' in out.keys():\n",
        "            answer_start = out['answers'][0]['start'] # Use index 0 for instant valid accuracy\n",
        "            answer_end = out['answers'][0]['end'] # Use index 0 for instant valid accuracy\n",
        "            if use_bos:\n",
        "                answer_start += 1 # for <BOS> token\n",
        "                answer_end += 1 # for <BOS> token\n",
        "            examples.append(data.Example.fromlist([\n",
        "                                                #    out['context'], \n",
        "                                                #    out['question'], \n",
        "                                                   out['context'] + [\"<CLS>\"] + out['question'], # Use <CLS> token\n",
        "                                                   answer_start,\n",
        "                                                   answer_end,\n",
        "                                                   len(self.id_list)], \n",
        "                                                  fields))\n",
        "            self.id_list.append(out['id'])\n",
        "            self.reference.append({'id':example['id'], 'answers':example['answers']})\n",
        "\n",
        "    super(SQuAD2Dataset, self).__init__(examples, fields, **kwargs)\n",
        "\n",
        "\n",
        "class SQuAD2Dataloader():\n",
        "  \"\"\"\n",
        "  Make the dataloader for SQuAD 1.0\n",
        "  \"\"\"\n",
        "  def __init__(self, train_data=None, valid_data=None, batch_size=64, device='cpu', \n",
        "                max_length=255, min_freq=2, fix_length=None,\n",
        "                use_bos=True, use_eos=True, shuffle=True\n",
        "              ):\n",
        "\n",
        "    super(SQuAD2Dataloader, self).__init__()\n",
        "\n",
        "    self.text = data.Field(sequential=True, use_vocab=True, batch_first=True, \n",
        "                           include_lengths=True, fix_length=fix_length, \n",
        "                           init_token='<BOS>' if use_bos else None, \n",
        "                           eos_token='<EOS>' if use_eos else None\n",
        "                          )\n",
        "    self.answer_start = data.Field(sequential = False, use_vocab = False)\n",
        "    self.answer_end = data.Field(sequential = False, use_vocab = False)\n",
        "    self.id_index = data.Field(sequential = False, use_vocab = False)\n",
        "    \n",
        "    train = SQuAD2Dataset(data_list=train_data, \n",
        "                          fields = [\n",
        "                                    # ('context', self.text),\n",
        "                                    # ('question', self.text),\n",
        "                                    ('context_question', self.text),\n",
        "                                    ('answer_start', self.answer_start),\n",
        "                                    ('answer_end', self.answer_end),\n",
        "                                    ('id_index', self.id_index)\n",
        "                                    ], \n",
        "                          use_bos = use_bos,\n",
        "                          max_length = max_length\n",
        "                          )\n",
        "    valid = SQuAD2Dataset(data_list=valid_data, \n",
        "                          fields = [\n",
        "                                    # ('context', self.text),\n",
        "                                    # ('question', self.text),\n",
        "                                    ('context_question', self.text),\n",
        "                                    ('answer_start', self.answer_start),\n",
        "                                    ('answer_end', self.answer_end),\n",
        "                                    ('id_index', self.id_index)\n",
        "                                    ], \n",
        "                          use_bos = use_bos,\n",
        "                          max_length = max_length\n",
        "                          )\n",
        "    self.train_id_list = train.id_list\n",
        "    self.valid_id_list = valid.id_list\n",
        "\n",
        "    self.train_reference = train.reference\n",
        "    self.valid_reference = valid.reference\n",
        "    \n",
        "    self.train_iter = data.BucketIterator(train, batch_size=batch_size,\n",
        "                                          device=device,\n",
        "                                          shuffle=shuffle,\n",
        "                                          sort_key=lambda x: len(x.context_question), \n",
        "                                          sort_within_batch = True\n",
        "                                          )\n",
        "    self.valid_iter = data.BucketIterator(valid, batch_size=batch_size,\n",
        "                                          device=device,\n",
        "                                          shuffle=shuffle,\n",
        "                                          sort_key=lambda x: len(x.context_question),\n",
        "                                          sort_within_batch = True\n",
        "                                          )\n",
        "    \n",
        "    self.text.build_vocab(train)\n",
        "\n",
        "\n",
        "train_dataset = squad_dataset['train']\n",
        "valid_dataset = squad_dataset['validation']\n",
        "\n",
        "print('# of train data : {}'.format(len(train_dataset)))\n",
        "print('# of vaild data : {}'.format(len(valid_dataset)))\n",
        "\n",
        "batch_size = 128\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "max_length = 253 # 255 - 2 for <BOS> and <EOS>\n",
        "min_freq = 2\n",
        "use_bos = False\n",
        "use_eos = False\n",
        "print(\"device : \", device)\n",
        "\n",
        "loader = SQuAD2Dataloader(train_dataset, valid_dataset, batch_size=batch_size, \n",
        "                          device=device, max_length=max_length, min_freq=min_freq,\n",
        "                          use_bos=use_bos, use_eos=use_eos)\n",
        "print('\\nFinish making the dataloader')\n",
        "print(\"batch_size : \", batch_size)\n",
        "print(\"max_length : \", max_length)\n",
        "print('# of used train data ~ {}'.format((len(loader.train_iter)) * batch_size))\n",
        "print('# of used vaild data ~ {}'.format((len(loader.valid_iter)) * batch_size))\n",
        "\n",
        "vocab = loader.text.vocab\n",
        "vocab_list = list(vocab.stoi.keys())\n",
        "print('# of vocab : {}'.format(len(vocab_list)))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of train data : 87599\n",
            "# of vaild data : 10570\n",
            "device :  cuda\n",
            "\n",
            "Finish making the dataloader\n",
            "batch_size :  128\n",
            "max_length :  253\n",
            "# of used train data ~ 81408\n",
            "# of used vaild data ~ 9856\n",
            "# of vocab : 86390\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpOqM8mxUZPd"
      },
      "source": [
        "\n",
        "** Result 3.2.1 (Preprocessing) **\n",
        "\n",
        "```\n",
        "# of train data : 87599\n",
        "# of vaild data : 10570\n",
        "device :  cuda\n",
        "\n",
        "Finish making the dataloader\n",
        "batch_size :  128\n",
        "max_length :  253\n",
        "# of used train data ~ 81408\n",
        "# of used vaild data ~ 9856\n",
        "# of vocab : 86390\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXpHPdDLGdd0"
      },
      "source": [
        "**Answer 3.2**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGstR1-CGfr6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "72be28b00b274d759acda1728732bb0b",
            "e9fd50cd2f23464bbd8a4c150265933d",
            "173283dcd6834f3b9dd67ba595b37443",
            "554874a3c03a4ba5b42d824ba7738935",
            "6789272aca9049bb89a9227da3d34995",
            "f3968173b2f942b69a4f14318d376284",
            "e1cba5c2060b49d1be6b1c06bf38a0fc",
            "2ab9da4c64d54829a9405c416ad79b6c"
          ]
        },
        "outputId": "22f2264e-53ea-4c43-d3ad-24b38f6a0ef2"
      },
      "source": [
        "import torch.nn as nn\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device : \", device)\n",
        "\n",
        "# Vocabulary : Use vocab_list\n",
        "\n",
        "# Construct the LSTM Model\n",
        "embedding_dim = 128 # usually bigger, e.g. 128\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "n_label = max_length+1 if use_bos else max_length\n",
        "emb_dropout = 0.5\n",
        "rnn_dropout = 0.5\n",
        "bidirectional = True\n",
        "enable_layer_norm = True\n",
        "rnnmodel = ClassificationLSTMModel(embedding_dim, hidden_dim, n_layers, n_label, emb_dropout, rnn_dropout, bidirectional, enable_layer_norm, device).to(device)\n",
        "\n",
        "print(\"batch_size : \", batch_size)\n",
        "print(\"max_length : \", max_length)\n",
        "print(\"embedding_dim : \", embedding_dim)\n",
        "print(\"hidden_dim : \", hidden_dim)\n",
        "print(\"n_layers : \", n_layers)\n",
        "print(\"emb_dropout : \", emb_dropout)\n",
        "print(\"rnn_dropout_and_fc_dropout : \", rnn_dropout)\n",
        "if bidirectional:\n",
        "    print(\"bidirectional : True\")\n",
        "else:\n",
        "    print(\"bidirectional : False\")\n",
        "if enable_layer_norm:\n",
        "    print(\"enable_layer_norm : True\")\n",
        "else:\n",
        "    print(\"enable_layer_norm : False\")\n",
        "\n",
        "# Construct the data loader\n",
        "train_iter = loader.train_iter\n",
        "valid_iter = loader.valid_iter\n",
        "\n",
        "train_id_list = loader.train_id_list\n",
        "valid_id_list = loader.valid_id_list\n",
        "train_reference = loader.train_reference\n",
        "valid_reference = loader.valid_reference\n",
        "\n",
        "# Training\n",
        "learning_rate = 5e-4\n",
        "print(\"learning_rate : \", learning_rate)\n",
        "\n",
        "PAD_IDX = vocab.stoi['<pad>']\n",
        "cel = nn.CrossEntropyLoss(ignore_index=PAD_IDX) # Ignore Padding\n",
        "# optimizer = torch.optim.SGD(rnnmodel.parameters(), lr=1e-1)\n",
        "optimizer = torch.optim.Adam(rnnmodel.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 30\n",
        "max_norm = 5\n",
        "\n",
        "# Evaluate\n",
        "squad_metric = load_metric('squad')\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    train_loss = 0\n",
        "    train_accuracy = 0.0\n",
        "    train_data_num = 0\n",
        "    train_prediction = list()\n",
        "    for train_i, train_batch in enumerate(train_iter):\n",
        "        context, context_length = train_batch.context_question\n",
        "        answer_start = train_batch.answer_start\n",
        "        answer_end = train_batch.answer_end\n",
        "        train_id_index = train_batch.id_index\n",
        "\n",
        "        logits_start, logits_end = rnnmodel(context, context_length)\n",
        "\n",
        "        optimizer.zero_grad() # reset process\n",
        "        loss = cel(logits_start, answer_start) + cel(logits_end, answer_end) # Loss, a.k.a L\n",
        "        loss.backward() # compute gradients\n",
        "        # print(torch.norm(rnnmodel.lstm.weight_hh_l0.grad), loss.item())\n",
        "        # torch.nn.utils.clip_grad_norm_(rnnmodel.parameters(), max_norm) # gradent clipping\n",
        "        optimizer.step() # update parameters\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        _, train_start_preds = torch.max(logits_start, 1)\n",
        "        _, train_end_preds = torch.max(logits_end, 1)\n",
        "        train_accuracy += ((train_start_preds == answer_start) * (train_end_preds == answer_end)).sum().float()\n",
        "\n",
        "        train_data_num += context.shape[0]\n",
        "\n",
        "        for train_j in range(context.shape[0]):\n",
        "            pred_text = \"\"\n",
        "            start = train_start_preds[train_j]\n",
        "            end = train_end_preds[train_j]\n",
        "            if start < end:\n",
        "                pred_text = [vocab_list[text_id] for text_id in context[train_j][start:end+1]]\n",
        "                pred_text = \" \".join(pred_text)\n",
        "\n",
        "            # start = answer_start[train_j]\n",
        "            # end = answer_end[train_j]\n",
        "            # answer_text = [vocab_list[text_id] for text_id in context[train_j][start:end+1]]\n",
        "            # answer_text = \" \".join(answer_text)\n",
        "            # print(pred_text, answer_text, train_reference[train_id_index[train_j]], \"\\n\")\n",
        "\n",
        "            train_prediction.append({'id':train_id_list[train_id_index[train_j]], 'prediction_text':pred_text})\n",
        "\n",
        "    train_result = squad_metric.compute(predictions=train_prediction, references=train_reference)\n",
        "    print('train:: Epoch:', '%04d' % (epoch + 1), \n",
        "          'cost =', '{:.6f},'.format(train_loss / train_data_num), \n",
        "          'my exact_match =', '{:.6f}'.format(train_accuracy / train_data_num),\n",
        "          'other_squad_metric : ', train_result)\n",
        "        \n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        with torch.no_grad():\n",
        "            valid_loss = 0\n",
        "            valid_accuracy = 0.0\n",
        "            valid_data_num = 0\n",
        "            valid_prediction = list()\n",
        "            for valid_i, valid_batch in enumerate(valid_iter):\n",
        "                context, context_length = valid_batch.context_question\n",
        "                answer_start = valid_batch.answer_start\n",
        "                answer_end = valid_batch.answer_end\n",
        "                valid_id_index = valid_batch.id_index\n",
        "\n",
        "                logits_start, logits_end = rnnmodel(context, context_length)\n",
        "\n",
        "                loss = cel(logits_start, answer_start) + cel(logits_end, answer_end) # Loss, a.k.a L\n",
        "                valid_loss += loss.item()\n",
        "\n",
        "                _, valid_start_preds = torch.max(logits_start, 1)\n",
        "                _, valid_end_preds = torch.max(logits_end, 1)\n",
        "                valid_accuracy += ((valid_start_preds == answer_start) * (valid_end_preds == answer_end)).sum().float()\n",
        "\n",
        "                valid_data_num += context.shape[0]\n",
        "\n",
        "                for valid_j in range(context.shape[0]):\n",
        "                    pred_text = \"\"\n",
        "                    start = valid_start_preds[valid_j]\n",
        "                    end = valid_end_preds[valid_j]\n",
        "                    if start < end:\n",
        "                        pred_text = [vocab_list[text_id] for text_id in context[valid_j][start:end+1]]\n",
        "                        pred_text = \" \".join(pred_text)\n",
        "                    valid_prediction.append({'id':valid_id_list[valid_id_index[valid_j]], 'prediction_text':pred_text})\n",
        "                \n",
        "            valid_result = squad_metric.compute(predictions=valid_prediction, references=valid_reference)\n",
        "            print('valid:: Epoch:', '%04d' % (epoch + 1), \n",
        "                  'cost =', '{:.6f},'.format(valid_loss / valid_data_num), \n",
        "                  'my exact_match =', '{:.6f},'.format(valid_accuracy / valid_data_num),\n",
        "                  'other_squad_metric : ', valid_result)\n",
        "            \n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device :  cuda\n",
            "batch_size :  128\n",
            "max_length :  253\n",
            "embedding_dim :  128\n",
            "hidden_dim :  256\n",
            "n_layers :  2\n",
            "emb_dropout :  0.5\n",
            "rnn_dropout_and_fc_dropout :  0.5\n",
            "bidirectional : True\n",
            "enable_layer_norm : True\n",
            "learning_rate :  0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72be28b00b274d759acda1728732bb0b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "train:: Epoch: 0001 cost = 0.079870, argmax acc = 0.000480 other_squad_metric :  {'exact_match': 0.05289052890528905, 'f1': 5.989309692302495}\n",
            "valid:: Epoch: 0001 cost = 0.079658, argmax acc = 0.001119, other_squad_metric :  {'exact_match': 0.010169836265636124, 'f1': 6.076181438808642}\n",
            "train:: Epoch: 0002 cost = 0.079319, argmax acc = 0.002030 other_squad_metric :  {'exact_match': 0.05166051660516605, 'f1': 5.409903198798298}\n",
            "valid:: Epoch: 0002 cost = 0.079311, argmax acc = 0.003458, other_squad_metric :  {'exact_match': 0.06101901759381674, 'f1': 5.7264634760190045}\n",
            "train:: Epoch: 0003 cost = 0.079070, argmax acc = 0.004600 other_squad_metric :  {'exact_match': 0.12915129151291513, 'f1': 4.623574903221184}\n",
            "valid:: Epoch: 0003 cost = 0.079317, argmax acc = 0.004271, other_squad_metric :  {'exact_match': 0.1830570527814502, 'f1': 4.96396233745034}\n",
            "train:: Epoch: 0004 cost = 0.078806, argmax acc = 0.007085 other_squad_metric :  {'exact_match': 0.3124231242312423, 'f1': 4.482858149196208}\n",
            "valid:: Epoch: 0004 cost = 0.079252, argmax acc = 0.007221, other_squad_metric :  {'exact_match': 0.3356045967659921, 'f1': 4.793506052164472}\n",
            "train:: Epoch: 0005 cost = 0.078508, argmax acc = 0.008708 other_squad_metric :  {'exact_match': 0.42066420664206644, 'f1': 4.502234244770526}\n",
            "valid:: Epoch: 0005 cost = 0.079273, argmax acc = 0.007627, other_squad_metric :  {'exact_match': 0.3966236143598088, 'f1': 5.125529379910425}\n",
            "train:: Epoch: 0006 cost = 0.078141, argmax acc = 0.010185 other_squad_metric :  {'exact_match': 0.5768757687576875, 'f1': 4.7975583007366245}\n",
            "valid:: Epoch: 0006 cost = 0.079312, argmax acc = 0.009051, other_squad_metric :  {'exact_match': 0.46781246821926165, 'f1': 4.773259851830603}\n",
            "train:: Epoch: 0007 cost = 0.077821, argmax acc = 0.011021 other_squad_metric :  {'exact_match': 0.5953259532595326, 'f1': 4.83455868092752}\n",
            "valid:: Epoch: 0007 cost = 0.079348, argmax acc = 0.009356, other_squad_metric :  {'exact_match': 0.41696328689108103, 'f1': 5.091698404526903}\n",
            "train:: Epoch: 0008 cost = 0.077476, argmax acc = 0.012362 other_squad_metric :  {'exact_match': 0.6629766297662977, 'f1': 4.984677232806258}\n",
            "valid:: Epoch: 0008 cost = 0.079552, argmax acc = 0.008034, other_squad_metric :  {'exact_match': 0.5390013220787145, 'f1': 4.841777080462419}\n",
            "train:: Epoch: 0009 cost = 0.077151, argmax acc = 0.013542 other_squad_metric :  {'exact_match': 0.7429274292742928, 'f1': 5.147279034148527}\n",
            "valid:: Epoch: 0009 cost = 0.079495, argmax acc = 0.008949, other_squad_metric :  {'exact_match': 0.4474727956879894, 'f1': 4.919948065225899}\n",
            "train:: Epoch: 0010 cost = 0.076794, argmax acc = 0.013887 other_squad_metric :  {'exact_match': 0.7761377613776138, 'f1': 5.2560421593567135}\n",
            "valid:: Epoch: 0010 cost = 0.079807, argmax acc = 0.009051, other_squad_metric :  {'exact_match': 0.5186616495474423, 'f1': 5.023962187600448}\n",
            "train:: Epoch: 0011 cost = 0.076487, argmax acc = 0.014772 other_squad_metric :  {'exact_match': 0.8597785977859779, 'f1': 5.355466359956851}\n",
            "valid:: Epoch: 0011 cost = 0.080142, argmax acc = 0.008238, other_squad_metric :  {'exact_match': 0.49832197701617004, 'f1': 4.92387781952113}\n",
            "train:: Epoch: 0012 cost = 0.076147, argmax acc = 0.015166 other_squad_metric :  {'exact_match': 0.9298892988929889, 'f1': 5.58156635329186}\n",
            "valid:: Epoch: 0012 cost = 0.080227, argmax acc = 0.010475, other_squad_metric :  {'exact_match': 0.5898505034068952, 'f1': 4.9997467309597}\n",
            "train:: Epoch: 0013 cost = 0.075850, argmax acc = 0.016396 other_squad_metric :  {'exact_match': 0.980319803198032, 'f1': 5.709361305923348}\n",
            "valid:: Epoch: 0013 cost = 0.080358, argmax acc = 0.008339, other_squad_metric :  {'exact_match': 0.5084918132818061, 'f1': 4.906783610764015}\n",
            "train:: Epoch: 0014 cost = 0.075527, argmax acc = 0.016002 other_squad_metric :  {'exact_match': 1.030750307503075, 'f1': 5.7895020394261865}\n",
            "valid:: Epoch: 0014 cost = 0.080466, argmax acc = 0.009356, other_squad_metric :  {'exact_match': 0.5186616495474423, 'f1': 4.8639803711206095}\n",
            "train:: Epoch: 0015 cost = 0.075258, argmax acc = 0.017122 other_squad_metric :  {'exact_match': 1.039360393603936, 'f1': 5.929675999299098}\n",
            "valid:: Epoch: 0015 cost = 0.080841, argmax acc = 0.009763, other_squad_metric :  {'exact_match': 0.4474727956879894, 'f1': 4.720280108404927}\n",
            "train:: Epoch: 0016 cost = 0.074940, argmax acc = 0.017294 other_squad_metric :  {'exact_match': 1.0922509225092252, 'f1': 5.9386564306645875}\n",
            "valid:: Epoch: 0016 cost = 0.080967, argmax acc = 0.009661, other_squad_metric :  {'exact_match': 0.5695108308756229, 'f1': 4.737350504812802}\n",
            "train:: Epoch: 0017 cost = 0.074684, argmax acc = 0.018192 other_squad_metric :  {'exact_match': 1.118081180811808, 'f1': 6.067873580600416}\n",
            "valid:: Epoch: 0017 cost = 0.080978, argmax acc = 0.009356, other_squad_metric :  {'exact_match': 0.4779823044848978, 'f1': 4.737894245953374}\n",
            "train:: Epoch: 0018 cost = 0.074393, argmax acc = 0.018893 other_squad_metric :  {'exact_match': 1.153751537515375, 'f1': 6.085247510336405}\n",
            "valid:: Epoch: 0018 cost = 0.081367, argmax acc = 0.008034, other_squad_metric :  {'exact_match': 0.4067934506254449, 'f1': 4.725415697406601}\n",
            "train:: Epoch: 0019 cost = 0.074138, argmax acc = 0.019200 other_squad_metric :  {'exact_match': 1.2263222632226323, 'f1': 6.192327102928718}\n",
            "valid:: Epoch: 0019 cost = 0.081337, argmax acc = 0.009356, other_squad_metric :  {'exact_match': 0.4779823044848978, 'f1': 4.761990326948866}\n",
            "train:: Epoch: 0020 cost = 0.073868, argmax acc = 0.019828 other_squad_metric :  {'exact_match': 1.2484624846248462, 'f1': 6.259461564994696}\n",
            "valid:: Epoch: 0020 cost = 0.081493, argmax acc = 0.009966, other_squad_metric :  {'exact_match': 0.5390013220787145, 'f1': 4.915876268127727}\n",
            "train:: Epoch: 0021 cost = 0.073591, argmax acc = 0.020234 other_squad_metric :  {'exact_match': 1.2226322263222633, 'f1': 6.320693636443308}\n",
            "valid:: Epoch: 0021 cost = 0.081802, argmax acc = 0.008238, other_squad_metric :  {'exact_match': 0.5593409946099868, 'f1': 4.731472551932656}\n",
            "train:: Epoch: 0022 cost = 0.073366, argmax acc = 0.020677 other_squad_metric :  {'exact_match': 1.3665436654366543, 'f1': 6.410087668775246}\n",
            "valid:: Epoch: 0022 cost = 0.082137, argmax acc = 0.008441, other_squad_metric :  {'exact_match': 0.4881521407505339, 'f1': 4.748025437134684}\n",
            "train:: Epoch: 0023 cost = 0.073149, argmax acc = 0.020480 other_squad_metric :  {'exact_match': 1.2878228782287824, 'f1': 6.436317355530415}\n",
            "valid:: Epoch: 0023 cost = 0.082268, argmax acc = 0.008238, other_squad_metric :  {'exact_match': 0.5084918132818061, 'f1': 4.577179652162081}\n",
            "train:: Epoch: 0024 cost = 0.072879, argmax acc = 0.021611 other_squad_metric :  {'exact_match': 1.3284132841328413, 'f1': 6.534731076891177}\n",
            "valid:: Epoch: 0024 cost = 0.082333, argmax acc = 0.008746, other_squad_metric :  {'exact_match': 0.49832197701617004, 'f1': 4.854091224465016}\n",
            "train:: Epoch: 0025 cost = 0.072581, argmax acc = 0.021771 other_squad_metric :  {'exact_match': 1.3468634686346863, 'f1': 6.566252875913047}\n",
            "valid:: Epoch: 0025 cost = 0.082600, argmax acc = 0.008136, other_squad_metric :  {'exact_match': 0.6203600122038035, 'f1': 4.427058829862266}\n",
            "train:: Epoch: 0026 cost = 0.072373, argmax acc = 0.022226 other_squad_metric :  {'exact_match': 1.3825338253382533, 'f1': 6.691890118043898}\n",
            "valid:: Epoch: 0026 cost = 0.082687, argmax acc = 0.009661, other_squad_metric :  {'exact_match': 0.5084918132818061, 'f1': 4.726996743387878}\n",
            "train:: Epoch: 0027 cost = 0.072126, argmax acc = 0.022128 other_squad_metric :  {'exact_match': 1.3800738007380073, 'f1': 6.704347870959806}\n",
            "valid:: Epoch: 0027 cost = 0.083415, argmax acc = 0.007119, other_squad_metric :  {'exact_match': 0.3559442692972643, 'f1': 4.3227320610159}\n",
            "train:: Epoch: 0028 cost = 0.071940, argmax acc = 0.022964 other_squad_metric :  {'exact_match': 1.4858548585485856, 'f1': 6.8166554814508356}\n",
            "valid:: Epoch: 0028 cost = 0.082823, argmax acc = 0.009458, other_squad_metric :  {'exact_match': 0.5695108308756229, 'f1': 4.664080646130991}\n",
            "train:: Epoch: 0029 cost = 0.071652, argmax acc = 0.023149 other_squad_metric :  {'exact_match': 1.4710947109471095, 'f1': 6.8521612048196765}\n",
            "valid:: Epoch: 0029 cost = 0.083159, argmax acc = 0.007526, other_squad_metric :  {'exact_match': 0.4373029594223533, 'f1': 4.5875795966660275}\n",
            "train:: Epoch: 0030 cost = 0.071447, argmax acc = 0.024428 other_squad_metric :  {'exact_match': 1.4920049200492005, 'f1': 6.911899083776001}\n",
            "valid:: Epoch: 0030 cost = 0.083746, argmax acc = 0.006814, other_squad_metric :  {'exact_match': 0.4271331231567172, 'f1': 4.501357764974634}\n",
            "train:: Epoch: 0031 cost = 0.071172, argmax acc = 0.024945 other_squad_metric :  {'exact_match': 1.5891758917589176, 'f1': 7.14310523963246}\n",
            "valid:: Epoch: 0031 cost = 0.083821, argmax acc = 0.007424, other_squad_metric :  {'exact_match': 0.4474727956879894, 'f1': 4.426732009177166}\n",
            "train:: Epoch: 0032 cost = 0.070933, argmax acc = 0.025178 other_squad_metric :  {'exact_match': 1.5707257072570726, 'f1': 7.100587584299848}\n",
            "valid:: Epoch: 0032 cost = 0.084167, argmax acc = 0.007322, other_squad_metric :  {'exact_match': 0.5288314858130784, 'f1': 4.336364783978499}\n",
            "train:: Epoch: 0033 cost = 0.070716, argmax acc = 0.025646 other_squad_metric :  {'exact_match': 1.6408364083640836, 'f1': 7.230598951569533}\n",
            "valid:: Epoch: 0033 cost = 0.084090, argmax acc = 0.007322, other_squad_metric :  {'exact_match': 0.5084918132818061, 'f1': 4.543467719005027}\n",
            "train:: Epoch: 0034 cost = 0.070453, argmax acc = 0.025806 other_squad_metric :  {'exact_match': 1.6186961869618697, 'f1': 7.191681394622516}\n",
            "valid:: Epoch: 0034 cost = 0.084361, argmax acc = 0.007526, other_squad_metric :  {'exact_match': 0.46781246821926165, 'f1': 4.736551729213181}\n",
            "train:: Epoch: 0035 cost = 0.070197, argmax acc = 0.026556 other_squad_metric :  {'exact_match': 1.6137761377613775, 'f1': 7.2650431586045725}\n",
            "valid:: Epoch: 0035 cost = 0.084460, argmax acc = 0.008034, other_squad_metric :  {'exact_match': 0.5084918132818061, 'f1': 4.496105225498365}\n",
            "train:: Epoch: 0036 cost = 0.070010, argmax acc = 0.027011 other_squad_metric :  {'exact_match': 1.6863468634686347, 'f1': 7.352410576934097}\n",
            "valid:: Epoch: 0036 cost = 0.084535, argmax acc = 0.007831, other_squad_metric :  {'exact_match': 0.4779823044848978, 'f1': 4.4192404175252715}\n",
            "train:: Epoch: 0037 cost = 0.069749, argmax acc = 0.027835 other_squad_metric :  {'exact_match': 1.7392373923739237, 'f1': 7.513770905912509}\n",
            "valid:: Epoch: 0037 cost = 0.084879, argmax acc = 0.006814, other_squad_metric :  {'exact_match': 0.3864537780941727, 'f1': 4.509360088792529}\n",
            "train:: Epoch: 0038 cost = 0.069538, argmax acc = 0.028352 other_squad_metric :  {'exact_match': 1.795817958179582, 'f1': 7.567768556050303}\n",
            "valid:: Epoch: 0038 cost = 0.085208, argmax acc = 0.007119, other_squad_metric :  {'exact_match': 0.3152649242347198, 'f1': 4.320164108583346}\n",
            "train:: Epoch: 0039 cost = 0.069376, argmax acc = 0.028893 other_squad_metric :  {'exact_match': 1.7761377613776137, 'f1': 7.477864285397355}\n",
            "valid:: Epoch: 0039 cost = 0.085120, argmax acc = 0.007119, other_squad_metric :  {'exact_match': 0.3762839418285366, 'f1': 4.465381510521311}\n",
            "train:: Epoch: 0040 cost = 0.069104, argmax acc = 0.028647 other_squad_metric :  {'exact_match': 1.7416974169741697, 'f1': 7.64480520077062}\n",
            "valid:: Epoch: 0040 cost = 0.085419, argmax acc = 0.006509, other_squad_metric :  {'exact_match': 0.3661141055629004, 'f1': 4.3012572132869}\n",
            "train:: Epoch: 0041 cost = 0.068922, argmax acc = 0.029397 other_squad_metric :  {'exact_match': 1.897908979089791, 'f1': 7.657625834154736}\n",
            "valid:: Epoch: 0041 cost = 0.085548, argmax acc = 0.007017, other_squad_metric :  {'exact_match': 0.4373029594223533, 'f1': 4.419533677236369}\n",
            "train:: Epoch: 0042 cost = 0.068677, argmax acc = 0.030074 other_squad_metric :  {'exact_match': 1.8560885608856088, 'f1': 7.782342928965346}\n",
            "valid:: Epoch: 0042 cost = 0.085584, argmax acc = 0.007119, other_squad_metric :  {'exact_match': 0.4373029594223533, 'f1': 4.279498076008183}\n",
            "train:: Epoch: 0043 cost = 0.068484, argmax acc = 0.030578 other_squad_metric :  {'exact_match': 1.8929889298892988, 'f1': 7.918387526381218}\n",
            "valid:: Epoch: 0043 cost = 0.086364, argmax acc = 0.005492, other_squad_metric :  {'exact_match': 0.4373029594223533, 'f1': 4.302321630297892}\n",
            "train:: Epoch: 0044 cost = 0.068248, argmax acc = 0.030664 other_squad_metric :  {'exact_match': 1.952029520295203, 'f1': 7.9328011644837195}\n",
            "valid:: Epoch: 0044 cost = 0.085853, argmax acc = 0.007322, other_squad_metric :  {'exact_match': 0.4779823044848978, 'f1': 4.6060141788937745}\n",
            "train:: Epoch: 0045 cost = 0.068045, argmax acc = 0.030812 other_squad_metric :  {'exact_match': 1.944649446494465, 'f1': 8.078085092699313}\n",
            "valid:: Epoch: 0045 cost = 0.086303, argmax acc = 0.006509, other_squad_metric :  {'exact_match': 0.5084918132818061, 'f1': 4.511649043562105}\n",
            "train:: Epoch: 0046 cost = 0.067739, argmax acc = 0.032706 other_squad_metric :  {'exact_match': 2.025830258302583, 'f1': 8.129184878529529}\n",
            "valid:: Epoch: 0046 cost = 0.086710, argmax acc = 0.004882, other_squad_metric :  {'exact_match': 0.4373029594223533, 'f1': 4.139784476826585}\n",
            "train:: Epoch: 0047 cost = 0.067588, argmax acc = 0.031587 other_squad_metric :  {'exact_match': 1.985239852398524, 'f1': 8.122182574030209}\n",
            "valid:: Epoch: 0047 cost = 0.086525, argmax acc = 0.005390, other_squad_metric :  {'exact_match': 0.41696328689108103, 'f1': 4.51468017892951}\n",
            "train:: Epoch: 0048 cost = 0.067337, argmax acc = 0.032386 other_squad_metric :  {'exact_match': 2.054120541205412, 'f1': 8.30270446700247}\n",
            "valid:: Epoch: 0048 cost = 0.087089, argmax acc = 0.005492, other_squad_metric :  {'exact_match': 0.3356045967659921, 'f1': 4.134395096402048}\n",
            "train:: Epoch: 0049 cost = 0.067101, argmax acc = 0.033432 other_squad_metric :  {'exact_match': 2.1316113161131613, 'f1': 8.364209134811277}\n",
            "valid:: Epoch: 0049 cost = 0.087063, argmax acc = 0.005593, other_squad_metric :  {'exact_match': 0.41696328689108103, 'f1': 4.447391790002926}\n",
            "train:: Epoch: 0050 cost = 0.066838, argmax acc = 0.033137 other_squad_metric :  {'exact_match': 2.2189421894218944, 'f1': 8.515114560353371}\n",
            "valid:: Epoch: 0050 cost = 0.087535, argmax acc = 0.005288, other_squad_metric :  {'exact_match': 0.32543476050035597, 'f1': 4.027121477755915}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5FA-w0YUoVj"
      },
      "source": [
        "** Comment 3.2 **\n",
        "\n",
        "Still bad performance.\n",
        "\n",
        "** Result 3.2.2 (Training & Validation) **\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "device :  cuda\n",
        "batch_size :  128\n",
        "max_length :  253\n",
        "embedding_dim :  128\n",
        "hidden_dim :  256\n",
        "n_layers :  2\n",
        "emb_dropout :  0.5\n",
        "rnn_dropout_and_fc_dropout :  0.5\n",
        "bidirectional : True\n",
        "enable_layer_norm : True\n",
        "learning_rate :  0.0005\n",
        "100%\n",
        "50/50 [1:29:20<00:00, 107.20s/it]\n",
        "train:: Epoch: 0001 cost = 0.079870, argmax acc = 0.000480 other_squad_metric :  {'exact_match': 0.05289052890528905, 'f1': 5.989309692302495}\n",
        "valid:: Epoch: 0001 cost = 0.079658, argmax acc = 0.001119, other_squad_metric :  {'exact_match': 0.010169836265636124, 'f1': 6.076181438808642}\n",
        "train:: Epoch: 0002 cost = 0.079319, argmax acc = 0.002030 other_squad_metric :  {'exact_match': 0.05166051660516605, 'f1': 5.409903198798298}\n",
        "valid:: Epoch: 0002 cost = 0.079311, argmax acc = 0.003458, other_squad_metric :  {'exact_match': 0.06101901759381674, 'f1': 5.7264634760190045}\n",
        "train:: Epoch: 0003 cost = 0.079070, argmax acc = 0.004600 other_squad_metric :  {'exact_match': 0.12915129151291513, 'f1': 4.623574903221184}\n",
        "valid:: Epoch: 0003 cost = 0.079317, argmax acc = 0.004271, other_squad_metric :  {'exact_match': 0.1830570527814502, 'f1': 4.96396233745034}\n",
        "train:: Epoch: 0004 cost = 0.078806, argmax acc = 0.007085 other_squad_metric :  {'exact_match': 0.3124231242312423, 'f1': 4.482858149196208}\n",
        "valid:: Epoch: 0004 cost = 0.079252, argmax acc = 0.007221, other_squad_metric :  {'exact_match': 0.3356045967659921, 'f1': 4.793506052164472}\n",
        "train:: Epoch: 0005 cost = 0.078508, argmax acc = 0.008708 other_squad_metric :  {'exact_match': 0.42066420664206644, 'f1': 4.502234244770526}\n",
        "valid:: Epoch: 0005 cost = 0.079273, argmax acc = 0.007627, other_squad_metric :  {'exact_match': 0.3966236143598088, 'f1': 5.125529379910425}\n",
        "train:: Epoch: 0006 cost = 0.078141, argmax acc = 0.010185 other_squad_metric :  {'exact_match': 0.5768757687576875, 'f1': 4.7975583007366245}\n",
        "valid:: Epoch: 0006 cost = 0.079312, argmax acc = 0.009051, other_squad_metric :  {'exact_match': 0.46781246821926165, 'f1': 4.773259851830603}\n",
        "train:: Epoch: 0007 cost = 0.077821, argmax acc = 0.011021 other_squad_metric :  {'exact_match': 0.5953259532595326, 'f1': 4.83455868092752}\n",
        "valid:: Epoch: 0007 cost = 0.079348, argmax acc = 0.009356, other_squad_metric :  {'exact_match': 0.41696328689108103, 'f1': 5.091698404526903}\n",
        "train:: Epoch: 0008 cost = 0.077476, argmax acc = 0.012362 other_squad_metric :  {'exact_match': 0.6629766297662977, 'f1': 4.984677232806258}\n",
        "valid:: Epoch: 0008 cost = 0.079552, argmax acc = 0.008034, other_squad_metric :  {'exact_match': 0.5390013220787145, 'f1': 4.841777080462419}\n",
        "train:: Epoch: 0009 cost = 0.077151, argmax acc = 0.013542 other_squad_metric :  {'exact_match': 0.7429274292742928, 'f1': 5.147279034148527}\n",
        "valid:: Epoch: 0009 cost = 0.079495, argmax acc = 0.008949, other_squad_metric :  {'exact_match': 0.4474727956879894, 'f1': 4.919948065225899}\n",
        "train:: Epoch: 0010 cost = 0.076794, argmax acc = 0.013887 other_squad_metric :  {'exact_match': 0.7761377613776138, 'f1': 5.2560421593567135}\n",
        "valid:: Epoch: 0010 cost = 0.079807, argmax acc = 0.009051, other_squad_metric :  {'exact_match': 0.5186616495474423, 'f1': 5.023962187600448}\n",
        "train:: Epoch: 0011 cost = 0.076487, argmax acc = 0.014772 other_squad_metric :  {'exact_match': 0.8597785977859779, 'f1': 5.355466359956851}\n",
        "valid:: Epoch: 0011 cost = 0.080142, argmax acc = 0.008238, other_squad_metric :  {'exact_match': 0.49832197701617004, 'f1': 4.92387781952113}\n",
        "train:: Epoch: 0012 cost = 0.076147, argmax acc = 0.015166 other_squad_metric :  {'exact_match': 0.9298892988929889, 'f1': 5.58156635329186}\n",
        "valid:: Epoch: 0012 cost = 0.080227, argmax acc = 0.010475, other_squad_metric :  {'exact_match': 0.5898505034068952, 'f1': 4.9997467309597}\n",
        "train:: Epoch: 0013 cost = 0.075850, argmax acc = 0.016396 other_squad_metric :  {'exact_match': 0.980319803198032, 'f1': 5.709361305923348}\n",
        "valid:: Epoch: 0013 cost = 0.080358, argmax acc = 0.008339, other_squad_metric :  {'exact_match': 0.5084918132818061, 'f1': 4.906783610764015}\n",
        "train:: Epoch: 0014 cost = 0.075527, argmax acc = 0.016002 other_squad_metric :  {'exact_match': 1.030750307503075, 'f1': 5.7895020394261865}\n",
        "valid:: Epoch: 0014 cost = 0.080466, argmax acc = 0.009356, other_squad_metric :  {'exact_match': 0.5186616495474423, 'f1': 4.8639803711206095}\n",
        "train:: Epoch: 0015 cost = 0.075258, argmax acc = 0.017122 other_squad_metric :  {'exact_match': 1.039360393603936, 'f1': 5.929675999299098}\n",
        "valid:: Epoch: 0015 cost = 0.080841, argmax acc = 0.009763, other_squad_metric :  {'exact_match': 0.4474727956879894, 'f1': 4.720280108404927}\n",
        "train:: Epoch: 0016 cost = 0.074940, argmax acc = 0.017294 other_squad_metric :  {'exact_match': 1.0922509225092252, 'f1': 5.9386564306645875}\n",
        "valid:: Epoch: 0016 cost = 0.080967, argmax acc = 0.009661, other_squad_metric :  {'exact_match': 0.5695108308756229, 'f1': 4.737350504812802}\n",
        "train:: Epoch: 0017 cost = 0.074684, argmax acc = 0.018192 other_squad_metric :  {'exact_match': 1.118081180811808, 'f1': 6.067873580600416}\n",
        "valid:: Epoch: 0017 cost = 0.080978, argmax acc = 0.009356, other_squad_metric :  {'exact_match': 0.4779823044848978, 'f1': 4.737894245953374}\n",
        "train:: Epoch: 0018 cost = 0.074393, argmax acc = 0.018893 other_squad_metric :  {'exact_match': 1.153751537515375, 'f1': 6.085247510336405}\n",
        "valid:: Epoch: 0018 cost = 0.081367, argmax acc = 0.008034, other_squad_metric :  {'exact_match': 0.4067934506254449, 'f1': 4.725415697406601}\n",
        "train:: Epoch: 0019 cost = 0.074138, argmax acc = 0.019200 other_squad_metric :  {'exact_match': 1.2263222632226323, 'f1': 6.192327102928718}\n",
        "valid:: Epoch: 0019 cost = 0.081337, argmax acc = 0.009356, other_squad_metric :  {'exact_match': 0.4779823044848978, 'f1': 4.761990326948866}\n",
        "train:: Epoch: 0020 cost = 0.073868, argmax acc = 0.019828 other_squad_metric :  {'exact_match': 1.2484624846248462, 'f1': 6.259461564994696}\n",
        "valid:: Epoch: 0020 cost = 0.081493, argmax acc = 0.009966, other_squad_metric :  {'exact_match': 0.5390013220787145, 'f1': 4.915876268127727}\n",
        "train:: Epoch: 0021 cost = 0.073591, argmax acc = 0.020234 other_squad_metric :  {'exact_match': 1.2226322263222633, 'f1': 6.320693636443308}\n",
        "valid:: Epoch: 0021 cost = 0.081802, argmax acc = 0.008238, other_squad_metric :  {'exact_match': 0.5593409946099868, 'f1': 4.731472551932656}\n",
        "train:: Epoch: 0022 cost = 0.073366, argmax acc = 0.020677 other_squad_metric :  {'exact_match': 1.3665436654366543, 'f1': 6.410087668775246}\n",
        "valid:: Epoch: 0022 cost = 0.082137, argmax acc = 0.008441, other_squad_metric :  {'exact_match': 0.4881521407505339, 'f1': 4.748025437134684}\n",
        "train:: Epoch: 0023 cost = 0.073149, argmax acc = 0.020480 other_squad_metric :  {'exact_match': 1.2878228782287824, 'f1': 6.436317355530415}\n",
        "valid:: Epoch: 0023 cost = 0.082268, argmax acc = 0.008238, other_squad_metric :  {'exact_match': 0.5084918132818061, 'f1': 4.577179652162081}\n",
        "train:: Epoch: 0024 cost = 0.072879, argmax acc = 0.021611 other_squad_metric :  {'exact_match': 1.3284132841328413, 'f1': 6.534731076891177}\n",
        "valid:: Epoch: 0024 cost = 0.082333, argmax acc = 0.008746, other_squad_metric :  {'exact_match': 0.49832197701617004, 'f1': 4.854091224465016}\n",
        "train:: Epoch: 0025 cost = 0.072581, argmax acc = 0.021771 other_squad_metric :  {'exact_match': 1.3468634686346863, 'f1': 6.566252875913047}\n",
        "valid:: Epoch: 0025 cost = 0.082600, argmax acc = 0.008136, other_squad_metric :  {'exact_match': 0.6203600122038035, 'f1': 4.427058829862266}\n",
        "train:: Epoch: 0026 cost = 0.072373, argmax acc = 0.022226 other_squad_metric :  {'exact_match': 1.3825338253382533, 'f1': 6.691890118043898}\n",
        "valid:: Epoch: 0026 cost = 0.082687, argmax acc = 0.009661, other_squad_metric :  {'exact_match': 0.5084918132818061, 'f1': 4.726996743387878}\n",
        "train:: Epoch: 0027 cost = 0.072126, argmax acc = 0.022128 other_squad_metric :  {'exact_match': 1.3800738007380073, 'f1': 6.704347870959806}\n",
        "valid:: Epoch: 0027 cost = 0.083415, argmax acc = 0.007119, other_squad_metric :  {'exact_match': 0.3559442692972643, 'f1': 4.3227320610159}\n",
        "train:: Epoch: 0028 cost = 0.071940, argmax acc = 0.022964 other_squad_metric :  {'exact_match': 1.4858548585485856, 'f1': 6.8166554814508356}\n",
        "valid:: Epoch: 0028 cost = 0.082823, argmax acc = 0.009458, other_squad_metric :  {'exact_match': 0.5695108308756229, 'f1': 4.664080646130991}\n",
        "train:: Epoch: 0029 cost = 0.071652, argmax acc = 0.023149 other_squad_metric :  {'exact_match': 1.4710947109471095, 'f1': 6.8521612048196765}\n",
        "valid:: Epoch: 0029 cost = 0.083159, argmax acc = 0.007526, other_squad_metric :  {'exact_match': 0.4373029594223533, 'f1': 4.5875795966660275}\n",
        "train:: Epoch: 0030 cost = 0.071447, argmax acc = 0.024428 other_squad_metric :  {'exact_match': 1.4920049200492005, 'f1': 6.911899083776001}\n",
        "valid:: Epoch: 0030 cost = 0.083746, argmax acc = 0.006814, other_squad_metric :  {'exact_match': 0.4271331231567172, 'f1': 4.501357764974634}\n",
        "train:: Epoch: 0031 cost = 0.071172, argmax acc = 0.024945 other_squad_metric :  {'exact_match': 1.5891758917589176, 'f1': 7.14310523963246}\n",
        "valid:: Epoch: 0031 cost = 0.083821, argmax acc = 0.007424, other_squad_metric :  {'exact_match': 0.4474727956879894, 'f1': 4.426732009177166}\n",
        "train:: Epoch: 0032 cost = 0.070933, argmax acc = 0.025178 other_squad_metric :  {'exact_match': 1.5707257072570726, 'f1': 7.100587584299848}\n",
        "valid:: Epoch: 0032 cost = 0.084167, argmax acc = 0.007322, other_squad_metric :  {'exact_match': 0.5288314858130784, 'f1': 4.336364783978499}\n",
        "train:: Epoch: 0033 cost = 0.070716, argmax acc = 0.025646 other_squad_metric :  {'exact_match': 1.6408364083640836, 'f1': 7.230598951569533}\n",
        "valid:: Epoch: 0033 cost = 0.084090, argmax acc = 0.007322, other_squad_metric :  {'exact_match': 0.5084918132818061, 'f1': 4.543467719005027}\n",
        "train:: Epoch: 0034 cost = 0.070453, argmax acc = 0.025806 other_squad_metric :  {'exact_match': 1.6186961869618697, 'f1': 7.191681394622516}\n",
        "valid:: Epoch: 0034 cost = 0.084361, argmax acc = 0.007526, other_squad_metric :  {'exact_match': 0.46781246821926165, 'f1': 4.736551729213181}\n",
        "train:: Epoch: 0035 cost = 0.070197, argmax acc = 0.026556 other_squad_metric :  {'exact_match': 1.6137761377613775, 'f1': 7.2650431586045725}\n",
        "valid:: Epoch: 0035 cost = 0.084460, argmax acc = 0.008034, other_squad_metric :  {'exact_match': 0.5084918132818061, 'f1': 4.496105225498365}\n",
        "train:: Epoch: 0036 cost = 0.070010, argmax acc = 0.027011 other_squad_metric :  {'exact_match': 1.6863468634686347, 'f1': 7.352410576934097}\n",
        "valid:: Epoch: 0036 cost = 0.084535, argmax acc = 0.007831, other_squad_metric :  {'exact_match': 0.4779823044848978, 'f1': 4.4192404175252715}\n",
        "train:: Epoch: 0037 cost = 0.069749, argmax acc = 0.027835 other_squad_metric :  {'exact_match': 1.7392373923739237, 'f1': 7.513770905912509}\n",
        "valid:: Epoch: 0037 cost = 0.084879, argmax acc = 0.006814, other_squad_metric :  {'exact_match': 0.3864537780941727, 'f1': 4.509360088792529}\n",
        "train:: Epoch: 0038 cost = 0.069538, argmax acc = 0.028352 other_squad_metric :  {'exact_match': 1.795817958179582, 'f1': 7.567768556050303}\n",
        "valid:: Epoch: 0038 cost = 0.085208, argmax acc = 0.007119, other_squad_metric :  {'exact_match': 0.3152649242347198, 'f1': 4.320164108583346}\n",
        "train:: Epoch: 0039 cost = 0.069376, argmax acc = 0.028893 other_squad_metric :  {'exact_match': 1.7761377613776137, 'f1': 7.477864285397355}\n",
        "valid:: Epoch: 0039 cost = 0.085120, argmax acc = 0.007119, other_squad_metric :  {'exact_match': 0.3762839418285366, 'f1': 4.465381510521311}\n",
        "train:: Epoch: 0040 cost = 0.069104, argmax acc = 0.028647 other_squad_metric :  {'exact_match': 1.7416974169741697, 'f1': 7.64480520077062}\n",
        "valid:: Epoch: 0040 cost = 0.085419, argmax acc = 0.006509, other_squad_metric :  {'exact_match': 0.3661141055629004, 'f1': 4.3012572132869}\n",
        "train:: Epoch: 0041 cost = 0.068922, argmax acc = 0.029397 other_squad_metric :  {'exact_match': 1.897908979089791, 'f1': 7.657625834154736}\n",
        "valid:: Epoch: 0041 cost = 0.085548, argmax acc = 0.007017, other_squad_metric :  {'exact_match': 0.4373029594223533, 'f1': 4.419533677236369}\n",
        "train:: Epoch: 0042 cost = 0.068677, argmax acc = 0.030074 other_squad_metric :  {'exact_match': 1.8560885608856088, 'f1': 7.782342928965346}\n",
        "valid:: Epoch: 0042 cost = 0.085584, argmax acc = 0.007119, other_squad_metric :  {'exact_match': 0.4373029594223533, 'f1': 4.279498076008183}\n",
        "train:: Epoch: 0043 cost = 0.068484, argmax acc = 0.030578 other_squad_metric :  {'exact_match': 1.8929889298892988, 'f1': 7.918387526381218}\n",
        "valid:: Epoch: 0043 cost = 0.086364, argmax acc = 0.005492, other_squad_metric :  {'exact_match': 0.4373029594223533, 'f1': 4.302321630297892}\n",
        "train:: Epoch: 0044 cost = 0.068248, argmax acc = 0.030664 other_squad_metric :  {'exact_match': 1.952029520295203, 'f1': 7.9328011644837195}\n",
        "valid:: Epoch: 0044 cost = 0.085853, argmax acc = 0.007322, other_squad_metric :  {'exact_match': 0.4779823044848978, 'f1': 4.6060141788937745}\n",
        "train:: Epoch: 0045 cost = 0.068045, argmax acc = 0.030812 other_squad_metric :  {'exact_match': 1.944649446494465, 'f1': 8.078085092699313}\n",
        "valid:: Epoch: 0045 cost = 0.086303, argmax acc = 0.006509, other_squad_metric :  {'exact_match': 0.5084918132818061, 'f1': 4.511649043562105}\n",
        "train:: Epoch: 0046 cost = 0.067739, argmax acc = 0.032706 other_squad_metric :  {'exact_match': 2.025830258302583, 'f1': 8.129184878529529}\n",
        "valid:: Epoch: 0046 cost = 0.086710, argmax acc = 0.004882, other_squad_metric :  {'exact_match': 0.4373029594223533, 'f1': 4.139784476826585}\n",
        "train:: Epoch: 0047 cost = 0.067588, argmax acc = 0.031587 other_squad_metric :  {'exact_match': 1.985239852398524, 'f1': 8.122182574030209}\n",
        "valid:: Epoch: 0047 cost = 0.086525, argmax acc = 0.005390, other_squad_metric :  {'exact_match': 0.41696328689108103, 'f1': 4.51468017892951}\n",
        "train:: Epoch: 0048 cost = 0.067337, argmax acc = 0.032386 other_squad_metric :  {'exact_match': 2.054120541205412, 'f1': 8.30270446700247}\n",
        "valid:: Epoch: 0048 cost = 0.087089, argmax acc = 0.005492, other_squad_metric :  {'exact_match': 0.3356045967659921, 'f1': 4.134395096402048}\n",
        "train:: Epoch: 0049 cost = 0.067101, argmax acc = 0.033432 other_squad_metric :  {'exact_match': 2.1316113161131613, 'f1': 8.364209134811277}\n",
        "valid:: Epoch: 0049 cost = 0.087063, argmax acc = 0.005593, other_squad_metric :  {'exact_match': 0.41696328689108103, 'f1': 4.447391790002926}\n",
        "train:: Epoch: 0050 cost = 0.066838, argmax acc = 0.033137 other_squad_metric :  {'exact_match': 2.2189421894218944, 'f1': 8.515114560353371}\n",
        "valid:: Epoch: 0050 cost = 0.087535, argmax acc = 0.005288, other_squad_metric :  {'exact_match': 0.32543476050035597, 'f1': 4.027121477755915}\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76NH5MK7L1OG"
      },
      "source": [
        "## 4. LSTM + Attention for SQuAD\n",
        "\n",
        "**Problem 4.1** *(20 points)* Here, we will be appending an attention layer on top of LSTM outputs. We will use a single-head attention sublayer from Transformer. That is, you will implement \n",
        "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d}}\\right)V,$$\n",
        "where $Q, K, V$ is obtained by the linear transformation of the hidden states of the LSTM outputs $H$, i.e. $Q = HW^Q, K=HW^K, V=HW^V$ ($W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d}$ are trainable weights). Note that the output of $\\text{Attention}$ layer has the same dimension as $H$, so you can directly append your token classification layer on top of it. Report the accuracy and compare it with 3.2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoL0X9C2Gi0v"
      },
      "source": [
        "**Answer 4.1**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAvP_sFiGkYj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "f9f37a66-e32f-42ea-9379-659e81161094"
      },
      "source": [
        "import torch.nn as nn\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "class AttentionLSTMModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, n_layers, n_label, emb_dropout, rnn_dropout, bidirectional, enable_layer_norm, device):\n",
        "        super(ClassificationLSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
        "                            hidden_size=hidden_dim, \n",
        "                            num_layers=n_layers, \n",
        "                            dropout=rnn_dropout, \n",
        "                            bidirectional=bidirectional)\n",
        "      \n",
        "        n_direction = 2 if bidirectional else 1\n",
        "        self.fc_start = nn.Linear(hidden_dim*n_direction, n_label, bias=True)\n",
        "        self.fc_end = nn.Linear(hidden_dim*n_direction, n_label, bias=True)\n",
        "\n",
        "        # Layer_normalization\n",
        "        self.enable_layer_norm = enable_layer_norm\n",
        "        if enable_layer_norm:\n",
        "            self.norm1 = nn.LayerNorm(embedding_dim)\n",
        "            self.norm2 = nn.LayerNorm(hidden_dim*n_direction)\n",
        "\n",
        "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
        "        self.fc_dropout = nn.Dropout(rnn_dropout)\n",
        "        self.bidirectional = bidirectional\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, input_tensor, src_seq_lens):\n",
        "        emb = self.embedding(input_tensor) # emb.shape = batch * len * hidden\n",
        "\n",
        "        # Layer_normalization\n",
        "        if self.enable_layer_norm:\n",
        "            emb = self.norm1(emb)\n",
        "\n",
        "        emb = self.emb_dropout(emb)\n",
        "        emb = emb.transpose(0, 1) # emb.shape = len * batch * hidden\n",
        "\n",
        "        # n_direction = 2 if bidirectional else 1\n",
        "        # hidden = torch.zeros(n_layers*n_direction, context.shape[0], hidden_dim, requires_grad=True).to(self.device)\n",
        "        # cell = torch.zeros(n_layers*n_direction, context.shape[0], hidden_dim, requires_grad=True).to(self.device)\n",
        "\n",
        "        # nn.LSTM\n",
        "        packed = pack_padded_sequence(emb, src_seq_lens.tolist(), batch_first=False)\n",
        "        outs, (hidden, cell) = self.lstm(packed)\n",
        "        outs, out_lens = pad_packed_sequence(outs, batch_first=False)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            hidden = torch.stack([hidden[-2], hidden[-1]], dim=0)\n",
        "        else:\n",
        "            hidden = hidden[-1].unsqueeze(dim=0)\n",
        "        print(hidden.shape)\n",
        "        hidden = hidden.transpose(0, 1)\n",
        "        hidden = hidden.contiguous().view(hidden.shape[0], -1)\n",
        "        print(hidden.shape)\n",
        "\n",
        "        # # Layer_normalization\n",
        "        # if self.enable_layer_norm:\n",
        "        #     hidden = self.norm2(hidden)\n",
        "\n",
        "        hidden = self.fc_dropout(hidden)\n",
        "        logits_start = self.fc_start(hidden)\n",
        "        logits_end = self.fc_end(hidden)\n",
        "        return (logits_start, logits_end)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device : \", device)\n",
        "\n",
        "# Vocabulary : Use vocab_list\n",
        "\n",
        "# Construct the LSTM Model\n",
        "embedding_dim = 128 # usually bigger, e.g. 128\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "n_label = max_length+1 if use_bos else max_length\n",
        "emb_dropout = 0.5\n",
        "rnn_dropout = 0.5\n",
        "bidirectional = True\n",
        "enable_layer_norm = True\n",
        "rnnmodel = AttentionLSTMModel(embedding_dim, hidden_dim, n_layers, n_label, emb_dropout, rnn_dropout, bidirectional, enable_layer_norm, device).to(device)\n",
        "\n",
        "print(\"batch_size : \", batch_size)\n",
        "print(\"max_length : \", max_length)\n",
        "print(\"embedding_dim : \", embedding_dim)\n",
        "print(\"hidden_dim : \", hidden_dim)\n",
        "print(\"n_layers : \", n_layers)\n",
        "print(\"emb_dropout : \", emb_dropout)\n",
        "print(\"rnn_dropout_and_fc_dropout : \", rnn_dropout)\n",
        "if bidirectional:\n",
        "    print(\"bidirectional : True\")\n",
        "else:\n",
        "    print(\"bidirectional : False\")\n",
        "if enable_layer_norm:\n",
        "    print(\"enable_layer_norm : True\")\n",
        "else:\n",
        "    print(\"enable_layer_norm : False\")\n",
        "\n",
        "# Construct the data loader\n",
        "train_iter = loader.train_iter\n",
        "valid_iter = loader.valid_iter\n",
        "\n",
        "train_id_list = loader.train_id_list\n",
        "valid_id_list = loader.valid_id_list\n",
        "train_reference = loader.train_reference\n",
        "valid_reference = loader.valid_reference\n",
        "\n",
        "# Training\n",
        "learning_rate = 5e-4\n",
        "print(\"learning_rate : \", learning_rate)\n",
        "\n",
        "PAD_IDX = vocab.stoi['<pad>']\n",
        "cel = nn.CrossEntropyLoss(ignore_index=PAD_IDX) # Ignore Padding\n",
        "# optimizer = torch.optim.SGD(rnnmodel.parameters(), lr=1e-1)\n",
        "optimizer = torch.optim.Adam(rnnmodel.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 30\n",
        "max_norm = 5\n",
        "\n",
        "# Evaluate\n",
        "squad_metric = load_metric('squad')\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    train_loss = 0\n",
        "    train_accuracy = 0.0\n",
        "    train_data_num = 0\n",
        "    train_prediction = list()\n",
        "    for train_i, train_batch in enumerate(train_iter):\n",
        "        context, context_length = train_batch.context_question\n",
        "        answer_start = train_batch.answer_start\n",
        "        answer_end = train_batch.answer_end\n",
        "        train_id_index = train_batch.id_index\n",
        "\n",
        "        logits_start, logits_end = rnnmodel(context, context_length)\n",
        "\n",
        "        optimizer.zero_grad() # reset process\n",
        "        loss = cel(logits_start, answer_start) + cel(logits_end, answer_end) # Loss, a.k.a L\n",
        "        loss.backward() # compute gradients\n",
        "        # print(torch.norm(rnnmodel.lstm.weight_hh_l0.grad), loss.item())\n",
        "        # torch.nn.utils.clip_grad_norm_(rnnmodel.parameters(), max_norm) # gradent clipping\n",
        "        optimizer.step() # update parameters\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        _, train_start_preds = torch.max(logits_start, 1)\n",
        "        _, train_end_preds = torch.max(logits_end, 1)\n",
        "        train_accuracy += ((train_start_preds == answer_start) * (train_end_preds == answer_end)).sum().float()\n",
        "\n",
        "        train_data_num += context.shape[0]\n",
        "\n",
        "        for train_j in range(context.shape[0]):\n",
        "            pred_text = \"\"\n",
        "            start = train_start_preds[train_j]\n",
        "            end = train_end_preds[train_j]\n",
        "            if start < end:\n",
        "                pred_text = [vocab_list[text_id] for text_id in context[train_j][start:end+1]]\n",
        "                pred_text = \" \".join(pred_text)\n",
        "\n",
        "            # start = answer_start[train_j]\n",
        "            # end = answer_end[train_j]\n",
        "            # answer_text = [vocab_list[text_id] for text_id in context[train_j][start:end+1]]\n",
        "            # answer_text = \" \".join(answer_text)\n",
        "            # print(pred_text, answer_text, train_reference[train_id_index[train_j]], \"\\n\")\n",
        "\n",
        "            train_prediction.append({'id':train_id_list[train_id_index[train_j]], 'prediction_text':pred_text})\n",
        "\n",
        "    train_result = squad_metric.compute(predictions=train_prediction, references=train_reference)\n",
        "    print('train:: Epoch:', '%04d' % (epoch + 1), \n",
        "          'cost =', '{:.6f},'.format(train_loss / train_data_num), \n",
        "          'argmax acc =', '{:.6f}'.format(train_accuracy / train_data_num),\n",
        "          'other_squad_metric : ', train_result)\n",
        "        \n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        with torch.no_grad():\n",
        "            valid_loss = 0\n",
        "            valid_accuracy = 0.0\n",
        "            valid_data_num = 0\n",
        "            valid_prediction = list()\n",
        "            for valid_i, valid_batch in enumerate(valid_iter):\n",
        "                context, context_length = valid_batch.context_question\n",
        "                answer_start = valid_batch.answer_start\n",
        "                answer_end = valid_batch.answer_end\n",
        "                valid_id_index = valid_batch.id_index\n",
        "\n",
        "                logits_start, logits_end = rnnmodel(context, context_length)\n",
        "\n",
        "                loss = cel(logits_start, answer_start) + cel(logits_end, answer_end) # Loss, a.k.a L\n",
        "                valid_loss += loss.item()\n",
        "\n",
        "                _, valid_start_preds = torch.max(logits_start, 1)\n",
        "                _, valid_end_preds = torch.max(logits_end, 1)\n",
        "                valid_accuracy += ((valid_start_preds == answer_start) * (valid_end_preds == answer_end)).sum().float()\n",
        "\n",
        "                valid_data_num += context.shape[0]\n",
        "\n",
        "                for valid_j in range(context.shape[0]):\n",
        "                    pred_text = \"\"\n",
        "                    start = valid_start_preds[valid_j]\n",
        "                    end = valid_end_preds[valid_j]\n",
        "                    if start < end:\n",
        "                        pred_text = [vocab_list[text_id] for text_id in context[valid_j][start:end+1]]\n",
        "                        pred_text = \" \".join(pred_text)\n",
        "                    valid_prediction.append({'id':valid_id_list[valid_id_index[valid_j]], 'prediction_text':pred_text})\n",
        "                \n",
        "            valid_result = squad_metric.compute(predictions=valid_prediction, references=valid_reference)\n",
        "            print('valid:: Epoch:', '%04d' % (epoch + 1), \n",
        "                  'cost =', '{:.6f},'.format(valid_loss / valid_data_num), \n",
        "                  'argmax acc =', '{:.6f},'.format(valid_accuracy / valid_data_num),\n",
        "                  'other_squad_metric : ', valid_result)\n",
        "            "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6aeb0805468e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"device : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT_N1lJqQFY-"
      },
      "source": [
        "**Problem 4.2** *(10 points)* On top of the attention layer, let's add another layer of (bi-directional) LSTM. So this will look like a *sandwich* where the LSTM is bread and the attention is ham. How does it affect the accuracy? Explain why do you think this happens. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yi557rwGlSG"
      },
      "source": [
        "**Answer 4.2**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJv7a2fiGmTe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN6XQLYJRrR6"
      },
      "source": [
        "## 5. Attention is All You Need\n",
        "\n",
        "**Problem 5.1 (bonus)** *(20 points)*  Implement full Transformer encoder to entirely replace LSTMs. You are allowed to copy and paste code from [*Annotated Transformer*](https://nlp.seas.harvard.edu/2018/04/03/attention.html) (but nowhere else). Report the accuracy and explain what seems to happening with attetion-only model compared to LSTM+Attention model(s). \n",
        "\n",
        "**Answer 5.1**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5kALeKhU4JK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zesTXw0GpK2"
      },
      "source": [
        "**Problem 5.2 (bonus)** *(10 points)* Replace Transformer's sinusoidal position encoding with a fixed-length (of 256) position embedding. That is, you will create a 256-by-$d$ trainable parameter matrix for the position encoding that replaces the variable-length sinusoidal encoding. What is the clear disdvantage of this approach? Report the accuracy and compare it with 5.1. Note that this also has a clear advantage, as we will see in our future lecture on Pretrained Language Model, and more specifically, BERT (Devlin et al., 2018).\n",
        "\n",
        "**Answer 5.2**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_-YXpobIfUP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}